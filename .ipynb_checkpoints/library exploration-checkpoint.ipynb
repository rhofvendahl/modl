{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I guess I am feeling kinda tired. I feel overwhelmed, a bit, maybe hungry. I dunno. I find myself wanting something, but I'm not sure what it is. I feel stressed certainly, too much to do maybe? But I'm not totally sure what I should be doing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(myself, wanting, something)\n"
     ]
    }
   ],
   "source": [
    "svo_triples = textacy.extract.subject_verb_object_triples(doc)\n",
    "\n",
    "for triple in svo_triples:\n",
    "    print(triple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am feeling --> kinda tired\n",
      "I feel --> overwhelmed, a bit, maybe hungry\n",
      "I feel --> stressed certainly, too much to do maybe\n"
     ]
    }
   ],
   "source": [
    "# returns (entity, cue, fragment)\n",
    "statements = textacy.extract.semistructured_statements(doc, 'I', cue='feel')\n",
    "\n",
    "for entity, cue, fragment in statements:\n",
    "    print(entity, cue, '-->', fragment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent: I guess I am feeling kinda tired. \n",
      "verbs: [guess, feeling]\n",
      "I guess (I, guess, I am feeling kinda tired)\n",
      "I feeling (I, am feeling, kinda tired)\n",
      "I feeling (I, feel, overwhelmed, a bit, maybe hungry)\n",
      "I feeling (I, feel, stressed certainly, too much to do maybe)\n",
      "\n",
      "\n",
      "sent: I feel overwhelmed, a bit, maybe hungry. \n",
      "verbs: [feel]\n",
      "I feel (I, am feeling, kinda tired)\n",
      "I feel (I, feel, overwhelmed, a bit, maybe hungry)\n",
      "I feel (I, feel, stressed certainly, too much to do maybe)\n",
      "\n",
      "\n",
      "sent: I dunno. \n",
      "verbs: [dunno]\n",
      "\n",
      "\n",
      "sent: I find myself wanting something, but I'm not sure what it is. \n",
      "verbs: [find, wanting, 'm, is]\n",
      "I find (I, find, myself wanting something, but I'm not sure what it is)\n",
      "I 'm (I, 'm, not sure what it is)\n",
      "I 'm (I, 'm, not totally sure what I should be doing)\n",
      "\n",
      "\n",
      "sent: I feel stressed certainly, too much to do maybe? \n",
      "verbs: [feel, do]\n",
      "I feel (I, am feeling, kinda tired)\n",
      "I feel (I, feel, overwhelmed, a bit, maybe hungry)\n",
      "I feel (I, feel, stressed certainly, too much to do maybe)\n",
      "\n",
      "\n",
      "sent: But I'm not totally sure what I should be doing? \n",
      "verbs: ['m, doing]\n",
      "I 'm (I, 'm, not sure what it is)\n",
      "I 'm (I, 'm, not totally sure what I should be doing)\n",
      "\n",
      "\n",
      "(I, 'm, not sure what it is)\n",
      "(I, find, myself wanting something, but I'm not sure what it is)\n",
      "(I, feel, overwhelmed, a bit, maybe hungry)\n",
      "(I, guess, I am feeling kinda tired)\n",
      "(I, feel, stressed certainly, too much to do maybe)\n",
      "(I, 'm, not totally sure what I should be doing)\n",
      "(I, am feeling, kinda tired)\n"
     ]
    }
   ],
   "source": [
    "# get cues\n",
    "all_statements = []\n",
    "for sent in doc.sents:\n",
    "    verbs = textacy.spacier.utils.get_main_verbs_of_sent(sent)\n",
    "    print('sent:', sent, '\\nverbs:', verbs)\n",
    "    for verb in verbs:\n",
    "        objects = textacy.spacier.utils.get_objects_of_verb(verb)\n",
    "        subjects = textacy.spacier.utils.get_subjects_of_verb(verb)\n",
    "        for subject in subjects:\n",
    "            statements = textacy.extract.semistructured_statements(doc, subject.text, verb.lemma_)\n",
    "            for statement in statements:\n",
    "                print(subject, verb, statement)\n",
    "                all_statements += [statement]\n",
    "    \n",
    "    print('\\n')\n",
    "for statement in set(all_statements):\n",
    "    print(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConfigurationError",
     "evalue": "'Cannot register decomposable_attention as Model; name already in use for DecomposableAttention'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConfigurationError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-98f42d5d6d17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"decomposable_attention\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mDecomposableAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \"\"\"\n\u001b[1;32m     19\u001b[0m     \u001b[0mThis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mimplements\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mDecomposable\u001b[0m \u001b[0mAttention\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mdescribed\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m\"\u001b[0m\u001b[0mA\u001b[0m \u001b[0mDecomposable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/common/registrable.py\u001b[0m in \u001b[0;36madd_subclass_to_registry\u001b[0;34m(subclass)\u001b[0m\n\u001b[1;32m     47\u001b[0m                 message = \"Cannot register %s as %s; name already in use for %s\" % (\n\u001b[1;32m     48\u001b[0m                         name, cls.__name__, registry[name].__name__)\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mConfigurationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mregistry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msubclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConfigurationError\u001b[0m: 'Cannot register decomposable_attention as Model; name already in use for DecomposableAttention'"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Optional, List, Any\n",
    "\n",
    "import torch\n",
    "\n",
    "from allennlp.common.checks import check_dimensions_match\n",
    "from allennlp.data import Vocabulary\n",
    "from allennlp.models.model import Model\n",
    "from allennlp.modules import FeedForward\n",
    "from allennlp.modules import Seq2SeqEncoder, SimilarityFunction, TimeDistributed, TextFieldEmbedder\n",
    "from allennlp.modules.matrix_attention.legacy_matrix_attention import LegacyMatrixAttention\n",
    "from allennlp.nn import InitializerApplicator, RegularizerApplicator\n",
    "from allennlp.nn.util import get_text_field_mask, masked_softmax, weighted_sum\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "\n",
    "\n",
    "# @Model.register(\"decomposable_attention\")\n",
    "class DecomposableAttention(Model):\n",
    "    \"\"\"\n",
    "    This ``Model`` implements the Decomposable Attention model described in `\"A Decomposable\n",
    "    Attention Model for Natural Language Inference\"\n",
    "    <https://www.semanticscholar.org/paper/A-Decomposable-Attention-Model-for-Natural-Languag-Parikh-T%C3%A4ckstr%C3%B6m/07a9478e87a8304fc3267fa16e83e9f3bbd98b27>`_\n",
    "    by Parikh et al., 2016, with some optional enhancements before the decomposable attention\n",
    "    actually happens.  Parikh's original model allowed for computing an \"intra-sentence\" attention\n",
    "    before doing the decomposable entailment step.  We generalize this to any\n",
    "    :class:`Seq2SeqEncoder` that can be applied to the premise and/or the hypothesis before\n",
    "    computing entailment.\n",
    "    The basic outline of this model is to get an embedded representation of each word in the\n",
    "    premise and hypothesis, align words between the two, compare the aligned phrases, and make a\n",
    "    final entailment decision based on this aggregated comparison.  Each step in this process uses\n",
    "    a feedforward network to modify the representation.\n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab : ``Vocabulary``\n",
    "    text_field_embedder : ``TextFieldEmbedder``\n",
    "        Used to embed the ``premise`` and ``hypothesis`` ``TextFields`` we get as input to the\n",
    "        model.\n",
    "    attend_feedforward : ``FeedForward``\n",
    "        This feedforward network is applied to the encoded sentence representations before the\n",
    "        similarity matrix is computed between words in the premise and words in the hypothesis.\n",
    "    similarity_function : ``SimilarityFunction``\n",
    "        This is the similarity function used when computing the similarity matrix between words in\n",
    "        the premise and words in the hypothesis.\n",
    "    compare_feedforward : ``FeedForward``\n",
    "        This feedforward network is applied to the aligned premise and hypothesis representations,\n",
    "        individually.\n",
    "    aggregate_feedforward : ``FeedForward``\n",
    "        This final feedforward network is applied to the concatenated, summed result of the\n",
    "        ``compare_feedforward`` network, and its output is used as the entailment class logits.\n",
    "    premise_encoder : ``Seq2SeqEncoder``, optional (default=``None``)\n",
    "        After embedding the premise, we can optionally apply an encoder.  If this is ``None``, we\n",
    "        will do nothing.\n",
    "    hypothesis_encoder : ``Seq2SeqEncoder``, optional (default=``None``)\n",
    "        After embedding the hypothesis, we can optionally apply an encoder.  If this is ``None``,\n",
    "        we will use the ``premise_encoder`` for the encoding (doing nothing if ``premise_encoder``\n",
    "        is also ``None``).\n",
    "    initializer : ``InitializerApplicator``, optional (default=``InitializerApplicator()``)\n",
    "        Used to initialize the model parameters.\n",
    "    regularizer : ``RegularizerApplicator``, optional (default=``None``)\n",
    "        If provided, will be used to calculate the regularization penalty during training.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab: Vocabulary,\n",
    "                 text_field_embedder: TextFieldEmbedder,\n",
    "                 attend_feedforward: FeedForward,\n",
    "                 similarity_function: SimilarityFunction,\n",
    "                 compare_feedforward: FeedForward,\n",
    "                 aggregate_feedforward: FeedForward,\n",
    "                 premise_encoder: Optional[Seq2SeqEncoder] = None,\n",
    "                 hypothesis_encoder: Optional[Seq2SeqEncoder] = None,\n",
    "                 initializer: InitializerApplicator = InitializerApplicator(),\n",
    "                 regularizer: Optional[RegularizerApplicator] = None) -> None:\n",
    "        super(DecomposableAttention, self).__init__(vocab, regularizer)\n",
    "\n",
    "        self._text_field_embedder = text_field_embedder\n",
    "        self._attend_feedforward = TimeDistributed(attend_feedforward)\n",
    "        self._matrix_attention = LegacyMatrixAttention(similarity_function)\n",
    "        self._compare_feedforward = TimeDistributed(compare_feedforward)\n",
    "        self._aggregate_feedforward = aggregate_feedforward\n",
    "        self._premise_encoder = premise_encoder\n",
    "        self._hypothesis_encoder = hypothesis_encoder or premise_encoder\n",
    "\n",
    "        self._num_labels = vocab.get_vocab_size(namespace=\"labels\")\n",
    "\n",
    "        check_dimensions_match(text_field_embedder.get_output_dim(), attend_feedforward.get_input_dim(),\n",
    "                               \"text field embedding dim\", \"attend feedforward input dim\")\n",
    "        check_dimensions_match(aggregate_feedforward.get_output_dim(), self._num_labels,\n",
    "                               \"final output dimension\", \"number of labels\")\n",
    "\n",
    "        self._accuracy = CategoricalAccuracy()\n",
    "        self._loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        initializer(self)\n",
    "\n",
    "    def forward(self,  # type: ignore\n",
    "                premise: Dict[str, torch.LongTensor],\n",
    "                hypothesis: Dict[str, torch.LongTensor],\n",
    "                label: torch.IntTensor = None,\n",
    "                metadata: List[Dict[str, Any]] = None) -> Dict[str, torch.Tensor]:\n",
    "        # pylint: disable=arguments-differ\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        premise : Dict[str, torch.LongTensor]\n",
    "            From a ``TextField``\n",
    "        hypothesis : Dict[str, torch.LongTensor]\n",
    "            From a ``TextField``\n",
    "        label : torch.IntTensor, optional, (default = None)\n",
    "            From a ``LabelField``\n",
    "        metadata : ``List[Dict[str, Any]]``, optional, (default = None)\n",
    "            Metadata containing the original tokenization of the premise and\n",
    "            hypothesis with 'premise_tokens' and 'hypothesis_tokens' keys respectively.\n",
    "        Returns\n",
    "        -------\n",
    "        An output dictionary consisting of:\n",
    "        label_logits : torch.FloatTensor\n",
    "            A tensor of shape ``(batch_size, num_labels)`` representing unnormalised log\n",
    "            probabilities of the entailment label.\n",
    "        label_probs : torch.FloatTensor\n",
    "            A tensor of shape ``(batch_size, num_labels)`` representing probabilities of the\n",
    "            entailment label.\n",
    "        loss : torch.FloatTensor, optional\n",
    "            A scalar loss to be optimised.\n",
    "        \"\"\"\n",
    "        embedded_premise = self._text_field_embedder(premise)\n",
    "        embedded_hypothesis = self._text_field_embedder(hypothesis)\n",
    "        premise_mask = get_text_field_mask(premise).float()\n",
    "        hypothesis_mask = get_text_field_mask(hypothesis).float()\n",
    "\n",
    "        if self._premise_encoder:\n",
    "            embedded_premise = self._premise_encoder(embedded_premise, premise_mask)\n",
    "        if self._hypothesis_encoder:\n",
    "            embedded_hypothesis = self._hypothesis_encoder(embedded_hypothesis, hypothesis_mask)\n",
    "\n",
    "        projected_premise = self._attend_feedforward(embedded_premise)\n",
    "        projected_hypothesis = self._attend_feedforward(embedded_hypothesis)\n",
    "        # Shape: (batch_size, premise_length, hypothesis_length)\n",
    "        similarity_matrix = self._matrix_attention(projected_premise, projected_hypothesis)\n",
    "\n",
    "        # Shape: (batch_size, premise_length, hypothesis_length)\n",
    "        p2h_attention = masked_softmax(similarity_matrix, hypothesis_mask)\n",
    "        # Shape: (batch_size, premise_length, embedding_dim)\n",
    "        attended_hypothesis = weighted_sum(embedded_hypothesis, p2h_attention)\n",
    "\n",
    "        # Shape: (batch_size, hypothesis_length, premise_length)\n",
    "        h2p_attention = masked_softmax(similarity_matrix.transpose(1, 2).contiguous(), premise_mask)\n",
    "        # Shape: (batch_size, hypothesis_length, embedding_dim)\n",
    "        attended_premise = weighted_sum(embedded_premise, h2p_attention)\n",
    "\n",
    "        premise_compare_input = torch.cat([embedded_premise, attended_hypothesis], dim=-1)\n",
    "        hypothesis_compare_input = torch.cat([embedded_hypothesis, attended_premise], dim=-1)\n",
    "\n",
    "        compared_premise = self._compare_feedforward(premise_compare_input)\n",
    "        compared_premise = compared_premise * premise_mask.unsqueeze(-1)\n",
    "        # Shape: (batch_size, compare_dim)\n",
    "        compared_premise = compared_premise.sum(dim=1)\n",
    "\n",
    "        compared_hypothesis = self._compare_feedforward(hypothesis_compare_input)\n",
    "        compared_hypothesis = compared_hypothesis * hypothesis_mask.unsqueeze(-1)\n",
    "        # Shape: (batch_size, compare_dim)\n",
    "        compared_hypothesis = compared_hypothesis.sum(dim=1)\n",
    "\n",
    "        aggregate_input = torch.cat([compared_premise, compared_hypothesis], dim=-1)\n",
    "        label_logits = self._aggregate_feedforward(aggregate_input)\n",
    "        label_probs = torch.nn.functional.softmax(label_logits, dim=-1)\n",
    "\n",
    "        output_dict = {\"label_logits\": label_logits,\n",
    "                       \"label_probs\": label_probs,\n",
    "                       \"h2p_attention\": h2p_attention,\n",
    "                       \"p2h_attention\": p2h_attention}\n",
    "\n",
    "        if label is not None:\n",
    "            loss = self._loss(label_logits, label.long().view(-1))\n",
    "            self._accuracy(label_logits, label)\n",
    "            output_dict[\"loss\"] = loss\n",
    "\n",
    "        if metadata is not None:\n",
    "            output_dict[\"premise_tokens\"] = [x[\"premise_tokens\"] for x in metadata]\n",
    "            output_dict[\"hypothesis_tokens\"] = [x[\"hypothesis_tokens\"] for x in metadata]\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\n",
    "                'accuracy': self._accuracy.get_metric(reset),\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
