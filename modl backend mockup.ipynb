{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_coref_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'November was a trying month… on the 7th Dante had a major accident. 5 minutes before school and he and some friends are climbing the fence, I tell him it’s not a good idea and to get down. I turn back to talk to Jodi (on of my best mom friend’s at the school) and Dante comes to me screaming with his hand full of blood. I run him into my classroom and get him to the sink, as I turn on the water to clean the area the flap of his thumb lifts away and I see the bone. Shit. This isn’t something I can fix here, I grab my first aid kit and wrap it like crazy because it’s bleeding like crazy. I phone James and tell him to get to the ER as Dante is screaming and freaking out in the background as I’m trying to usher him back to the car as he’s bleeding like a stuffed pig. Unfortunately in the ER I learned that my child doesn’t take to freezing, an hour of gel freezing and he still felt the 2 needles as they went in, 15 minutes later and he felt the last 2 stitches of 8. He needed more because his finger still had gaps, the doctor didn’t want to cause him anymore pain so he glued them. It was an intense and deep gash that spiraled all the way up his thumb. I was trying to stay strong for him but I did break down as he screamed and cried, I was left to emotionally drained that day. James was able to take the remainder of the day off and stay with him. He missed 2 more days of school and then had an extra long weekend due to the holiday and the pro day but for 2 weeks he couldn’t write (of course it was his right hand.) 3 doctor visits later and he finally got them out full last week, the first visit the doctor wanted them in longer because of the severity. 2nd time he could only get 6 out because the glue had gotten on the last 2 stitches and he didn’t want to have to dig them out so we had to soak and dissolve the glue for 3 days. 3rd time the last 2 came out.  Even now he’s slowly regaining his writing skills as there was some nerve damage.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'So I have had a good day today. I found out we got the other half of our funding for my travel grant, which paid for my friend to come with me. So that’s good, she and I will both get some money back. I took my dogs to the pet store so my girl dog could get a new collar, but she wanted to beat everyone up. This is an ongoing issue with her. She’s so little and cute too but damn she acts like she’s gonna go for the jugular with everyone she doesn’t know! She did end up with a cute new collar tho, it has pineapples on it. I went to the dentist and she’s happy with my Invisalign progress. We have three more trays and then she does an impression to make sure my teeth are where they need to be before they get the rest of the trays. YAY! And I don’t have to make another payment until closer to the end of my treatment. I had some work emails with the festival, and Jessie was bringing up some important points, and one of our potential artists was too expensive to work with, so Mutual Friend was asking for names for some other people we could work with. So I suggested like, three artists, and Jessie actually liked the idea of one of them doing it. Which is nice. I notice she is very encouraging at whatever I contribute to our collective. It’s sweet. I kind of know this is like, the only link we have with each other right now besides social media, so it seems like she’s trying to make sure I know she still wants me to be involved and doesn’t have bad feelings for me. And there was a short period when I was seriously thinking of leaving the collective and not working with this festival anymore. I was so sad, and felt so upset, and didn’t know what to do about Jessie. It felt really close to me throwing in the towel. But I hung on through the festival and it doesn’t seem so bad from this viewpoint now with more time that has passed. And we have been gentle, if reserved, with each other. I mean her last personal email to me however many weeks ago wasn’t very nice. But it seems like we’ve been able to put it aside for work reasons. I dunno. I still feel like if anything was gonna get mended between us, she would need to make the first moves on that. I really don’t want to try reaching out and get rejected even as a friend again. I miss her though. And sometimes I think she misses me. But I don’t want to approach her assuming we both miss each other and have her turn it on me again and make out like all these things are all in my head. I don’t know about that butch I went on a date with last night. I feel more of a friend vibe from her, than a romantic one. I can’t help it, I am just not attracted to butches. And I don’t know how to flirt with them. And I don’t think of them in a sexy way. But I WOULD like another butch buddy. I mean yeah maybe Femmes do play games, or maybe I just chased all the wrong Femmes. Maybe I’ll just leave this and not think about it much until I get back to town in January.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'well, i tried to get an x-ray of my neck today but, when i got to the medical center and stood in line to wait to get checked in, i was told my doctor hadn’t sent over the orders for it! and the thing is he said he had already sent it when i asked him if i needed anything to get it done. i don’t like being lied to. so, since i have to go to the medical center thursday morning for a consultation for p/t, i’ll just go on back over and get the xray. the lady there said monday and tuesday are really busy days and thursday would be much better.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I can’t help but feel annoyed, angry, disheartened, let down… and yet in another way I want to say “you don’t deserve to know him.” Dante’s growing into such an amazing child and yet it seems our family dwindles like crazy, he brings up James’ sister “aunty Tammy” and asks why we never see her. I say she’s busy because she has 2 of her own little boys, but that’s not the case. James’ sister had this dream of being an amazing aunt to  Dante and she has done nothing to be in his life. Birthday gifts, Christmas, there’s no communication or her ever asking about him, not that I even speak to  her much but she just doesn’t care to be an active part in his world which pisses me off to no end. James’ mother couldn’t get herself clean to stay in his life… she’s non existent to him. It boils my blood because when he was born she was so proud, got clean for a while, and then couldn’t hack it (ended up visiting and left her morphine out where our very smart 2 year old brought us a handful of pills and asked if they  were candy.) That was the last time she saw him and her memory has since been forgotten. You couldn’t even get clean to be in your grandchild’s life? She was always a pathetic excuse for a mother, James’ childhood simply enrages me, the idea of a child living the way he did because of her ways makes me sick. She doesn’t deserve to know my child. My own brother “uncle Jason” is seen in passing about 5 times a year, he’s good with Dante, pleasant enough considering my brother has so many anger issues. He’s also a drug addict so it’s not like I would ever allow him time alone with Dante, not that he’d ever want to spend time with him. Christmas is coming up and in a way it’s bittersweet. My half cousin Tianna’s two girls (Stella and Piper)  have two sets of everything, tons of aunts and uncles, and they have a huge loving family unit. Dante doesn’t have that, yes his grandparents love him like crazy but his family connection is like mine. When I was young I only had one set of grandparents, my dad was adopted and his mother wasn’t around at all… in a way my grandparents adopted him as well when he married my young at the age of 18. I had my uncle George and my Omi and Opi and my mom and dad and my brother. I remember when George met Denise and I met Tianna (Denise’s child from her first marriage.) I  remember the day that I learned that George had proposed to Denise and that they were getting married, I cried and my mom thought I was happy. I wasn’t happy, I was devastated that suddenly I had to share my family (horrible to thing to cry about right?) Tianna already had 2 sets of grandparents, she had tons of aunts, and now she was getting my uncle whom I loved and thought was the coolest guy around as a dad. I was so angry. I never really got to  meet Tianna’s dad’s side of the family, I met her grandparents a few times but they never remembered my name which really hurt and annoyed me. I joined soccer and Tianna’s dad was the coach, he wasn’t nice to me which drove the wedge deeper. It hurts… it hurts that my child has the same issue that I did although I really don’t think he’s realized that he’s different. His “grandpa Morgan” isn’t his real grandpa, more like a man who took on his father and tried to “raise” him to be a man, he obviously didn’t stay with James’ mom but he’s still in our life and I’m grateful  that he’s there. Unfortunately he’s not around much, we see him 2-3 times a year because he lives in Golden. Uncle Adam is James’ childhood best friend, a good guy  who’s more of a businessman who  lives to the beat of his own drum and wouldn’t know what to do with  a child if his  life depended on it. He’s around but again it’s only a few times a year when he visits from Calgary. I want to say sometimes you get to choose your own family, but even the family I chose for him and thought would be around forever, the people who were there when he was born, grew, shared so many moments with are no longer around. They don’t seem to care either, it’s not like “aunty Kat” ever talks to me or asks about him. Seems like moving meant the end of our friendship and our “family ties.”'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Sheila was run over by a truck. She herself didn't see that coming. I told her she should take care of herself, but I know she'll just go and do her thing regardless of what I say. What a conundrum! This makes me wish I had never signed up to be friends with her, although I do love the girl.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = textacy.preprocess.normalize_whitespace(text)\n",
    "preprocessed = textacy.preprocess.preprocess_text(preprocessed, fix_unicode=True, no_contractions=True, no_accents=True)\n",
    "doc = nlp(preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now assuming all names are unique identifiers\n",
    "class Person:\n",
    "    statements = []\n",
    "    \n",
    "    def __init__(self, name, pronouns=None, mentions=[], user=False):\n",
    "        self.name = name\n",
    "#         self.gender = gender\n",
    "        self.mentions = mentions\n",
    "        self.user = user        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPGRADE AT SOME POINT TO EXTRACT GENDER, ACCOUNT FOR CLUSTERS WITHOUT NAMES\n",
    "# UPGRADE TO INCLUDE I, USER\n",
    "\n",
    "# assumes names are unique identifiers\n",
    "# assumes misspellings are diff people\n",
    "\n",
    "# MEMORYLESS FOR NOW; each change to text means a whole new model\n",
    "class Model:\n",
    "    people = []\n",
    "    \n",
    "    def __init__(self, )\n",
    "    def get_person_by_name(self, name):\n",
    "        for person in self.people:\n",
    "            if person.name == name:\n",
    "                return person\n",
    "        return None\n",
    "    \n",
    "    def update(self, text):\n",
    "        name_mentions = [ent for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "        names = set([name_mention.text for name_mention in name_mentions])\n",
    "        \n",
    "        # for clusters that includ ename mentions\n",
    "        for cluster in doc._.coref_clusters:\n",
    "            name = None\n",
    "            \n",
    "            for mention in cluster.mentions:\n",
    "                keyword = mention.root.text\n",
    "                if keyword in names:\n",
    "                    name = keyword\n",
    "                    \n",
    "            print(name, cluster.mentions)\n",
    "            if name != None:\n",
    "                person = self.get_person_by_name(name)\n",
    "                if person == None:\n",
    "                    person = Person(name, mentions=cluster.mentions)\n",
    "                    self.people += [person]\n",
    "                else:\n",
    "                    person.mentions += cluster.mentions\n",
    "            \n",
    "            # for named entities without clusters (single mentions)\n",
    "            for name_mention in name_mentions:\n",
    "                person = self.get_person_by_name(name_mention.text)\n",
    "                if person == None:\n",
    "                    person = Person(name_mention.text, mentions=[name_mention])\n",
    "                    self.people += [person]\n",
    "        \n",
    "    def resolve_people(self, doc):\n",
    "        tokens = [token.text for token in doc]\n",
    "\n",
    "        for person in self.people:\n",
    "            for mention in person.mentions:\n",
    "                \n",
    "                # determine resolved value\n",
    "                resolved = person.name\n",
    "                if mention.root.pos == 'ADJ':\n",
    "                    resolved += '\\'s'\n",
    "                                \n",
    "                # set first token to resolved value\n",
    "                tokens[mention.start] = resolved\n",
    "                \n",
    "                # set extra tokens in mention to blank\n",
    "                for i in range(mention.start+1, mention.end):\n",
    "                    tokens[i] = ''\n",
    "                \n",
    "        return ' '.join([token for token in tokens if token != ''])\n",
    "    \n",
    "    def update_people_statements(self, doc):\n",
    "        res = nlp(self.resolve_people(doc))\n",
    "        \n",
    "        for person in model.people:\n",
    "            statements = []\n",
    "            for mention in person.mentions:\n",
    "                head = mention.root.head\n",
    "                if head.pos_ == 'VERB':\n",
    "                    for statement in textacy.extract.semistructured_statements(res, person.name, head.lemma_):\n",
    "                        statements += [statement]\n",
    "            person.statements = list(set(statements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dante [the 7th Dante, he, him, Dante, his, him, him, his]\n",
      "None [school, the school]\n",
      "None [my first aid kit, it, it]\n",
      "James [James, him, him, he, he, he, He, his]\n",
      "None [the ER, the ER]\n",
      "None [his finger still had gaps, them]\n",
      "None [the doctor, him, he, his, him, he]\n",
      "James [James, him, He, he, his, he, he, he, he, his]\n",
      "None [them, them, them]\n",
      "None [the glue, the glue]\n",
      "\n",
      "Dante [the 7th Dante, he, him, Dante, his, him, him, his]\n",
      "Jodi [Jodi]\n",
      "James [James, James, him, him, he, he, he, He, his, James, him, He, he, his, he, he, he, he, his]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"November was a trying month ... on Dante had a major accident . 5 minutes before school and Dante and some friends are climbing the fence , I tell Dante it 's not a good idea and to get down . I turn back to talk to Jodi ( on of my best mom friend 's at the school ) and Dante comes to me screaming with Dante hand full of blood . I run Dante into my classroom and get Dante to the sink , as I turn on the water to clean the area the flap of Dante thumb lifts away and I see the bone . Shit . This is not something I can fix here , I grab my first aid kit and wrap it like crazy because it 's bleeding like crazy . I phone James and tell James to get to the ER as Dante is screaming and freaking out in the background as I am trying to usher James back to the car as James 's bleeding like a stuffed pig . Unfortunately in the ER I learned that my child does not take to freezing , an hour of gel freezing and James still felt the 2 needles as they went in , 15 minutes later and James felt the last 2 stitches of 8 . James needed more because James finger still had gaps , the doctor did not want to cause him anymore pain so he glued them . It was an intense and deep gash that spiraled all the way up his thumb . I was trying to stay strong for him but I did break down as he screamed and cried , I was left to emotionally drained that day . James was able to take the remainder of the day off and stay with James . James missed 2 more days of school and then had an extra long weekend due to the holiday and the pro day but for 2 weeks James could not write ( of course it was James right hand . ) 3 doctor visits later and James finally got them out full last week , the first visit the doctor wanted them in longer because of the severity . 2nd time James could only get 6 out because the glue had gotten on the last 2 stitches and James did not want to have to dig them out so we had to soak and dissolve the glue for 3 days . 3rd time the last 2 came out . Even now James 's slowly regaining James writing skills as there was some nerve damage .\""
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "model.update(doc)\n",
    "print()\n",
    "for person in model.people:\n",
    "    print(person.name, person.mentions)\n",
    "model.resolve_people(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.update_people_statements(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON Dante\n",
      "Dante - comes - to me screaming with Dante hand full of blood\n",
      "PERSON Jodi\n",
      "PERSON James\n",
      "James - was - able to take the remainder of the day off and stay with James\n",
      "James - bleeding - like a stuffed pig\n",
      "James - felt - the last 2 stitches of 8\n",
      "James - got - them out full last week , the first visit the doctor wanted them in longer because of the severity\n",
      "James - needed - more because James finger still had gaps\n",
      "James - did not want - to have to dig them out\n",
      "James - regaining - James writing skills as there was some nerve damage\n",
      "James - felt - the 2 needles as they went in , 15 minutes later and James felt the last 2 stitches of 8\n"
     ]
    }
   ],
   "source": [
    "for person in model.people:\n",
    "    print('PERSON', person.name)\n",
    "    for entity, cue, fragment in person.statements:\n",
    "        print(entity, '-', cue, '-', fragment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sheila was run over by a truck . Sheila did not see that coming . I told Sheila Sheila should take care of Sheila , but I know Sheila will just go and do Sheila thing regardless of what I say . What a conundrum ! This makes me wish I had never signed up to be friends with Sheila , although I do love Sheila .'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resolve_people(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NeuralCoref Scores to Improve Coref Res\n",
    "https://modelzoo.co/model/neuralcoref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2018 17:58:27 - INFO - event2mind_hack -   loading archive file data/event2mind.tar.gz\n",
      "12/06/2018 17:58:27 - INFO - event2mind_hack -   extracting archive file data/event2mind.tar.gz to temp dir /tmp/tmp0dlhchct\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   vocabulary.type = default\n",
      "12/06/2018 17:58:28 - INFO - allennlp.data.vocabulary -   Loading token dictionary from /tmp/tmp0dlhchct/vocabulary.\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'event2mind_hack.Model'> from params {'embedding_dropout': 0.2, 'encoder': {'bidirectional': True, 'hidden_size': 50, 'input_size': 300, 'num_layers': 1, 'type': 'gru'}, 'max_decoding_steps': 10, 'source_embedder': {'token_embedders': {'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding', 'vocab_namespace': 'source_tokens'}}}, 'target_namespace': 'target_tokens', 'type': 'event2mind'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fccd4e752e8>}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.type = event2mind\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'event2mind_hack.Event2Mind'> from params {'embedding_dropout': 0.2, 'encoder': {'bidirectional': True, 'hidden_size': 50, 'input_size': 300, 'num_layers': 1, 'type': 'gru'}, 'max_decoding_steps': 10, 'source_embedder': {'token_embedders': {'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding', 'vocab_namespace': 'source_tokens'}}}, 'target_namespace': 'target_tokens'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fccd4e752e8>}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'token_embedders': {'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding', 'vocab_namespace': 'source_tokens'}}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fccd4e752e8>}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.type = basic\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.embedder_to_indexer_map = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.allow_unmatched_keys = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 300, 'trainable': False, 'type': 'embedding', 'vocab_namespace': 'source_tokens'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fccd4e752e8>}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.type = embedding\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.num_embeddings = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.vocab_namespace = source_tokens\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.embedding_dim = 300\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.pretrained_file = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.projection_dim = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.trainable = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.padding_index = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.max_norm = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.norm_type = 2.0\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.scale_grad_by_freq = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.sparse = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.embedding_dropout = 0.2\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder'> from params {'bidirectional': True, 'hidden_size': 50, 'input_size': 300, 'num_layers': 1, 'type': 'gru'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fccd4e752e8>}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.encoder.type = gru\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.encoder.batch_first = True\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.encoder.bidirectional = True\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.encoder.hidden_size = 50\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.encoder.input_size = 300\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.encoder.num_layers = 1\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.encoder.batch_first = True\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.max_decoding_steps = 10\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.beam_size = 10\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.target_names = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.target_namespace = target_tokens\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.target_embedding_dim = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'source_token_indexers': {'tokens': {'namespace': 'source_tokens', 'type': 'single_id'}}, 'source_tokenizer': {'type': 'word', 'word_splitter': {'type': 'spacy'}}, 'target_token_indexers': {'tokens': {'namespace': 'target_tokens'}}, 'target_tokenizer': {'type': 'word'}, 'type': 'event2mind'} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.type = event2mind\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.event2mind.Event2MindDatasetReader'> from params {'source_token_indexers': {'tokens': {'namespace': 'source_tokens', 'type': 'single_id'}}, 'source_tokenizer': {'type': 'word', 'word_splitter': {'type': 'spacy'}}, 'target_token_indexers': {'tokens': {'namespace': 'target_tokens'}}, 'target_tokenizer': {'type': 'word'}} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.tokenizer.Tokenizer'> from params {'type': 'word', 'word_splitter': {'type': 'spacy'}} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.type = word\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.word_tokenizer.WordTokenizer'> from params {'word_splitter': {'type': 'spacy'}} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.word_splitter.WordSplitter'> from params {'type': 'spacy'} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.word_splitter.type = spacy\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.word_splitter.SpacyWordSplitter'> from params {} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.word_splitter.language = en_core_web_sm\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.word_splitter.pos_tags = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.word_splitter.parse = False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.word_splitter.ner = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.start_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.end_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.tokenizer.Tokenizer'> from params {'type': 'word'} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_tokenizer.type = word\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.word_tokenizer.WordTokenizer'> from params {} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_tokenizer.start_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_tokenizer.end_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.token_indexers.token_indexer.TokenIndexer'> from params {'namespace': 'source_tokens', 'type': 'single_id'} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_token_indexers.tokens.type = single_id\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer'> from params {'namespace': 'source_tokens'} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_token_indexers.tokens.namespace = source_tokens\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_token_indexers.tokens.lowercase_tokens = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_token_indexers.tokens.start_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_token_indexers.tokens.end_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.token_indexers.token_indexer.TokenIndexer'> from params {'namespace': 'target_tokens'} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_token_indexers.tokens.type = single_id\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer'> from params {'namespace': 'target_tokens'} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_token_indexers.tokens.namespace = target_tokens\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_token_indexers.tokens.lowercase_tokens = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_token_indexers.tokens.start_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_token_indexers.tokens.end_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_add_start_token = True\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.dummy_instances_for_vocab_generation = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.lazy = False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'xintent_top_k_predictions': [[4, 3, 3, 3],\n",
       "  [684, 3, 3, 3],\n",
       "  [323, 3, 3, 3],\n",
       "  [282, 3, 3, 3],\n",
       "  [255, 3, 3, 3],\n",
       "  [153, 229, 3, 3],\n",
       "  [13, 175, 3, 3],\n",
       "  [44, 251, 3, 3],\n",
       "  [13, 267, 3, 3],\n",
       "  [211, 32, 80, 3]],\n",
       " 'xintent_top_k_log_probabilities': [-1.128340721130371,\n",
       "  -4.298313617706299,\n",
       "  -4.499514579772949,\n",
       "  -4.616389751434326,\n",
       "  -4.652942657470703,\n",
       "  -5.256786823272705,\n",
       "  -5.543419361114502,\n",
       "  -6.178347587585449,\n",
       "  -6.431229114532471,\n",
       "  -9.508707046508789],\n",
       " 'xreact_top_k_predictions': [[54, 3],\n",
       "  [70, 3],\n",
       "  [53, 3],\n",
       "  [109, 3],\n",
       "  [5, 3],\n",
       "  [73, 3],\n",
       "  [11, 3],\n",
       "  [92, 3],\n",
       "  [25, 3],\n",
       "  [63, 3]],\n",
       " 'xreact_top_k_log_probabilities': [-3.147449016571045,\n",
       "  -3.159241199493408,\n",
       "  -3.1768059730529785,\n",
       "  -3.2743079662323,\n",
       "  -3.3373990058898926,\n",
       "  -3.516559362411499,\n",
       "  -3.596619129180908,\n",
       "  -3.8112082481384277,\n",
       "  -3.8679120540618896,\n",
       "  -3.9374184608459473],\n",
       " 'oreact_top_k_predictions': [[4, 3],\n",
       "  [63, 3],\n",
       "  [36, 3],\n",
       "  [89, 3],\n",
       "  [83, 3],\n",
       "  [91, 3],\n",
       "  [138, 3],\n",
       "  [53, 3],\n",
       "  [92, 3],\n",
       "  [5, 3]],\n",
       " 'oreact_top_k_log_probabilities': [-1.067413330078125,\n",
       "  -2.62384033203125,\n",
       "  -3.9126994609832764,\n",
       "  -3.979201316833496,\n",
       "  -4.0716962814331055,\n",
       "  -4.296855926513672,\n",
       "  -4.327937602996826,\n",
       "  -4.378903388977051,\n",
       "  -4.453375339508057,\n",
       "  -4.506431579589844],\n",
       " 'xintent_top_k_predicted_tokens': [['none'],\n",
       "  ['annoying'],\n",
       "  ['noticed'],\n",
       "  ['communicate'],\n",
       "  ['heard'],\n",
       "  ['express', 'anger'],\n",
       "  ['get', 'attention'],\n",
       "  ['show', 'affection'],\n",
       "  ['get', 'revenge'],\n",
       "  ['let', 'someone', 'know']],\n",
       " 'xreact_top_k_predicted_tokens': [['upset'],\n",
       "  ['worried'],\n",
       "  ['nervous'],\n",
       "  ['curious'],\n",
       "  ['happy'],\n",
       "  ['scared'],\n",
       "  ['satisfied'],\n",
       "  ['anxious'],\n",
       "  ['relieved'],\n",
       "  ['annoyed']],\n",
       " 'oreact_top_k_predicted_tokens': [['none'],\n",
       "  ['annoyed'],\n",
       "  ['angry'],\n",
       "  ['informed'],\n",
       "  ['surprised'],\n",
       "  ['interested'],\n",
       "  ['frustrated'],\n",
       "  ['nervous'],\n",
       "  ['anxious'],\n",
       "  ['happy']]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from event2mind_hack import load_event2mind_archive\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "\n",
    "archive = load_event2mind_archive('data/event2mind.tar.gz')\n",
    "predictor = Predictor.from_archive(archive)\n",
    "predictor.predict(\n",
    "  source=\"PersonX drops a hint\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36787944117144233"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.exp(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>p_log</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[none]</td>\n",
       "      <td>-1.128341</td>\n",
       "      <td>0.323570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[annoying]</td>\n",
       "      <td>-4.298314</td>\n",
       "      <td>0.013591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[noticed]</td>\n",
       "      <td>-4.499515</td>\n",
       "      <td>0.011114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[communicate]</td>\n",
       "      <td>-4.616390</td>\n",
       "      <td>0.009888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[heard]</td>\n",
       "      <td>-4.652943</td>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[express, anger]</td>\n",
       "      <td>-5.256787</td>\n",
       "      <td>0.005212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[get, attention]</td>\n",
       "      <td>-5.543419</td>\n",
       "      <td>0.003913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[show, affection]</td>\n",
       "      <td>-6.178348</td>\n",
       "      <td>0.002074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[get, revenge]</td>\n",
       "      <td>-6.431229</td>\n",
       "      <td>0.001610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[let, someone, know]</td>\n",
       "      <td>-9.508707</td>\n",
       "      <td>0.000074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tokens     p_log         p\n",
       "0                [none] -1.128341  0.323570\n",
       "1            [annoying] -4.298314  0.013591\n",
       "2             [noticed] -4.499515  0.011114\n",
       "3         [communicate] -4.616390  0.009888\n",
       "4               [heard] -4.652943  0.009534\n",
       "5      [express, anger] -5.256787  0.005212\n",
       "6      [get, attention] -5.543419  0.003913\n",
       "7     [show, affection] -6.178348  0.002074\n",
       "8        [get, revenge] -6.431229  0.001610\n",
       "9  [let, someone, know] -9.508707  0.000074"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xintent = pd.DataFrame({\n",
    "    'tokens': prediction['xintent_top_k_predicted_tokens'],\n",
    "    'p_log': prediction['xintent_top_k_log_probabilities']\n",
    "})\n",
    "xintent['p'] = xintent['p_log'].apply(math.exp)\n",
    "xintent.sort_values(by='p', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>p_log</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[upset]</td>\n",
       "      <td>-3.147449</td>\n",
       "      <td>0.042962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[worried]</td>\n",
       "      <td>-3.159241</td>\n",
       "      <td>0.042458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[nervous]</td>\n",
       "      <td>-3.176806</td>\n",
       "      <td>0.041719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[curious]</td>\n",
       "      <td>-3.274308</td>\n",
       "      <td>0.037843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[happy]</td>\n",
       "      <td>-3.337399</td>\n",
       "      <td>0.035529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[scared]</td>\n",
       "      <td>-3.516559</td>\n",
       "      <td>0.029701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[satisfied]</td>\n",
       "      <td>-3.596619</td>\n",
       "      <td>0.027416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[anxious]</td>\n",
       "      <td>-3.811208</td>\n",
       "      <td>0.022121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[relieved]</td>\n",
       "      <td>-3.867912</td>\n",
       "      <td>0.020902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[annoyed]</td>\n",
       "      <td>-3.937418</td>\n",
       "      <td>0.019498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        tokens     p_log         p\n",
       "0      [upset] -3.147449  0.042962\n",
       "1    [worried] -3.159241  0.042458\n",
       "2    [nervous] -3.176806  0.041719\n",
       "3    [curious] -3.274308  0.037843\n",
       "4      [happy] -3.337399  0.035529\n",
       "5     [scared] -3.516559  0.029701\n",
       "6  [satisfied] -3.596619  0.027416\n",
       "7    [anxious] -3.811208  0.022121\n",
       "8   [relieved] -3.867912  0.020902\n",
       "9    [annoyed] -3.937418  0.019498"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xreact = pd.DataFrame({\n",
    "    'tokens': prediction['xreact_top_k_predicted_tokens'],\n",
    "    'p_log': prediction['xreact_top_k_log_probabilities']\n",
    "})\n",
    "xreact['p'] = xreact['p_log'].apply(math.exp)\n",
    "xreact.sort_values(by='p', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>p_log</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[none]</td>\n",
       "      <td>-1.067413</td>\n",
       "      <td>0.343897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[annoyed]</td>\n",
       "      <td>-2.623840</td>\n",
       "      <td>0.072524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[angry]</td>\n",
       "      <td>-3.912699</td>\n",
       "      <td>0.019986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[informed]</td>\n",
       "      <td>-3.979201</td>\n",
       "      <td>0.018701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[surprised]</td>\n",
       "      <td>-4.071696</td>\n",
       "      <td>0.017048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[interested]</td>\n",
       "      <td>-4.296856</td>\n",
       "      <td>0.013611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[frustrated]</td>\n",
       "      <td>-4.327938</td>\n",
       "      <td>0.013195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[nervous]</td>\n",
       "      <td>-4.378903</td>\n",
       "      <td>0.012539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[anxious]</td>\n",
       "      <td>-4.453375</td>\n",
       "      <td>0.011639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[happy]</td>\n",
       "      <td>-4.506432</td>\n",
       "      <td>0.011038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tokens     p_log         p\n",
       "0        [none] -1.067413  0.343897\n",
       "1     [annoyed] -2.623840  0.072524\n",
       "2       [angry] -3.912699  0.019986\n",
       "3    [informed] -3.979201  0.018701\n",
       "4   [surprised] -4.071696  0.017048\n",
       "5  [interested] -4.296856  0.013611\n",
       "6  [frustrated] -4.327938  0.013195\n",
       "7     [nervous] -4.378903  0.012539\n",
       "8     [anxious] -4.453375  0.011639\n",
       "9       [happy] -4.506432  0.011038"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oreact = pd.DataFrame({\n",
    "    'tokens': prediction['oreact_top_k_predicted_tokens'],\n",
    "    'p_log': prediction['oreact_top_k_log_probabilities']\n",
    "})\n",
    "oreact['p'] = oreact['p_log'].apply(math.exp)\n",
    "oreact.sort_values(by='p', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
