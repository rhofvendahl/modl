{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_coref_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'November was a trying month… on the 7th Dante had a major accident. 5 minutes before school and he and some friends are climbing the fence, I tell him it’s not a good idea and to get down. I turn back to talk to Jodi (on of my best mom friend’s at the school) and Dante comes to me screaming with his hand full of blood. I run him into my classroom and get him to the sink, as I turn on the water to clean the area the flap of his thumb lifts away and I see the bone. Shit. This isn’t something I can fix here, I grab my first aid kit and wrap it like crazy because it’s bleeding like crazy. I phone James and tell him to get to the ER as Dante is screaming and freaking out in the background as I’m trying to usher him back to the car as he’s bleeding like a stuffed pig. Unfortunately in the ER I learned that my child doesn’t take to freezing, an hour of gel freezing and he still felt the 2 needles as they went in, 15 minutes later and he felt the last 2 stitches of 8. He needed more because his finger still had gaps, the doctor didn’t want to cause him anymore pain so he glued them. It was an intense and deep gash that spiraled all the way up his thumb. I was trying to stay strong for him but I did break down as he screamed and cried, I was left to emotionally drained that day. James was able to take the remainder of the day off and stay with him. He missed 2 more days of school and then had an extra long weekend due to the holiday and the pro day but for 2 weeks he couldn’t write (of course it was his right hand.) 3 doctor visits later and he finally got them out full last week, the first visit the doctor wanted them in longer because of the severity. 2nd time he could only get 6 out because the glue had gotten on the last 2 stitches and he didn’t want to have to dig them out so we had to soak and dissolve the glue for 3 days. 3rd time the last 2 came out.  Even now he’s slowly regaining his writing skills as there was some nerve damage.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'So I have had a good day today. I found out we got the other half of our funding for my travel grant, which paid for my friend to come with me. So that’s good, she and I will both get some money back. I took my dogs to the pet store so my girl dog could get a new collar, but she wanted to beat everyone up. This is an ongoing issue with her. She’s so little and cute too but damn she acts like she’s gonna go for the jugular with everyone she doesn’t know! She did end up with a cute new collar tho, it has pineapples on it. I went to the dentist and she’s happy with my Invisalign progress. We have three more trays and then she does an impression to make sure my teeth are where they need to be before they get the rest of the trays. YAY! And I don’t have to make another payment until closer to the end of my treatment. I had some work emails with the festival, and Jessie was bringing up some important points, and one of our potential artists was too expensive to work with, so Mutual Friend was asking for names for some other people we could work with. So I suggested like, three artists, and Jessie actually liked the idea of one of them doing it. Which is nice. I notice she is very encouraging at whatever I contribute to our collective. It’s sweet. I kind of know this is like, the only link we have with each other right now besides social media, so it seems like she’s trying to make sure I know she still wants me to be involved and doesn’t have bad feelings for me. And there was a short period when I was seriously thinking of leaving the collective and not working with this festival anymore. I was so sad, and felt so upset, and didn’t know what to do about Jessie. It felt really close to me throwing in the towel. But I hung on through the festival and it doesn’t seem so bad from this viewpoint now with more time that has passed. And we have been gentle, if reserved, with each other. I mean her last personal email to me however many weeks ago wasn’t very nice. But it seems like we’ve been able to put it aside for work reasons. I dunno. I still feel like if anything was gonna get mended between us, she would need to make the first moves on that. I really don’t want to try reaching out and get rejected even as a friend again. I miss her though. And sometimes I think she misses me. But I don’t want to approach her assuming we both miss each other and have her turn it on me again and make out like all these things are all in my head. I don’t know about that butch I went on a date with last night. I feel more of a friend vibe from her, than a romantic one. I can’t help it, I am just not attracted to butches. And I don’t know how to flirt with them. And I don’t think of them in a sexy way. But I WOULD like another butch buddy. I mean yeah maybe Femmes do play games, or maybe I just chased all the wrong Femmes. Maybe I’ll just leave this and not think about it much until I get back to town in January.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'well, i tried to get an x-ray of my neck today but, when i got to the medical center and stood in line to wait to get checked in, i was told my doctor hadn’t sent over the orders for it! and the thing is he said he had already sent it when i asked him if i needed anything to get it done. i don’t like being lied to. so, since i have to go to the medical center thursday morning for a consultation for p/t, i’ll just go on back over and get the xray. the lady there said monday and tuesday are really busy days and thursday would be much better.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I can’t help but feel annoyed, angry, disheartened, let down… and yet in another way I want to say “you don’t deserve to know him.” Dante’s growing into such an amazing child and yet it seems our family dwindles like crazy, he brings up James’ sister “aunty Tammy” and asks why we never see her. I say she’s busy because she has 2 of her own little boys, but that’s not the case. James’ sister had this dream of being an amazing aunt to  Dante and she has done nothing to be in his life. Birthday gifts, Christmas, there’s no communication or her ever asking about him, not that I even speak to  her much but she just doesn’t care to be an active part in his world which pisses me off to no end. James’ mother couldn’t get herself clean to stay in his life… she’s non existent to him. It boils my blood because when he was born she was so proud, got clean for a while, and then couldn’t hack it (ended up visiting and left her morphine out where our very smart 2 year old brought us a handful of pills and asked if they  were candy.) That was the last time she saw him and her memory has since been forgotten. You couldn’t even get clean to be in your grandchild’s life? She was always a pathetic excuse for a mother, James’ childhood simply enrages me, the idea of a child living the way he did because of her ways makes me sick. She doesn’t deserve to know my child. My own brother “uncle Jason” is seen in passing about 5 times a year, he’s good with Dante, pleasant enough considering my brother has so many anger issues. He’s also a drug addict so it’s not like I would ever allow him time alone with Dante, not that he’d ever want to spend time with him. Christmas is coming up and in a way it’s bittersweet. My half cousin Tianna’s two girls (Stella and Piper)  have two sets of everything, tons of aunts and uncles, and they have a huge loving family unit. Dante doesn’t have that, yes his grandparents love him like crazy but his family connection is like mine. When I was young I only had one set of grandparents, my dad was adopted and his mother wasn’t around at all… in a way my grandparents adopted him as well when he married my young at the age of 18. I had my uncle George and my Omi and Opi and my mom and dad and my brother. I remember when George met Denise and I met Tianna (Denise’s child from her first marriage.) I  remember the day that I learned that George had proposed to Denise and that they were getting married, I cried and my mom thought I was happy. I wasn’t happy, I was devastated that suddenly I had to share my family (horrible to thing to cry about right?) Tianna already had 2 sets of grandparents, she had tons of aunts, and now she was getting my uncle whom I loved and thought was the coolest guy around as a dad. I was so angry. I never really got to  meet Tianna’s dad’s side of the family, I met her grandparents a few times but they never remembered my name which really hurt and annoyed me. I joined soccer and Tianna’s dad was the coach, he wasn’t nice to me which drove the wedge deeper. It hurts… it hurts that my child has the same issue that I did although I really don’t think he’s realized that he’s different. His “grandpa Morgan” isn’t his real grandpa, more like a man who took on his father and tried to “raise” him to be a man, he obviously didn’t stay with James’ mom but he’s still in our life and I’m grateful  that he’s there. Unfortunately he’s not around much, we see him 2-3 times a year because he lives in Golden. Uncle Adam is James’ childhood best friend, a good guy  who’s more of a businessman who  lives to the beat of his own drum and wouldn’t know what to do with  a child if his  life depended on it. He’s around but again it’s only a few times a year when he visits from Calgary. I want to say sometimes you get to choose your own family, but even the family I chose for him and thought would be around forever, the people who were there when he was born, grew, shared so many moments with are no longer around. They don’t seem to care either, it’s not like “aunty Kat” ever talks to me or asks about him. Seems like moving meant the end of our friendship and our “family ties.”'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Sheila was run over by a truck. She herself didn't see that coming. I told her she should take care of herself, but I know she'll just go and do her thing regardless of what I say. What a conundrum! This makes me wish I had never signed up to be friends with her, although I do love the girl.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = textacy.preprocess.normalize_whitespace(text)\n",
    "preprocessed = textacy.preprocess.preprocess_text(preprocessed, fix_unicode=True, no_contractions=True, no_accents=True)\n",
    "doc = nlp(preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Extract People"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "November DATE\n",
      "a trying month DATE\n",
      "7th ORDINAL\n",
      "Dante PERSON\n",
      "5 minutes TIME\n",
      "Jodi PERSON\n",
      "Dante PERSON\n",
      "first ORDINAL\n",
      "James PERSON\n",
      "ER ORG\n",
      "Dante PERSON\n",
      "ER ORG\n",
      "15 minutes later TIME\n",
      "2 CARDINAL\n",
      "8 CARDINAL\n",
      "James PERSON\n",
      "the day DATE\n",
      "2 more days DATE\n",
      "an extra long weekend TIME\n",
      "2 weeks DATE\n",
      "3 CARDINAL\n",
      "first ORDINAL\n",
      "2nd ORDINAL\n",
      "6 CARDINAL\n",
      "2 CARDINAL\n",
      "3 days DATE\n",
      "3rd ORDINAL\n",
      "the last 2 DATE\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Dante', 'James', 'Jodi'}"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people = set([ent.text for ent in doc.ents if ent.label_ == 'PERSON'])\n",
    "people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "textacy.text_utils.keyword_in_context(doc.text, 'Christmas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dante pobj on\n",
      "Jodi pobj to\n",
      "Dante nsubj comes\n",
      "James ROOT James\n",
      "Dante nsubj screaming\n",
      "James nsubj was\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    if ent.label_ == 'PERSON':\n",
    "        token = ent.root\n",
    "        print(ent.text, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coreference Resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AllenNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/07/2018 21:48:33 - INFO - allennlp.models.archival -   loading archive file data/coref-model.tar.gz\n",
      "12/07/2018 21:48:33 - INFO - allennlp.models.archival -   extracting archive file data/coref-model.tar.gz to temp dir /tmp/tmp7nf4moi3\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   type = default\n",
      "12/07/2018 21:48:34 - INFO - allennlp.data.vocabulary -   Loading token dictionary from /tmp/tmp7nf4moi3/vocabulary.\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.model.Model'> from params {'antecedent_feedforward': {'activations': 'relu', 'dropout': 0.2, 'hidden_dims': 150, 'input_dim': 3680, 'num_layers': 2}, 'context_layer': {'bidirectional': True, 'dropout': 0.2, 'hidden_size': 200, 'input_size': 400, 'num_layers': 1, 'type': 'lstm'}, 'feature_size': 20, 'initializer': [['.*linear_layers.*weight', {'type': 'xavier_normal'}], ['.*scorer._module.*weight', {'type': 'xavier_normal'}], ['.*_global_attention._module.weight', {'type': 'xavier_normal'}], ['_distance_embedding.weight', {'type': 'xavier_normal'}], ['_span_width_embedding.weight', {'type': 'xavier_normal'}], ['_context_layer._module.weight_ih.*', {'type': 'xavier_normal'}], ['_context_layer._module.weight_hh.*', {'type': 'orthogonal'}]], 'lexical_dropout': 0.5, 'max_antecedents': 150, 'max_span_width': 10, 'mention_feedforward': {'activations': 'relu', 'dropout': 0.2, 'hidden_dims': 150, 'input_dim': 1220, 'num_layers': 2}, 'spans_per_word': 0.4, 'text_field_embedder': {'token_characters': {'embedding': {'embedding_dim': 16, 'num_embeddings': 262}, 'encoder': {'embedding_dim': 16, 'ngram_filter_sizes': [5], 'num_filters': 100, 'type': 'cnn'}, 'type': 'character_encoding'}, 'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'}}, 'type': 'coref'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7f56fcef0c88>}\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.type = coref\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.coreference_resolution.coref.CoreferenceResolver'> from params {'antecedent_feedforward': {'activations': 'relu', 'dropout': 0.2, 'hidden_dims': 150, 'input_dim': 3680, 'num_layers': 2}, 'context_layer': {'bidirectional': True, 'dropout': 0.2, 'hidden_size': 200, 'input_size': 400, 'num_layers': 1, 'type': 'lstm'}, 'feature_size': 20, 'initializer': [['.*linear_layers.*weight', {'type': 'xavier_normal'}], ['.*scorer._module.*weight', {'type': 'xavier_normal'}], ['.*_global_attention._module.weight', {'type': 'xavier_normal'}], ['_distance_embedding.weight', {'type': 'xavier_normal'}], ['_span_width_embedding.weight', {'type': 'xavier_normal'}], ['_context_layer._module.weight_ih.*', {'type': 'xavier_normal'}], ['_context_layer._module.weight_hh.*', {'type': 'orthogonal'}]], 'lexical_dropout': 0.5, 'max_antecedents': 150, 'max_span_width': 10, 'mention_feedforward': {'activations': 'relu', 'dropout': 0.2, 'hidden_dims': 150, 'input_dim': 1220, 'num_layers': 2}, 'spans_per_word': 0.4, 'text_field_embedder': {'token_characters': {'embedding': {'embedding_dim': 16, 'num_embeddings': 262}, 'encoder': {'embedding_dim': 16, 'ngram_filter_sizes': [5], 'num_filters': 100, 'type': 'cnn'}, 'type': 'character_encoding'}, 'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'}}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7f56fcef0c88>}\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'token_characters': {'embedding': {'embedding_dim': 16, 'num_embeddings': 262}, 'encoder': {'embedding_dim': 16, 'ngram_filter_sizes': [5], 'num_filters': 100, 'type': 'cnn'}, 'type': 'character_encoding'}, 'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7f56fcef0c88>}\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.text_field_embedder.type = basic\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.text_field_embedder.embedder_to_indexer_map = None\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.text_field_embedder.allow_unmatched_keys = False\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders = None\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding': {'embedding_dim': 16, 'num_embeddings': 262}, 'encoder': {'embedding_dim': 16, 'ngram_filter_sizes': [5], 'num_filters': 100, 'type': 'cnn'}, 'type': 'character_encoding'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7f56fcef0c88>}\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.type = character_encoding\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.num_embeddings = 262\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.vocab_namespace = token_characters\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.embedding_dim = 16\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.pretrained_file = None\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.projection_dim = None\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.trainable = True\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.padding_index = None\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.max_norm = None\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.norm_type = 2.0\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.scale_grad_by_freq = False\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.sparse = False\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder'> from params {'embedding_dim': 16, 'ngram_filter_sizes': [5], 'num_filters': 100, 'type': 'cnn'} and extras {}\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.type = cnn\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.seq2vec_encoders.cnn_encoder.CnnEncoder'> from params {'embedding_dim': 16, 'ngram_filter_sizes': [5], 'num_filters': 100} and extras {}\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.embedding_dim = 16\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.num_filters = 100\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.ngram_filter_sizes = [5]\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.output_dim = None\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.dropout = 0.0\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7f56fcef0c88>}\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.type = embedding\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.num_embeddings = None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.vocab_namespace = tokens\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.embedding_dim = 300\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.pretrained_file = None\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.projection_dim = None\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.trainable = False\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.padding_index = None\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.max_norm = None\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.norm_type = 2.0\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.scale_grad_by_freq = False\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.sparse = False\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'bidirectional': True, 'dropout': 0.2, 'hidden_size': 200, 'input_size': 400, 'num_layers': 1, 'type': 'lstm'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7f56fcef0c88>}\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.context_layer.type = lstm\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.context_layer.batch_first = True\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.context_layer.stateful = False\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.context_layer.bidirectional = True\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.context_layer.dropout = 0.2\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.context_layer.hidden_size = 200\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.context_layer.input_size = 400\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.context_layer.num_layers = 1\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.context_layer.batch_first = True\n",
      "/home/russell/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.mention_feedforward.input_dim = 1220\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.mention_feedforward.num_layers = 2\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.mention_feedforward.hidden_dims = 150\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.mention_feedforward.activations = relu\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.mention_feedforward.dropout = 0.2\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.antecedent_feedforward.input_dim = 3680\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.antecedent_feedforward.num_layers = 2\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.antecedent_feedforward.hidden_dims = 150\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.antecedent_feedforward.activations = relu\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.antecedent_feedforward.dropout = 0.2\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.feature_size = 20\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.max_span_width = 10\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.spans_per_word = 0.4\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.max_antecedents = 150\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.lexical_dropout = 0.5\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.initializer = [['.*linear_layers.*weight', {'type': 'xavier_normal'}], ['.*scorer._module.*weight', {'type': 'xavier_normal'}], ['.*_global_attention._module.weight', {'type': 'xavier_normal'}], ['_distance_embedding.weight', {'type': 'xavier_normal'}], ['_span_width_embedding.weight', {'type': 'xavier_normal'}], ['_context_layer._module.weight_ih.*', {'type': 'xavier_normal'}], ['_context_layer._module.weight_hh.*', {'type': 'orthogonal'}]]\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.initializer.list.list.type = xavier_normal\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.initializer.list.list.type = xavier_normal\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.initializer.list.list.type = xavier_normal\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.initializer.list.list.type = xavier_normal\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.initializer.list.list.type = xavier_normal\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.initializer.list.list.type = xavier_normal\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   model.initializer.list.list.type = orthogonal\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/07/2018 21:48:34 - INFO - allennlp.nn.initializers -   Initializing parameters\n",
      "12/07/2018 21:48:34 - INFO - allennlp.nn.initializers -   Initializing _context_layer._module.weight_ih_l0 using _context_layer._module.weight_ih.* intitializer\n",
      "12/07/2018 21:48:34 - INFO - allennlp.nn.initializers -   Initializing _context_layer._module.weight_hh_l0 using _context_layer._module.weight_hh.* intitializer\n",
      "12/07/2018 21:48:34 - INFO - allennlp.nn.initializers -   Initializing _context_layer._module.weight_ih_l0_reverse using _context_layer._module.weight_ih.* intitializer\n",
      "12/07/2018 21:48:34 - INFO - allennlp.nn.initializers -   Initializing _context_layer._module.weight_hh_l0_reverse using _context_layer._module.weight_hh.* intitializer\n",
      "12/07/2018 21:48:34 - INFO - allennlp.nn.initializers -   Initializing _antecedent_feedforward._module._linear_layers.0.weight using .*linear_layers.*weight intitializer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/07/2018 21:48:34 - INFO - allennlp.nn.initializers -   Initializing _antecedent_feedforward._module._linear_layers.1.weight using .*linear_layers.*weight intitializer\n",
      "12/07/2018 21:48:34 - INFO - allennlp.nn.initializers -   Initializing _mention_pruner._scorer.0._module._linear_layers.0.weight using .*linear_layers.*weight intitializer\n",
      "12/07/2018 21:48:34 - INFO - allennlp.nn.initializers -   Initializing _mention_pruner._scorer.0._module._linear_layers.1.weight using .*linear_layers.*weight intitializer\n",
      "12/07/2018 21:48:34 - INFO - allennlp.nn.initializers -   Initializing _antecedent_scorer._module.weight using .*scorer._module.*weight intitializer\n",
      "12/07/2018 21:48:34 - INFO - allennlp.nn.initializers -   Initializing _endpoint_span_extractor._span_width_embedding.weight using _span_width_embedding.weight intitializer\n",
      "12/07/2018 21:48:34 - INFO - allennlp.nn.initializers -   Initializing _attentive_span_extractor._global_attention._module.weight using .*_global_attention._module.weight intitializer\n",
      "12/07/2018 21:48:34 - INFO - allennlp.nn.initializers -   Initializing _distance_embedding.weight using _distance_embedding.weight intitializer\n",
      "12/07/2018 21:48:34 - INFO - allennlp.nn.initializers -   Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "12/07/2018 21:48:34 - INFO - allennlp.nn.initializers -      _antecedent_feedforward._module._linear_layers.0.bias\n",
      "12/07/2018 21:48:34 - INFO - allennlp.nn.initializers -      _antecedent_feedforward._module._linear_layers.1.bias\n",
      "12/07/2018 21:48:34 - INFO - allennlp.nn.initializers -      _antecedent_scorer._module.bias\n",
      "12/07/2018 21:48:34 - INFO - allennlp.nn.initializers -      _attentive_span_extractor._global_attention._module.bias\n",
      "12/07/2018 21:48:34 - INFO - allennlp.nn.initializers -      _context_layer._module.bias_hh_l0\n",
      "12/07/2018 21:48:34 - INFO - allennlp.nn.initializers -      _context_layer._module.bias_hh_l0_reverse\n",
      "12/07/2018 21:48:34 - INFO - allennlp.nn.initializers -      _context_layer._module.bias_ih_l0\n",
      "12/07/2018 21:48:34 - INFO - allennlp.nn.initializers -      _context_layer._module.bias_ih_l0_reverse\n",
      "12/07/2018 21:48:34 - INFO - allennlp.nn.initializers -      _mention_pruner._scorer.0._module._linear_layers.0.bias\n",
      "12/07/2018 21:48:34 - INFO - allennlp.nn.initializers -      _mention_pruner._scorer.0._module._linear_layers.1.bias\n",
      "12/07/2018 21:48:34 - INFO - allennlp.nn.initializers -      _mention_pruner._scorer.1._module.bias\n",
      "12/07/2018 21:48:34 - INFO - allennlp.nn.initializers -      _mention_pruner._scorer.1._module.weight\n",
      "12/07/2018 21:48:34 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_token_characters._embedding._module.weight\n",
      "12/07/2018 21:48:34 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.bias\n",
      "12/07/2018 21:48:34 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.weight\n",
      "12/07/2018 21:48:34 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_tokens.weight\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'max_span_width': 10, 'token_indexers': {'token_characters': {'type': 'characters'}, 'tokens': {'lowercase_tokens': False, 'type': 'single_id'}}, 'type': 'coref'} and extras {}\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   dataset_reader.type = coref\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.coreference_resolution.conll.ConllCorefReader'> from params {'max_span_width': 10, 'token_indexers': {'token_characters': {'type': 'characters'}, 'tokens': {'lowercase_tokens': False, 'type': 'single_id'}}} and extras {}\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   dataset_reader.max_span_width = 10\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.token_indexers.token_indexer.TokenIndexer'> from params {'type': 'characters'} and extras {}\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   dataset_reader.token_indexers.token_characters.type = characters\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.token_indexers.token_characters_indexer.TokenCharactersIndexer'> from params {} and extras {}\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   dataset_reader.token_indexers.token_characters.namespace = token_characters\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   dataset_reader.token_indexers.token_characters.start_tokens = None\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   dataset_reader.token_indexers.token_characters.end_tokens = None\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   dataset_reader.token_indexers.token_characters.min_padding_length = 0\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.token_indexers.token_indexer.TokenIndexer'> from params {'lowercase_tokens': False, 'type': 'single_id'} and extras {}\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.type = single_id\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer'> from params {'lowercase_tokens': False} and extras {}\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.namespace = tokens\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.lowercase_tokens = False\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.start_tokens = None\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.end_tokens = None\n",
      "12/07/2018 21:48:34 - INFO - allennlp.common.params -   dataset_reader.lazy = False\n"
     ]
    }
   ],
   "source": [
    "from allennlp.models.archival import load_archive\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "\n",
    "archive = load_archive('data/coref-model.tar.gz')\n",
    "predictor = Predictor.from_archive(archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "coref = predictor.predict(document = doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[November, he, him, him, him, his]\n",
      "[I, I, my, me, I, my, I, I, I, I, my, I, I, I, my, I, I, I]\n",
      "[Dante, Jodi, Dante, his, the ER, Dante, the ER]\n",
      "[my first aid kit, it, it]\n",
      "[James, him, him, he, James, him, He, he, his, he, he, he, he, his]\n",
      "[my child, he, he, He, his, him, his, him, he]\n",
      "[the 2 needles, they, them]\n",
      "[the doctor, he, the doctor]\n",
      "[his finger, his thumb]\n",
      "[3 doctor visits, them, them, them]\n",
      "[the last 2 stitches of 8, the last 2 stitches]\n",
      "[the glue, the glue]\n"
     ]
    }
   ],
   "source": [
    "for cluster in coref['clusters']:\n",
    "    spans = [doc[first:last+1] for first, last in cluster]\n",
    "    print(spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[November(0)] was a trying month ... on the 7th [Dante(2)] had a major accident . 5 minutes before school and [he(0)] and some friends are climbing the fence , [I(1)] tell [him(0)] it 's not a good idea and to get down . [I(1)] turn back to talk to [Jodi(2)] ( on of [my(1)] best mom friend 's at the school ) and [Dante(2)] comes to [me(1)] screaming with [his(2)] hand full of blood . [I(1)] run [him(0)] into [my(1)] classroom and get [him(0)] to the sink , as [I(1)] turn on the water to clean the area the flap of [his(0)] thumb lifts away and [I(1)] see the bone . Shit . This is not something [I(1)] can fix here , [I(1)] grab [my first aid kit(3)] and wrap [it(3)] like crazy because [it(3)] 's bleeding like crazy . [I(1)] phone [James(4)] and tell [him(4)] to get to [the ER(2)] as [Dante(2)] is screaming and freaking out in the background as [I(1)] am trying to usher [him(4)] back to the car as [he(4)] 's bleeding like a stuffed pig . Unfortunately in [the ER(2)] [I(1)] learned that [my child(5)] does not take to freezing , an hour of gel freezing and [he(5)] still felt [the 2 needles(6)] as [they(6)] went in , 15 minutes later and [he(5)] felt [the last 2 stitches of 8(10)] . [He(5)] needed more because [his finger(8)] still had gaps , [the doctor(7)] did not want to cause [him(5)] anymore pain so [he(7)] glued [them(6)] . It was an intense and deep gash that spiraled all the way up [his thumb(8)] . [I(1)] was trying to stay strong for [him(5)] but [I(1)] did break down as [he(5)] screamed and cried , [I(1)] was left to emotionally drained that day . [James(4)] was able to take the remainder of the day off and stay with [him(4)] . [He(4)] missed 2 more days of school and then had an extra long weekend due to the holiday and the pro day but for 2 weeks [he(4)] could not write ( of course it was [his(4)] right hand . ) [3 doctor visits(9)] later and [he(4)] finally got [them(9)] out full last week , the first visit [the doctor(7)] wanted [them(9)] in longer because of the severity . 2nd time [he(4)] could only get 6 out because [the glue(11)] had gotten on [the last 2 stitches(10)] and [he(4)] did not want to have to dig [them(9)] out so we had to soak and dissolve [the glue(11)] for 3 days . 3rd time the last 2 came out . Even now [he(4)] 's slowly regaining [his(4)] writing skills as there was some nerve damage .\""
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def coref_resolved(doc, coref):\n",
    "    resolved = [token.text for token in doc]\n",
    "    for token in doc:\n",
    "        cluster_n = 0\n",
    "        for cluster in coref['clusters']:\n",
    "            for first, last in cluster:\n",
    "                span = doc[first:last+1]\n",
    "                if first == last:\n",
    "                    resolved[first] = '[' + doc[first].text + '(' + str(cluster_n) + ')]'\n",
    "                else:\n",
    "                    resolved[first] = '[' + doc[first].text\n",
    "                    resolved[last] = doc[last].text + '(' + str(cluster_n) + ')]'\n",
    "\n",
    "            cluster_n += 1\n",
    "    return ' '.join(resolved)\n",
    "coref_resolved(doc, coref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NeuralCoref\n",
    "(maybe not quite as good? maybe it is. certainly easier.)\n",
    "\n",
    "(I think I'll use this together with named entity recognititon to ID unique people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc._.has_coref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[the 7th Dante: [the 7th Dante, he, him, Dante, his, him, him, his],\n",
       " school: [school, the school],\n",
       " my first aid kit: [my first aid kit, it, it],\n",
       " James: [James, him, him, he, he, he, He, his],\n",
       " the ER: [the ER, the ER],\n",
       " his finger still had gaps: [his finger still had gaps, them],\n",
       " the doctor: [the doctor, him, he, his, him, he],\n",
       " James: [James, him, He, he, his, he, he, he, he, his],\n",
       " them: [them, them, them],\n",
       " the glue: [the glue, the glue]]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc._.coref_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"November was a trying month... on the 7th Dante had a major accident. 5 minutes before school and he and some friends are climbing the fence, I tell him it's not a good idea and to get down. I turn back to talk to Jodi (on of my best mom friend's at the school) and Dante comes to me screaming with his hand full of blood. I run him into my classroom and get him to the sink, as I turn on the water to clean the area the flap of his thumb lifts away and I see the bone. Shit. This is not something I can fix here, I grab my first aid kit and wrap it like crazy because it's bleeding like crazy. I phone James and tell him to get to the ER as Dante is screaming and freaking out in the background as I am trying to usher him back to the car as he's bleeding like a stuffed pig. Unfortunately in the ER I learned that my child does not take to freezing, an hour of gel freezing and he still felt the 2 needles as they went in, 15 minutes later and he felt the last 2 stitches of 8. He needed more because his finger still had gaps, the doctor did not want to cause him anymore pain so he glued them. It was an intense and deep gash that spiraled all the way up his thumb. I was trying to stay strong for him but I did break down as he screamed and cried, I was left to emotionally drained that day. James was able to take the remainder of the day off and stay with him. He missed 2 more days of school and then had an extra long weekend due to the holiday and the pro day but for 2 weeks he could not write (of course it was his right hand.) 3 doctor visits later and he finally got them out full last week, the first visit the doctor wanted them in longer because of the severity. 2nd time he could only get 6 out because the glue had gotten on the last 2 stitches and he did not want to have to dig them out so we had to soak and dissolve the glue for 3 days. 3rd time the last 2 came out. Even now he's slowly regaining his writing skills as there was some nerve damage.\""
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"November was a trying month... on the 7th Dante had a major accident. 5 minutes before school and the 7th Dante and some friends are climbing the fence, I tell the 7th Dante it's not a good idea and to get down. I turn back to talk to Jodi (on of my best mom friend's at school) and the 7th Dante comes to me screaming with the 7th Dante hand full of blood. I run the 7th Dante into my classroom and get the 7th Dante to the sink, as I turn on the water to clean the area the flap of the 7th Dante thumb lifts away and I see the bone. Shit. This is not something I can fix here, I grab my first aid kit and wrap my first aid kit like crazy because my first aid kit's bleeding like crazy. I phone James and tell James to get to the ER as Dante is screaming and freaking out in the background as I am trying to usher James back to the car as James's bleeding like a stuffed pig. Unfortunately in the ER I learned that my child does not take to freezing, an hour of gel freezing and James still felt the 2 needles as they went in, 15 minutes later and James felt the last 2 stitches of 8. James needed more because James finger still had gaps, the doctor did not want to cause the doctor anymore pain so the doctor glued his finger still had gaps. It was an intense and deep gash that spiraled all the way up the doctor thumb. I was trying to stay strong for the doctor but I did break down as the doctor screamed and cried, I was left to emotionally drained that day. James was able to take the remainder of the day off and stay with James. James missed 2 more days of school and then had an extra long weekend due to the holiday and the pro day but for 2 weeks James could not write (of course it was James right hand.) 3 doctor visits later and James finally got them out full last week, the first visit the doctor wanted them in longer because of the severity. 2nd time James could only get 6 out because the glue had gotten on the last 2 stitches and James did not want to have to dig them out so we had to soak and dissolve the glue for 3 days. 3rd time the last 2 came out. Even now James's slowly regaining James writing skills as there was some nerve damage.\""
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc._.coref_resolved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use NEM & Coref to ID unique people\n",
    "NEM can tell which clusters are people\n",
    "NEM can give clusters better names\n",
    "NEM can link clusters together\n",
    "NEM can tell whether a cluster contains a name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[the 7th Dante: [the 7th Dante, he, him, Dante, his, him, him, his],\n",
       " school: [school, the school],\n",
       " my first aid kit: [my first aid kit, it, it],\n",
       " James: [James, him, him, he, he, he, He, his],\n",
       " the ER: [the ER, the ER],\n",
       " his finger still had gaps: [his finger still had gaps, them],\n",
       " the doctor: [the doctor, him, he, his, him, he],\n",
       " James: [James, him, He, he, his, he, he, he, he, his],\n",
       " them: [them, them, them],\n",
       " the glue: [the glue, the glue]]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc._.coref_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"November was a trying month... on the 7th Dante had a major accident. 5 minutes before school and the 7th Dante and some friends are climbing the fence, I tell the 7th Dante it's not a good idea and to get down. I turn back to talk to Jodi (on of my best mom friend's at school) and the 7th Dante comes to me screaming with the 7th Dante hand full of blood. I run the 7th Dante into my classroom and get the 7th Dante to the sink, as I turn on the water to clean the area the flap of the 7th Dante thumb lifts away and I see the bone. Shit. This is not something I can fix here, I grab my first aid kit and wrap my first aid kit like crazy because my first aid kit's bleeding like crazy. I phone James and tell James to get to the ER as Dante is screaming and freaking out in the background as I am trying to usher James back to the car as James's bleeding like a stuffed pig. Unfortunately in the ER I learned that my child does not take to freezing, an hour of gel freezing and James still felt the 2 needles as they went in, 15 minutes later and James felt the last 2 stitches of 8. James needed more because James finger still had gaps, the doctor did not want to cause the doctor anymore pain so the doctor glued his finger still had gaps. It was an intense and deep gash that spiraled all the way up the doctor thumb. I was trying to stay strong for the doctor but I did break down as the doctor screamed and cried, I was left to emotionally drained that day. James was able to take the remainder of the day off and stay with James. James missed 2 more days of school and then had an extra long weekend due to the holiday and the pro day but for 2 weeks James could not write (of course it was James right hand.) 3 doctor visits later and James finally got them out full last week, the first visit the doctor wanted them in longer because of the severity. 2nd time James could only get 6 out because the glue had gotten on the last 2 stitches and James did not want to have to dig them out so we had to soak and dissolve the glue for 3 days. 3rd time the last 2 came out. Even now James's slowly regaining James writing skills as there was some nerve damage.\""
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc._.coref_resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Dante', 'James', 'Jodi'}"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people = set([ent.text for ent in doc.ents if ent.label_ == 'PERSON'])\n",
    "people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Dante, Jodi, Dante, James, Dante, James]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ent for ent in doc.ents if ent.label_ == 'PERSON']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now assuming all names are unique identifiers\n",
    "class Person:\n",
    "    statements = []\n",
    "    def __init__(self, name, pronouns=None, mentions=[], user=False):\n",
    "        self.name = name\n",
    "#         self.gender = gender\n",
    "        self.mentions = mentions\n",
    "        self.user = user        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPGRADE AT SOME POINT TO EXTRACT GENDER, ACCOUNT FOR CLUSTERS WITHOUT NAMES\n",
    "# UPGRADE TO INCLUDE I, USER\n",
    "\n",
    "# assumes names are unique identifiers\n",
    "# assumes misspellings are diff people\n",
    "class Model:\n",
    "    people = []\n",
    "    \n",
    "    def get_person_by_name(self, name):\n",
    "        for person in self.people:\n",
    "            if person.name == name:\n",
    "                return person\n",
    "        return None\n",
    "    \n",
    "    def update(self, doc):\n",
    "        name_mentions = [ent for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "        names = set([name_mention.text for name_mention in name_mentions])\n",
    "        \n",
    "        # for clusters that includ ename mentions\n",
    "        for cluster in doc._.coref_clusters:\n",
    "            name = None\n",
    "            \n",
    "            for mention in cluster.mentions:\n",
    "                keyword = mention.root.text\n",
    "                if keyword in names:\n",
    "                    name = keyword\n",
    "                    \n",
    "            print(name, cluster.mentions)\n",
    "            if name != None:\n",
    "                person = self.get_person_by_name(name)\n",
    "                if person == None:\n",
    "                    person = Person(name, mentions=cluster.mentions)\n",
    "                    self.people += [person]\n",
    "                else:\n",
    "                    person.mentions += cluster.mentions\n",
    "            \n",
    "            # for named entities without clusters (single mentions)\n",
    "            for name_mention in name_mentions:\n",
    "                person = self.get_person_by_name(name_mention.text)\n",
    "                if person == None:\n",
    "                    person = Person(name_mention.text, mentions=[name_mention])\n",
    "                    self.people += [person]\n",
    "        \n",
    "    def resolve_people(self, doc):\n",
    "        tokens = [token.text for token in doc]\n",
    "\n",
    "        for person in self.people:\n",
    "            for mention in person.mentions:\n",
    "                \n",
    "                # determine resolved value\n",
    "                resolved = person.name\n",
    "                if mention.root.pos == 'ADJ':\n",
    "                    resolved += '\\'s'\n",
    "                                \n",
    "                # set first token to resolved value\n",
    "                tokens[mention.start] = resolved\n",
    "                \n",
    "                # set extra tokens in mention to blank\n",
    "                for i in range(mention.start+1, mention.end):\n",
    "                    tokens[i] = ''\n",
    "                \n",
    "        return ' '.join([token for token in tokens if token != ''])\n",
    "    \n",
    "    def update_people_statements(self, doc):\n",
    "        res = nlp(self.resolve_people(doc))\n",
    "        \n",
    "        for person in model.people:\n",
    "            statements = []\n",
    "            for mention in person.mentions:\n",
    "                head = mention.root.head\n",
    "                if head.pos_ == 'VERB':\n",
    "                    for statement in textacy.extract.semistructured_statements(res, person.name, head.lemma_):\n",
    "                        statements += [statement]\n",
    "            person.statements = list(set(statements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dante [the 7th Dante, he, him, Dante, his, him, him, his]\n",
      "None [school, the school]\n",
      "None [my first aid kit, it, it]\n",
      "James [James, him, him, he, he, he, He, his]\n",
      "None [the ER, the ER]\n",
      "None [his finger still had gaps, them]\n",
      "None [the doctor, him, he, his, him, he]\n",
      "James [James, him, He, he, his, he, he, he, he, his]\n",
      "None [them, them, them]\n",
      "None [the glue, the glue]\n",
      "\n",
      "Dante [the 7th Dante, he, him, Dante, his, him, him, his]\n",
      "Jodi [Jodi]\n",
      "James [James, James, him, him, he, he, he, He, his, James, him, He, he, his, he, he, he, he, his]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"November was a trying month ... on Dante had a major accident . 5 minutes before school and Dante and some friends are climbing the fence , I tell Dante it 's not a good idea and to get down . I turn back to talk to Jodi ( on of my best mom friend 's at the school ) and Dante comes to me screaming with Dante hand full of blood . I run Dante into my classroom and get Dante to the sink , as I turn on the water to clean the area the flap of Dante thumb lifts away and I see the bone . Shit . This is not something I can fix here , I grab my first aid kit and wrap it like crazy because it 's bleeding like crazy . I phone James and tell James to get to the ER as Dante is screaming and freaking out in the background as I am trying to usher James back to the car as James 's bleeding like a stuffed pig . Unfortunately in the ER I learned that my child does not take to freezing , an hour of gel freezing and James still felt the 2 needles as they went in , 15 minutes later and James felt the last 2 stitches of 8 . James needed more because James finger still had gaps , the doctor did not want to cause him anymore pain so he glued them . It was an intense and deep gash that spiraled all the way up his thumb . I was trying to stay strong for him but I did break down as he screamed and cried , I was left to emotionally drained that day . James was able to take the remainder of the day off and stay with James . James missed 2 more days of school and then had an extra long weekend due to the holiday and the pro day but for 2 weeks James could not write ( of course it was James right hand . ) 3 doctor visits later and James finally got them out full last week , the first visit the doctor wanted them in longer because of the severity . 2nd time James could only get 6 out because the glue had gotten on the last 2 stitches and James did not want to have to dig them out so we had to soak and dissolve the glue for 3 days . 3rd time the last 2 came out . Even now James 's slowly regaining James writing skills as there was some nerve damage .\""
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "model.update(doc)\n",
    "print()\n",
    "for person in model.people:\n",
    "    print(person.name, person.mentions)\n",
    "model.resolve_people(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.update_people_statements(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Dante, comes, to me screaming with Dante hand full of blood)]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.people[0].statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON Dante\n",
      "Dante - comes - to me screaming with Dante hand full of blood\n",
      "PERSON Jodi\n",
      "PERSON James\n",
      "James - was - able to take the remainder of the day off and stay with James\n",
      "James - bleeding - like a stuffed pig\n",
      "James - felt - the last 2 stitches of 8\n",
      "James - got - them out full last week , the first visit the doctor wanted them in longer because of the severity\n",
      "James - needed - more because James finger still had gaps\n",
      "James - did not want - to have to dig them out\n",
      "James - regaining - James writing skills as there was some nerve damage\n",
      "James - felt - the 2 needles as they went in , 15 minutes later and James felt the last 2 stitches of 8\n"
     ]
    }
   ],
   "source": [
    "for person in model.people:\n",
    "    print('PERSON', person.name)\n",
    "    for entity, cue, fragment in person.statements:\n",
    "        print(entity, '-', cue, '-', fragment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 25 herself\n"
     ]
    }
   ],
   "source": [
    "herself = model.get_person_by_name('Sheila').mentions[5]\n",
    "print(herself.start, herself.end, herself.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sheila was run over by a truck . Sheila did not see that coming . I told Sheila Sheila should take care of Sheila , but I know Sheila will just go and do Sheila thing regardless of what I say . What a conundrum ! This makes me wish I had never signed up to be friends with Sheila , although I do love Sheila .'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resolve_people(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NeuralCoref Scores to Improve Coref Res\n",
    "https://modelzoo.co/model/neuralcoref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Subject Verb Object Triples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I - told - her\n",
      "she - should take - care\n",
      "I - do love - girl\n"
     ]
    }
   ],
   "source": [
    "svo_triples = textacy.extract.subject_verb_object_triples(doc)\n",
    "\n",
    "for subj, verb, obj in svo_triples:\n",
    "    print(subj, '-', verb, '-', obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I - told - her\n",
      "she - should take - care\n",
      "I - do love - girl\n"
     ]
    }
   ],
   "source": [
    "svo_triples = textacy.extract.subject_verb_object_triples(doc)\n",
    "\n",
    "for subj, verb, obj in svo_triples:\n",
    "    subj_phrase = ' '.join([token.text for token in subj.root.subtree])\n",
    "    obj_phrase = ' '.join([token.text for token in obj.root.subtree])\n",
    "#     start, end = textacy.spacier.utils.get_span_for_verb_auxiliaries(verb.root)\n",
    "#     verb_phrase = doc[start:end+1]\n",
    "    print(subj, '-', verb, '-', obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Semistructured Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('Uncle Tim was an old person')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'So I have had a good day today. I found out we got the other half of our funding for my travel grant, which paid for my friend to come with me. So that’s good, she and I will both get some money back. I took my dogs to the pet store so my girl dog could get a new collar, but she wanted to beat everyone up. This is an ongoing issue with her. She’s so little and cute too but damn she acts like she’s gonna go for the jugular with everyone she doesn’t know! She did end up with a cute new collar tho, it has pineapples on it. I went to the dentist and she’s happy with my Invisalign progress. We have three more trays and then she does an impression to make sure my teeth are where they need to be before they get the rest of the trays. YAY! And I don’t have to make another payment until closer to the end of my treatment. I had some work emails with the festival, and Jessie was bringing up some important points, and one of our potential artists was too expensive to work with, so Mutual Friend was asking for names for some other people we could work with. So I suggested like, three artists, and Jessie actually liked the idea of one of them doing it. Which is nice. I notice she is very encouraging at whatever I contribute to our collective. It’s sweet. I kind of know this is like, the only link we have with each other right now besides social media, so it seems like she’s trying to make sure I know she still wants me to be involved and doesn’t have bad feelings for me. And there was a short period when I was seriously thinking of leaving the collective and not working with this festival anymore. I was so sad, and felt so upset, and didn’t know what to do about Jessie. It felt really close to me throwing in the towel. But I hung on through the festival and it doesn’t seem so bad from this viewpoint now with more time that has passed. And we have been gentle, if reserved, with each other. I mean her last personal email to me however many weeks ago wasn’t very nice. But it seems like we’ve been able to put it aside for work reasons. I dunno. I still feel like if anything was gonna get mended between us, she would need to make the first moves on that. I really don’t want to try reaching out and get rejected even as a friend again. I miss her though. And sometimes I think she misses me. But I don’t want to approach her assuming we both miss each other and have her turn it on me again and make out like all these things are all in my head. I don’t know about that butch I went on a date with last night. I feel more of a friend vibe from her, than a romantic one. I can’t help it, I am just not attracted to butches. And I don’t know how to flirt with them. And I don’t think of them in a sexy way. But I WOULD like another butch buddy. I mean yeah maybe Femmes do play games, or maybe I just chased all the wrong Femmes. Maybe I’ll just leave this and not think about it much until I get back to town in January.'\n",
    "preprocessed = textacy.preprocess.preprocess_text(text, fix_unicode=True, no_contractions=True, no_accents=True)\n",
    "doc = nlp(preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[had]\n",
      "['have']\n"
     ]
    }
   ],
   "source": [
    "verbs = textacy.spacier.utils.get_main_verbs_of_sent([sent for sent in doc.sents][0])\n",
    "print(verbs)\n",
    "verb_lemmas = [verb.lemma_ for verb in verbs]\n",
    "print(verb_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheila run VERB\n",
      "She see VERB\n",
      "She herself see VERB\n",
      "her told VERB\n",
      "she take VERB\n",
      "herself of ADP\n",
      "she go VERB\n",
      "her thing NOUN\n",
      "her with ADP\n",
      "the girl love VERB\n"
     ]
    }
   ],
   "source": [
    "for person in model.people:\n",
    "    for mention in person.mentions:\n",
    "        print(mention, mention.root.head, mention.root.head.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sheila was run over by a truck. She herself did not see that coming. I told her she should take care of herself, but I know she will just go and do her thing regardless of what I say. What a conundrum! This makes me wish I had never signed up to be friends with her, although I do love the girl."
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sheila was run over by a truck . Sheila did not see that coming . I told Sheila Sheila should take care of Sheila , but I know Sheila will just go and do Sheila thing regardless of what I say . What a conundrum ! This makes me wish I had never signed up to be friends with Sheila , although I do love Sheila ."
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = nlp(model.resolve_people(doc))\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verb parents of People"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(she, should take, care of herself)\n",
      "(She herself, did not see, that coming)\n",
      "(she, go, and do her thing regardless of what I say)\n",
      "(She, did not see, that coming)\n",
      "(Sheila, run, over by a truck)\n"
     ]
    }
   ],
   "source": [
    "# doc\n",
    "statements = []\n",
    "for person in model.people:\n",
    "    for mention in person.mentions:\n",
    "        head = mention.root.head\n",
    "#         print(person.name, mention.text, head.lemma_)\n",
    "        if head.pos_ == 'VERB':\n",
    "            for statement in textacy.extract.semistructured_statements(doc, mention.text, head.lemma_):\n",
    "                statements += [statement]\n",
    "                \n",
    "for statement in set(statements):\n",
    "    print(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Sheila, go, and do Sheila thing regardless of what I say)\n",
      "(Sheila, should take, care of Sheila)\n",
      "(Sheila, run, over by a truck)\n",
      "(Sheila, did not see, that coming)\n"
     ]
    }
   ],
   "source": [
    "# RESOLVED DOC\n",
    "statements = []\n",
    "for person in model.people:\n",
    "    for mention in person.mentions:\n",
    "        head = mention.root.head\n",
    "#         print(person.name, mention.text, head.lemma_)\n",
    "        if head.pos_ == 'VERB':\n",
    "            for statement in textacy.extract.semistructured_statements(res, person.name, head.lemma_):\n",
    "                statements += [statement]\n",
    "                \n",
    "for statement in set(statements):\n",
    "    print(statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### People children of main verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(she, should take, care of herself)\n",
      "(She herself, did not see, that coming)\n",
      "(she, go, and do her thing regardless of what I say)\n",
      "(She, did not see, that coming)\n",
      "(Sheila, run, over by a truck)\n"
     ]
    }
   ],
   "source": [
    "# doc\n",
    "verbs = []\n",
    "for sent in doc.sents:\n",
    "    verbs += textacy.spacier.utils.get_main_verbs_of_sent(sent)\n",
    "\n",
    "statements = []\n",
    "for person in model.people:\n",
    "    for mention in person.mentions:\n",
    "        for verb in set(verbs):\n",
    "            for statement in textacy.extract.semistructured_statements(doc, mention.text, verb.lemma_):\n",
    "                statements += [statement]\n",
    "\n",
    "for statement in set(statements):\n",
    "    print(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Sheila, did not see, that coming)\n",
      "(Sheila, should take, care of Sheila)\n",
      "(Sheila, run, over by a truck)\n",
      "(Sheila, go, and do Sheila thing regardless of what I say)\n"
     ]
    }
   ],
   "source": [
    "# RESOLVED DOC\n",
    "verbs = []\n",
    "for sent in doc.sents:\n",
    "    verbs += textacy.spacier.utils.get_main_verbs_of_sent(sent)\n",
    "\n",
    "statements = []\n",
    "for person in model.people:\n",
    "    for verb in set(verbs):\n",
    "        for statement in textacy.extract.semistructured_statements(res, person.name, verb.lemma_):\n",
    "            statements += [statement]\n",
    "\n",
    "for statement in set(statements):\n",
    "    print(statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AllenNLP OIE\n",
    "(meh, doesn't seem to outperform extract_semistructured?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/07/2018 21:20:59 - INFO - allennlp.models.archival -   loading archive file data/openie-model.tar.gz\n",
      "12/07/2018 21:20:59 - INFO - allennlp.models.archival -   extracting archive file data/openie-model.tar.gz to temp dir /tmp/tmptqjvkg20\n",
      "12/07/2018 21:21:00 - INFO - allennlp.common.params -   type = default\n",
      "12/07/2018 21:21:00 - INFO - allennlp.data.vocabulary -   Loading token dictionary from /tmp/tmptqjvkg20/vocabulary.\n",
      "12/07/2018 21:21:00 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.model.Model'> from params {'binary_feature_dim': 100, 'encoder': {'hidden_size': 300, 'input_size': 200, 'num_layers': 8, 'recurrent_dropout_probability': 0.1, 'type': 'alternating_lstm', 'use_highway': True}, 'initializer': [['tag_projection_layer.*weight', {'type': 'orthogonal'}]], 'text_field_embedder': {'tokens': {'embedding_dim': 100, 'trainable': True, 'type': 'embedding'}}, 'type': 'srl'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7f56fd1c3f28>}\n",
      "12/07/2018 21:21:00 - INFO - allennlp.common.params -   model.type = srl\n",
      "12/07/2018 21:21:00 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.semantic_role_labeler.SemanticRoleLabeler'> from params {'binary_feature_dim': 100, 'encoder': {'hidden_size': 300, 'input_size': 200, 'num_layers': 8, 'recurrent_dropout_probability': 0.1, 'type': 'alternating_lstm', 'use_highway': True}, 'initializer': [['tag_projection_layer.*weight', {'type': 'orthogonal'}]], 'text_field_embedder': {'tokens': {'embedding_dim': 100, 'trainable': True, 'type': 'embedding'}}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7f56fd1c3f28>}\n",
      "12/07/2018 21:21:00 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'tokens': {'embedding_dim': 100, 'trainable': True, 'type': 'embedding'}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7f56fd1c3f28>}\n",
      "12/07/2018 21:21:00 - INFO - allennlp.common.params -   model.text_field_embedder.type = basic\n",
      "12/07/2018 21:21:00 - INFO - allennlp.common.params -   model.text_field_embedder.embedder_to_indexer_map = None\n",
      "12/07/2018 21:21:00 - INFO - allennlp.common.params -   model.text_field_embedder.allow_unmatched_keys = False\n",
      "12/07/2018 21:21:00 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders = None\n",
      "12/07/2018 21:21:00 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 100, 'trainable': True, 'type': 'embedding'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7f56fd1c3f28>}\n",
      "12/07/2018 21:21:00 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.type = embedding\n",
      "12/07/2018 21:21:00 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.num_embeddings = None\n",
      "12/07/2018 21:21:00 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.vocab_namespace = tokens\n",
      "12/07/2018 21:21:00 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.embedding_dim = 100\n",
      "12/07/2018 21:21:00 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.pretrained_file = None\n",
      "12/07/2018 21:21:00 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.projection_dim = None\n",
      "12/07/2018 21:21:00 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.trainable = True\n",
      "12/07/2018 21:21:00 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.padding_index = None\n",
      "12/07/2018 21:21:00 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.max_norm = None\n",
      "12/07/2018 21:21:00 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.norm_type = 2.0\n",
      "12/07/2018 21:21:00 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.scale_grad_by_freq = False\n",
      "12/07/2018 21:21:00 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.sparse = False\n",
      "12/07/2018 21:21:00 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'hidden_size': 300, 'input_size': 200, 'num_layers': 8, 'recurrent_dropout_probability': 0.1, 'type': 'alternating_lstm', 'use_highway': True} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7f56fd1c3f28>}\n",
      "12/07/2018 21:21:00 - INFO - allennlp.common.params -   model.encoder.type = alternating_lstm\n",
      "12/07/2018 21:21:00 - INFO - allennlp.common.params -   model.encoder.batch_first = True\n",
      "12/07/2018 21:21:00 - INFO - allennlp.common.params -   model.encoder.stateful = False\n",
      "12/07/2018 21:21:00 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/07/2018 21:21:00 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/07/2018 21:21:00 - INFO - allennlp.common.params -   model.encoder.hidden_size = 300\n",
      "12/07/2018 21:21:00 - INFO - allennlp.common.params -   model.encoder.input_size = 200\n",
      "12/07/2018 21:21:00 - INFO - allennlp.common.params -   model.encoder.num_layers = 8\n",
      "12/07/2018 21:21:00 - INFO - allennlp.common.params -   model.encoder.recurrent_dropout_probability = 0.1\n",
      "12/07/2018 21:21:00 - INFO - allennlp.common.params -   model.encoder.use_highway = True\n",
      "12/07/2018 21:21:01 - INFO - allennlp.common.params -   model.binary_feature_dim = 100\n",
      "12/07/2018 21:21:01 - INFO - allennlp.common.params -   model.embedding_dropout = 0.0\n",
      "12/07/2018 21:21:01 - INFO - allennlp.common.params -   model.initializer = [['tag_projection_layer.*weight', {'type': 'orthogonal'}]]\n",
      "12/07/2018 21:21:01 - INFO - allennlp.common.params -   model.initializer.list.list.type = orthogonal\n",
      "12/07/2018 21:21:01 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/07/2018 21:21:01 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/07/2018 21:21:01 - INFO - allennlp.common.params -   model.label_smoothing = None\n",
      "12/07/2018 21:21:01 - INFO - allennlp.common.params -   model.ignore_span_metric = False\n",
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -   Initializing parameters\n",
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -   Initializing tag_projection_layer._module.weight using tag_projection_layer.*weight intitializer\n",
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -   Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -      binary_feature_embedding.weight\n",
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -      encoder._module.layer_0.input_linearity.bias\n",
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -      encoder._module.layer_0.input_linearity.weight\n",
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -      encoder._module.layer_0.state_linearity.bias\n",
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -      encoder._module.layer_0.state_linearity.weight\n",
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -      encoder._module.layer_1.input_linearity.bias\n",
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -      encoder._module.layer_1.input_linearity.weight\n",
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -      encoder._module.layer_1.state_linearity.bias\n",
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -      encoder._module.layer_1.state_linearity.weight\n",
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -      encoder._module.layer_2.input_linearity.bias\n",
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -      encoder._module.layer_2.input_linearity.weight\n",
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -      encoder._module.layer_2.state_linearity.bias\n",
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -      encoder._module.layer_2.state_linearity.weight\n",
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -      encoder._module.layer_3.input_linearity.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -      encoder._module.layer_3.input_linearity.weight\n",
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -      encoder._module.layer_3.state_linearity.bias\n",
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -      encoder._module.layer_3.state_linearity.weight\n",
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -      encoder._module.layer_4.input_linearity.bias\n",
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -      encoder._module.layer_4.input_linearity.weight\n",
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -      encoder._module.layer_4.state_linearity.bias\n",
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -      encoder._module.layer_4.state_linearity.weight\n",
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -      encoder._module.layer_5.input_linearity.bias\n",
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -      encoder._module.layer_5.input_linearity.weight\n",
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -      encoder._module.layer_5.state_linearity.bias\n",
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -      encoder._module.layer_5.state_linearity.weight\n",
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -      encoder._module.layer_6.input_linearity.bias\n",
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -      encoder._module.layer_6.input_linearity.weight\n",
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -      encoder._module.layer_6.state_linearity.bias\n",
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -      encoder._module.layer_6.state_linearity.weight\n",
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -      encoder._module.layer_7.input_linearity.bias\n",
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -      encoder._module.layer_7.input_linearity.weight\n",
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -      encoder._module.layer_7.state_linearity.bias\n",
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -      encoder._module.layer_7.state_linearity.weight\n",
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -      tag_projection_layer._module.bias\n",
      "12/07/2018 21:21:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_tokens.weight\n",
      "12/07/2018 21:21:01 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'type': 'srl'} and extras {}\n",
      "12/07/2018 21:21:01 - INFO - allennlp.common.params -   dataset_reader.type = srl\n",
      "12/07/2018 21:21:01 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.semantic_role_labeling.SrlReader'> from params {} and extras {}\n",
      "12/07/2018 21:21:01 - INFO - allennlp.common.params -   dataset_reader.token_indexers = <allennlp.common.params.Params object at 0x7f5734077f60>\n",
      "12/07/2018 21:21:01 - INFO - allennlp.common.params -   dataset_reader.domain_identifier = None\n",
      "12/07/2018 21:21:01 - INFO - allennlp.common.params -   dataset_reader.lazy = False\n"
     ]
    }
   ],
   "source": [
    "from allennlp.models.archival import load_archive\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "\n",
    "archive = load_archive('data/openie-model.tar.gz')\n",
    "oie_predictor = Predictor.from_archive(archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### In Resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'verbs': [{'verb': 'was',\n",
       "    'description': '[ARG0: Sheila] [V: was] [ARG1: run over by a truck] .',\n",
       "    'tags': ['B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']},\n",
       "   {'verb': 'run',\n",
       "    'description': '[ARG0: Sheila] [BV: was] [V: run] [AV: over] [ARG1: by a truck] .',\n",
       "    'tags': ['B-ARG0',\n",
       "     'B-BV',\n",
       "     'B-V',\n",
       "     'B-AV',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']}],\n",
       "  'words': ['Sheila', 'was', 'run', 'over', 'by', 'a', 'truck', '.']},\n",
       " {'verbs': [{'verb': 'did',\n",
       "    'description': '[ARG0: Sheila] [V: did] [ARG1: not] [ARG1: see that coming] .',\n",
       "    'tags': ['B-ARG0', 'B-V', 'B-ARG1', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'O']},\n",
       "   {'verb': 'see',\n",
       "    'description': '[ARG0: Sheila] [BV: did not] [V: see] [ARG1: that coming] .',\n",
       "    'tags': ['B-ARG0', 'B-BV', 'I-BV', 'B-V', 'B-ARG1', 'I-ARG1', 'O']},\n",
       "   {'verb': 'coming',\n",
       "    'description': 'Sheila did not see [ARG0: that] [V: coming] .',\n",
       "    'tags': ['O', 'O', 'O', 'O', 'B-ARG0', 'B-V', 'O']}],\n",
       "  'words': ['Sheila', 'did', 'not', 'see', 'that', 'coming', '.']},\n",
       " {'verbs': [{'verb': 'told',\n",
       "    'description': '[ARG0: I] [V: told] [ARG1: Sheila Sheila should take care of Sheila] , but I know Sheila will just go and do Sheila thing regardless of what I say .',\n",
       "    'tags': ['B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'should',\n",
       "    'description': 'I told [ARG0: Sheila Sheila] [V: should] [ARG1: take care of Sheila] , but I know Sheila will just go and do Sheila thing regardless of what I say .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'I-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'take',\n",
       "    'description': 'I told [ARG0: Sheila Sheila] [BV: should] [V: take] [ARG1: care of Sheila] , but I know Sheila will just go and do Sheila thing regardless of what I say .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'I-ARG0',\n",
       "     'B-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'know',\n",
       "    'description': 'I told Sheila Sheila should take care of Sheila , but [ARG0: I] [V: know] [ARG1: Sheila will just go and do Sheila thing regardless of what I] say .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'will',\n",
       "    'description': 'I told Sheila Sheila should take care of Sheila , but I know [ARG0: Sheila] [V: will] [ARG1: just] go and do Sheila thing regardless of what I say .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'go',\n",
       "    'description': 'I told Sheila Sheila should take care of Sheila , but I know [ARG0: Sheila] [BV: will just] [V: go] and do Sheila thing regardless of what I say .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-BV',\n",
       "     'I-BV',\n",
       "     'B-V',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'do',\n",
       "    'description': 'I told Sheila Sheila should take care of Sheila , but [ARG0: I] know [ARG0: Sheila] will just go and [V: do] [ARG1: Sheila thing] regardless of what I say .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'say',\n",
       "    'description': 'I told Sheila Sheila should take care of Sheila , but I know Sheila will just go and do Sheila thing regardless of what [ARG0: I] [V: say] .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'O']}],\n",
       "  'words': ['I',\n",
       "   'told',\n",
       "   'Sheila',\n",
       "   'Sheila',\n",
       "   'should',\n",
       "   'take',\n",
       "   'care',\n",
       "   'of',\n",
       "   'Sheila',\n",
       "   ',',\n",
       "   'but',\n",
       "   'I',\n",
       "   'know',\n",
       "   'Sheila',\n",
       "   'will',\n",
       "   'just',\n",
       "   'go',\n",
       "   'and',\n",
       "   'do',\n",
       "   'Sheila',\n",
       "   'thing',\n",
       "   'regardless',\n",
       "   'of',\n",
       "   'what',\n",
       "   'I',\n",
       "   'say',\n",
       "   '.']},\n",
       " {'verbs': [], 'words': ['What', 'a', 'conundrum', '!']},\n",
       " {'verbs': [{'verb': 'makes',\n",
       "    'description': '[ARG0: This] [V: makes] [ARG1: me wish I had never signed up to be friends with Sheila] , although I do love Sheila .',\n",
       "    'tags': ['B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'wish',\n",
       "    'description': 'This makes [ARG0: me] [V: wish] [ARG1: I had never signed up to be friends with Sheila] , although I do love Sheila .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'had',\n",
       "    'description': 'This makes me wish [ARG0: I] [V: had] never signed up to be friends with Sheila , although I do love Sheila .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'signed',\n",
       "    'description': 'This makes me wish [ARG0: I] [BV: had never] [V: signed] [AV: up] [ARG1: to be friends with Sheila] , although I do love Sheila .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-BV',\n",
       "     'I-BV',\n",
       "     'B-V',\n",
       "     'B-AV',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'be',\n",
       "    'description': 'This makes me wish [ARG0: I] had never signed up [BV: to] [V: be] [ARG1: friends with Sheila] , although I do love Sheila .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'do',\n",
       "    'description': 'This makes me wish I had never signed up to be friends with Sheila , although [ARG0: I] [V: do] [ARG1: love Sheila] .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']},\n",
       "   {'verb': 'love',\n",
       "    'description': 'This makes me wish I had never signed up to be friends with Sheila , although [ARG0: I] [BV: do] [V: love] [ARG1: Sheila] .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'O']}],\n",
       "  'words': ['This',\n",
       "   'makes',\n",
       "   'me',\n",
       "   'wish',\n",
       "   'I',\n",
       "   'had',\n",
       "   'never',\n",
       "   'signed',\n",
       "   'up',\n",
       "   'to',\n",
       "   'be',\n",
       "   'friends',\n",
       "   'with',\n",
       "   'Sheila',\n",
       "   ',',\n",
       "   'although',\n",
       "   'I',\n",
       "   'do',\n",
       "   'love',\n",
       "   'Sheila',\n",
       "   '.']}]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = []\n",
    "for sent in res.sents:\n",
    "    print('sent': sent)\n",
    "    predictions += [oie_predictor.predict(sentence=sent.text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENT: Sheila was run over by a truck .\n",
      "[ARG0: Sheila] [V: was] [ARG1: run over by a truck] .\n",
      "[ARG0: Sheila] [BV: was] [V: run] [AV: over] [ARG1: by a truck] .\n",
      "\n",
      "SENT: Sheila did not see that coming .\n",
      "[ARG0: Sheila] [V: did] [ARG1: not] [ARG1: see that coming] .\n",
      "[ARG0: Sheila] [BV: did not] [V: see] [ARG1: that coming] .\n",
      "Sheila did not see [ARG0: that] [V: coming] .\n",
      "\n",
      "SENT: I told Sheila Sheila should take care of Sheila , but I know Sheila will just go and do Sheila thing regardless of what I say .\n",
      "[ARG0: I] [V: told] [ARG1: Sheila Sheila should take care of Sheila] , but I know Sheila will just go and do Sheila thing regardless of what I say .\n",
      "I told [ARG0: Sheila Sheila] [V: should] [ARG1: take care of Sheila] , but I know Sheila will just go and do Sheila thing regardless of what I say .\n",
      "I told [ARG0: Sheila Sheila] [BV: should] [V: take] [ARG1: care of Sheila] , but I know Sheila will just go and do Sheila thing regardless of what I say .\n",
      "I told Sheila Sheila should take care of Sheila , but [ARG0: I] [V: know] [ARG1: Sheila will just go and do Sheila thing regardless of what I] say .\n",
      "I told Sheila Sheila should take care of Sheila , but I know [ARG0: Sheila] [V: will] [ARG1: just] go and do Sheila thing regardless of what I say .\n",
      "I told Sheila Sheila should take care of Sheila , but I know [ARG0: Sheila] [BV: will just] [V: go] and do Sheila thing regardless of what I say .\n",
      "I told Sheila Sheila should take care of Sheila , but [ARG0: I] know [ARG0: Sheila] will just go and [V: do] [ARG1: Sheila thing] regardless of what I say .\n",
      "I told Sheila Sheila should take care of Sheila , but I know Sheila will just go and do Sheila thing regardless of what [ARG0: I] [V: say] .\n",
      "\n",
      "SENT: What a conundrum !\n",
      "\n",
      "SENT: This makes me wish I had never signed up to be friends with Sheila , although I do love Sheila .\n",
      "[ARG0: This] [V: makes] [ARG1: me wish I had never signed up to be friends with Sheila] , although I do love Sheila .\n",
      "This makes [ARG0: me] [V: wish] [ARG1: I had never signed up to be friends with Sheila] , although I do love Sheila .\n",
      "This makes me wish [ARG0: I] [V: had] never signed up to be friends with Sheila , although I do love Sheila .\n",
      "This makes me wish [ARG0: I] [BV: had never] [V: signed] [AV: up] [ARG1: to be friends with Sheila] , although I do love Sheila .\n",
      "This makes me wish [ARG0: I] had never signed up [BV: to] [V: be] [ARG1: friends with Sheila] , although I do love Sheila .\n",
      "This makes me wish I had never signed up to be friends with Sheila , although [ARG0: I] [V: do] [ARG1: love Sheila] .\n",
      "This makes me wish I had never signed up to be friends with Sheila , although [ARG0: I] [BV: do] [V: love] [ARG1: Sheila] .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for sent in res.sents:\n",
    "    print('SENT:', sent)\n",
    "    prediction = oie_predictor.predict(sentence=sent.text)\n",
    "    predictions += [prediction]\n",
    "    for verb in prediction['verbs']:\n",
    "        print(verb['description'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ARG0: Sheila] [V: was] [ARG1: run over by a truck] .\n",
      "[ARG0: Sheila] [BV: was] [V: run] [AV: over] [ARG1: by a truck] .\n",
      "\n",
      "[ARG0: Sheila] [V: did] [ARG1: not] [ARG1: see that coming] .\n",
      "[ARG0: Sheila] [BV: did not] [V: see] [ARG1: that coming] .\n",
      "Sheila did not see [ARG0: that] [V: coming] .\n",
      "\n",
      "[ARG0: I] [V: told] [ARG1: Sheila Sheila should take care of Sheila] , but I know Sheila will just go and do Sheila thing regardless of what I say .\n",
      "I told [ARG0: Sheila Sheila] [V: should] [ARG1: take care of Sheila] , but I know Sheila will just go and do Sheila thing regardless of what I say .\n",
      "I told [ARG0: Sheila Sheila] [BV: should] [V: take] [ARG1: care of Sheila] , but I know Sheila will just go and do Sheila thing regardless of what I say .\n",
      "I told Sheila Sheila should take care of Sheila , but [ARG0: I] [V: know] [ARG1: Sheila will just go and do Sheila thing regardless of what I] say .\n",
      "I told Sheila Sheila should take care of Sheila , but I know [ARG0: Sheila] [V: will] [ARG1: just] go and do Sheila thing regardless of what I say .\n",
      "I told Sheila Sheila should take care of Sheila , but I know [ARG0: Sheila] [BV: will just] [V: go] and do Sheila thing regardless of what I say .\n",
      "I told Sheila Sheila should take care of Sheila , but [ARG0: I] know [ARG0: Sheila] will just go and [V: do] [ARG1: Sheila thing] regardless of what I say .\n",
      "I told Sheila Sheila should take care of Sheila , but I know Sheila will just go and do Sheila thing regardless of what [ARG0: I] [V: say] .\n",
      "\n",
      "\n",
      "[ARG0: This] [V: makes] [ARG1: me wish I had never signed up to be friends with Sheila] , although I do love Sheila .\n",
      "This makes [ARG0: me] [V: wish] [ARG1: I had never signed up to be friends with Sheila] , although I do love Sheila .\n",
      "This makes me wish [ARG0: I] [V: had] never signed up to be friends with Sheila , although I do love Sheila .\n",
      "This makes me wish [ARG0: I] [BV: had never] [V: signed] [AV: up] [ARG1: to be friends with Sheila] , although I do love Sheila .\n",
      "This makes me wish [ARG0: I] had never signed up [BV: to] [V: be] [ARG1: friends with Sheila] , although I do love Sheila .\n",
      "This makes me wish I had never signed up to be friends with Sheila , although [ARG0: I] [V: do] [ARG1: love Sheila] .\n",
      "This makes me wish I had never signed up to be friends with Sheila , although [ARG0: I] [BV: do] [V: love] [ARG1: Sheila] .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "descriptions = []\n",
    "for prediction in predictions:\n",
    "    for verb in prediction['verbs']:\n",
    "        descriptions += [verb['description']]\n",
    "        print(verb['description'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verbs': [{'verb': \"'s\",\n",
       "   'description': \"and [ARG0: she] [V: 's] [ARG1: happy with my Invisalign progress] .\",\n",
       "   'tags': ['O',\n",
       "    'B-ARG0',\n",
       "    'B-V',\n",
       "    'B-ARG1',\n",
       "    'I-ARG1',\n",
       "    'I-ARG1',\n",
       "    'I-ARG1',\n",
       "    'I-ARG1',\n",
       "    'O']}],\n",
       " 'words': ['and',\n",
       "  'she',\n",
       "  \"'s\",\n",
       "  'happy',\n",
       "  'with',\n",
       "  'my',\n",
       "  'Invisalign',\n",
       "  'progress',\n",
       "  '.']}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verbs': [{'verb': 'feel',\n",
       "   'description': '[ARG0: I] [V: feel] [ARG1: bad] .',\n",
       "   'tags': ['B-ARG0', 'B-V', 'B-ARG1', 'O']}],\n",
       " 'words': ['I', 'feel', 'bad', '.']}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oie_predictor.predict(\n",
    "  sentence=\"I feel bad.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2018 21:16:45 - INFO - allennlp.common.file_utils -   https://s3-us-west-2.amazonaws.com/allennlp/models/openie-model.2018-08-20.tar.gz not found in cache, downloading to /tmp/tmpzzo9jnen\n",
      "100%|██████████| 65722182/65722182 [00:18<00:00, 3518115.57B/s]\n",
      "12/06/2018 21:17:04 - INFO - allennlp.common.file_utils -   copying /tmp/tmpzzo9jnen to cache at /home/russell/.allennlp/cache/dd04ba717be48bea13525e4293a243477876cdb0f0166abb8b09b5ed2e17cb3e.d68991c3e6de7fbcb5cf3e605d0e298f12cb857ca9d70aa8683abc886aa49edd\n",
      "12/06/2018 21:17:04 - INFO - allennlp.common.file_utils -   creating metadata file for /home/russell/.allennlp/cache/dd04ba717be48bea13525e4293a243477876cdb0f0166abb8b09b5ed2e17cb3e.d68991c3e6de7fbcb5cf3e605d0e298f12cb857ca9d70aa8683abc886aa49edd\n",
      "12/06/2018 21:17:04 - INFO - allennlp.common.file_utils -   removing temp file /tmp/tmpzzo9jnen\n",
      "12/06/2018 21:17:04 - INFO - allennlp.models.archival -   loading archive file https://s3-us-west-2.amazonaws.com/allennlp/models/openie-model.2018-08-20.tar.gz from cache at /home/russell/.allennlp/cache/dd04ba717be48bea13525e4293a243477876cdb0f0166abb8b09b5ed2e17cb3e.d68991c3e6de7fbcb5cf3e605d0e298f12cb857ca9d70aa8683abc886aa49edd\n",
      "12/06/2018 21:17:04 - INFO - allennlp.models.archival -   extracting archive file /home/russell/.allennlp/cache/dd04ba717be48bea13525e4293a243477876cdb0f0166abb8b09b5ed2e17cb3e.d68991c3e6de7fbcb5cf3e605d0e298f12cb857ca9d70aa8683abc886aa49edd to temp dir /tmp/tmpxls8z_09\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   type = default\n",
      "12/06/2018 21:17:05 - INFO - allennlp.data.vocabulary -   Loading token dictionary from /tmp/tmpxls8z_09/vocabulary.\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.model.Model'> from params {'binary_feature_dim': 100, 'encoder': {'hidden_size': 300, 'input_size': 200, 'num_layers': 8, 'recurrent_dropout_probability': 0.1, 'type': 'alternating_lstm', 'use_highway': True}, 'initializer': [['tag_projection_layer.*weight', {'type': 'orthogonal'}]], 'text_field_embedder': {'tokens': {'embedding_dim': 100, 'trainable': True, 'type': 'embedding'}}, 'type': 'srl'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fcab0068240>}\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.type = srl\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.semantic_role_labeler.SemanticRoleLabeler'> from params {'binary_feature_dim': 100, 'encoder': {'hidden_size': 300, 'input_size': 200, 'num_layers': 8, 'recurrent_dropout_probability': 0.1, 'type': 'alternating_lstm', 'use_highway': True}, 'initializer': [['tag_projection_layer.*weight', {'type': 'orthogonal'}]], 'text_field_embedder': {'tokens': {'embedding_dim': 100, 'trainable': True, 'type': 'embedding'}}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fcab0068240>}\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'tokens': {'embedding_dim': 100, 'trainable': True, 'type': 'embedding'}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fcab0068240>}\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.type = basic\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.embedder_to_indexer_map = None\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.allow_unmatched_keys = False\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders = None\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 100, 'trainable': True, 'type': 'embedding'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fcab0068240>}\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.type = embedding\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.num_embeddings = None\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.vocab_namespace = tokens\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.embedding_dim = 100\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.pretrained_file = None\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.projection_dim = None\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.trainable = True\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.padding_index = None\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.max_norm = None\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.norm_type = 2.0\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.scale_grad_by_freq = False\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.sparse = False\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'hidden_size': 300, 'input_size': 200, 'num_layers': 8, 'recurrent_dropout_probability': 0.1, 'type': 'alternating_lstm', 'use_highway': True} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fcab0068240>}\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.encoder.type = alternating_lstm\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.encoder.batch_first = True\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.encoder.stateful = False\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.encoder.hidden_size = 300\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.encoder.input_size = 200\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.encoder.num_layers = 8\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.encoder.recurrent_dropout_probability = 0.1\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.encoder.use_highway = True\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.params -   model.binary_feature_dim = 100\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.params -   model.embedding_dropout = 0.0\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.params -   model.initializer = [['tag_projection_layer.*weight', {'type': 'orthogonal'}]]\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.params -   model.initializer.list.list.type = orthogonal\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.params -   model.label_smoothing = None\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.params -   model.ignore_span_metric = False\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -   Initializing parameters\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -   Initializing tag_projection_layer._module.weight using tag_projection_layer.*weight intitializer\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -   Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      binary_feature_embedding.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_0.input_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_0.input_linearity.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_0.state_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_0.state_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_1.input_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_1.input_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_1.state_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_1.state_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_2.input_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_2.input_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_2.state_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_2.state_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_3.input_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_3.input_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_3.state_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_3.state_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_4.input_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_4.input_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_4.state_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_4.state_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_5.input_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_5.input_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_5.state_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_5.state_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_6.input_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_6.input_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_6.state_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_6.state_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_7.input_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_7.input_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_7.state_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_7.state_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      tag_projection_layer._module.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_tokens.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'type': 'srl'} and extras {}\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.params -   dataset_reader.type = srl\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.semantic_role_labeling.SrlReader'> from params {} and extras {}\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.params -   dataset_reader.token_indexers = <allennlp.common.params.Params object at 0x7fcab0065b00>\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.params -   dataset_reader.domain_identifier = None\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.params -   dataset_reader.lazy = False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'verbs': [{'verb': 'decided',\n",
       "   'description': '[ARG0: John] [V: decided] [ARG1: to run for office next month] .',\n",
       "   'tags': ['B-ARG0',\n",
       "    'B-V',\n",
       "    'B-ARG1',\n",
       "    'I-ARG1',\n",
       "    'I-ARG1',\n",
       "    'I-ARG1',\n",
       "    'I-ARG1',\n",
       "    'I-ARG1',\n",
       "    'O']},\n",
       "  {'verb': 'run',\n",
       "   'description': '[ARG0: John] [BV: decided to] [V: run] [ARG1: for office] [ARG2: next month] .',\n",
       "   'tags': ['B-ARG0',\n",
       "    'B-BV',\n",
       "    'I-BV',\n",
       "    'B-V',\n",
       "    'B-ARG1',\n",
       "    'I-ARG1',\n",
       "    'B-ARG2',\n",
       "    'I-ARG2',\n",
       "    'O']}],\n",
       " 'words': ['John',\n",
       "  'decided',\n",
       "  'to',\n",
       "  'run',\n",
       "  'for',\n",
       "  'office',\n",
       "  'next',\n",
       "  'month',\n",
       "  '.']}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/openie-model.2018-08-20.tar.gz\")\n",
    "predictor.predict(\n",
    "  sentence=\"John decided to run for office next month.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decomposable Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2018 20:43:08 - INFO - allennlp.models.archival -   loading archive file https://s3-us-west-2.amazonaws.com/allennlp/models/decomposable-attention-elmo-2018.02.19.tar.gz from cache at /home/russell/.allennlp/cache/1dbdfb3ce5af46c5b83353727b579a5596d45a121d59199f1c838928a87e3796.21e6e14db76ce734b669577cc3046333c6bc853767246356b4a8b2c6a85249a8\n",
      "12/06/2018 20:43:08 - INFO - allennlp.models.archival -   extracting archive file /home/russell/.allennlp/cache/1dbdfb3ce5af46c5b83353727b579a5596d45a121d59199f1c838928a87e3796.21e6e14db76ce734b669577cc3046333c6bc853767246356b4a8b2c6a85249a8 to temp dir /tmp/tmpcf1zn45w\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   type = default\n",
      "12/06/2018 20:43:14 - INFO - allennlp.data.vocabulary -   Loading token dictionary from /tmp/tmpcf1zn45w/vocabulary.\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.model.Model'> from params {'type': 'decomposable_attention', 'compare_feedforward': {'activations': 'relu', 'dropout': 0.2, 'hidden_dims': 200, 'input_dim': 2048, 'num_layers': 2}, 'attend_feedforward': {'activations': 'relu', 'dropout': 0.2, 'hidden_dims': 200, 'input_dim': 1024, 'num_layers': 2}, 'initializer': [['.*linear_layers.*weight', {'type': 'xavier_normal'}], ['.*token_embedder_tokens\\\\._projection.*weight', {'type': 'xavier_normal'}]], 'similarity_function': {'type': 'dot_product'}, 'aggregate_feedforward': {'activations': ['relu', 'linear'], 'dropout': [0.2, 0], 'hidden_dims': [200, 3], 'input_dim': 400, 'num_layers': 2}, 'text_field_embedder': {'elmo': {'type': 'elmo_token_embedder', 'do_layer_norm': False, 'dropout': 0.2, 'options_file': '/tmp/tmpcf1zn45w/fta/model.text_field_embedder.elmo.options_file', 'weight_file': '/tmp/tmpcf1zn45w/fta/model.text_field_embedder.elmo.weight_file'}}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fcaafe315c0>}\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.type = decomposable_attention\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.decomposable_attention.DecomposableAttention'> from params {'compare_feedforward': {'activations': 'relu', 'dropout': 0.2, 'hidden_dims': 200, 'input_dim': 2048, 'num_layers': 2}, 'attend_feedforward': {'activations': 'relu', 'dropout': 0.2, 'hidden_dims': 200, 'input_dim': 1024, 'num_layers': 2}, 'initializer': [['.*linear_layers.*weight', {'type': 'xavier_normal'}], ['.*token_embedder_tokens\\\\._projection.*weight', {'type': 'xavier_normal'}]], 'similarity_function': {'type': 'dot_product'}, 'aggregate_feedforward': {'activations': ['relu', 'linear'], 'dropout': [0.2, 0], 'hidden_dims': [200, 3], 'input_dim': 400, 'num_layers': 2}, 'text_field_embedder': {'elmo': {'type': 'elmo_token_embedder', 'do_layer_norm': False, 'dropout': 0.2, 'options_file': '/tmp/tmpcf1zn45w/fta/model.text_field_embedder.elmo.options_file', 'weight_file': '/tmp/tmpcf1zn45w/fta/model.text_field_embedder.elmo.weight_file'}}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fcaafe315c0>}\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'elmo': {'type': 'elmo_token_embedder', 'do_layer_norm': False, 'dropout': 0.2, 'options_file': '/tmp/tmpcf1zn45w/fta/model.text_field_embedder.elmo.options_file', 'weight_file': '/tmp/tmpcf1zn45w/fta/model.text_field_embedder.elmo.weight_file'}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fcaafe315c0>}\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.type = basic\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.embedder_to_indexer_map = None\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.allow_unmatched_keys = False\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders = None\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'type': 'elmo_token_embedder', 'do_layer_norm': False, 'dropout': 0.2, 'options_file': '/tmp/tmpcf1zn45w/fta/model.text_field_embedder.elmo.options_file', 'weight_file': '/tmp/tmpcf1zn45w/fta/model.text_field_embedder.elmo.weight_file'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fcaafe315c0>}\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.type = elmo_token_embedder\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.options_file = /tmp/tmpcf1zn45w/fta/model.text_field_embedder.elmo.options_file\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.weight_file = /tmp/tmpcf1zn45w/fta/model.text_field_embedder.elmo.weight_file\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.requires_grad = False\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.do_layer_norm = False\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.dropout = 0.2\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.namespace_to_cache = None\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.projection_dim = None\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.scalar_mix_parameters = None\n",
      "12/06/2018 20:43:14 - INFO - allennlp.modules.elmo -   Initializing ELMo\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-64bb115feb91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPredictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://s3-us-west-2.amazonaws.com/allennlp/models/decomposable-attention-elmo-2018.02.19.tar.gz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/predictors/predictor.py\u001b[0m in \u001b[0;36mfrom_path\u001b[0;34m(cls, archive_path, predictor_name)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mPredictor\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mPredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchive_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictor_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/models/archival.py\u001b[0m in \u001b[0;36mload_archive\u001b[0;34m(archive_file, cuda_device, overrides, weights_file)\u001b[0m\n\u001b[1;32m    151\u001b[0m                        \u001b[0mweights_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                        \u001b[0mserialization_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserialization_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                        cuda_device=cuda_device)\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtempdir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/models/model.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, config, serialization_dir, weights_file, cuda_device)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# This allows subclasses of Model to override _load.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mby_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserialization_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/models/model.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(cls, config, serialization_dir, weights_file, cuda_device)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;31m# want the code to look for it, so we remove it from the parameters here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mremove_pretrained_embedding_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0mmodel_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcuda_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/common/from_params.py\u001b[0m in \u001b[0;36mfrom_params\u001b[0;34m(cls, params, **extras)\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0mextras\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mextras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtakes_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msubclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;31m# This is not a base class, so convert our params and extras into a dict of kwargs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/common/from_params.py\u001b[0m in \u001b[0;36mfrom_params\u001b[0;34m(cls, params, **extras)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;31m# This class has a constructor, so create kwargs for it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/common/from_params.py\u001b[0m in \u001b[0;36mcreate_kwargs\u001b[0;34m(cls, params, **extras)\u001b[0m\n\u001b[1;32m    145\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannotation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mby_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannotation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msubextras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;31m# Not optional and not supplied, that's an error!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/common/from_params.py\u001b[0m in \u001b[0;36mfrom_params\u001b[0;34m(cls, params, **extras)\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0mextras\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mextras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtakes_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msubclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;31m# This is not a base class, so convert our params and extras into a dict of kwargs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/modules/text_field_embedders/basic_text_field_embedder.py\u001b[0m in \u001b[0;36mfrom_params\u001b[0;34m(cls, vocab, params)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0membedder_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m                 \u001b[0mtoken_embedders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenEmbedder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedder_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/common/from_params.py\u001b[0m in \u001b[0;36mfrom_params\u001b[0;34m(cls, params, **extras)\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0mextras\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mextras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtakes_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msubclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;31m# This is not a base class, so convert our params and extras into a dict of kwargs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/modules/token_embedders/elmo_token_embedder.py\u001b[0m in \u001b[0;36mfrom_params\u001b[0;34m(cls, vocab, params)\u001b[0m\n\u001b[1;32m    124\u001b[0m                    \u001b[0mprojection_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprojection_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                    \u001b[0mvocab_to_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_to_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                    scalar_mix_parameters=scalar_mix_parameters)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/modules/token_embedders/elmo_token_embedder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, options_file, weight_file, do_layer_norm, dropout, requires_grad, projection_dim, vocab_to_cache, scalar_mix_parameters)\u001b[0m\n\u001b[1;32m     63\u001b[0m                           \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                           \u001b[0mvocab_to_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_to_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                           scalar_mix_parameters=scalar_mix_parameters)\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprojection_dim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_projection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elmo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_output_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprojection_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/modules/elmo.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, options_file, weight_file, num_output_representations, requires_grad, do_layer_norm, dropout, vocab_to_cache, keep_sentence_boundaries, scalar_mix_parameters, module)\u001b[0m\n\u001b[1;32m    106\u001b[0m                                         \u001b[0mweight_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                                         \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                                         vocab_to_cache=vocab_to_cache)\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_cached_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab_to_cache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keep_sentence_boundaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeep_sentence_boundaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/modules/elmo.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, options_file, weight_file, requires_grad, vocab_to_cache)\u001b[0m\n\u001b[1;32m    549\u001b[0m                                    \u001b[0mmemory_cell_clip_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lstm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cell_clip'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m                                    \u001b[0mstate_projection_clip_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lstm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'proj_clip'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                                    requires_grad=requires_grad)\n\u001b[0m\u001b[1;32m    552\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elmo_lstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;31m# Number of representation layers including context independent layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/modules/elmo_lstm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size, hidden_size, cell_size, num_layers, requires_grad, recurrent_dropout_probability, memory_cell_clip_value, state_projection_clip_value)\u001b[0m\n\u001b[1;32m     92\u001b[0m                                                     \u001b[0mrecurrent_dropout_probability\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                                                     \u001b[0mmemory_cell_clip_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                                                     state_projection_clip_value)\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mlstm_input_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/modules/lstm_cell_with_projection.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size, hidden_size, cell_size, go_forward, recurrent_dropout_probability, memory_cell_clip_value, state_projection_clip_value)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# Additional projection matrix for making the hidden state smaller.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_projection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/modules/lstm_cell_with_projection.py\u001b[0m in \u001b[0;36mreset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# Use sensible default initializations for parameters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mblock_orthogonal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_linearity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mblock_orthogonal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_linearity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_linearity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/nn/initializers.py\u001b[0m in \u001b[0;36mblock_orthogonal\u001b[0;34m(tensor, split_sizes, gain)\u001b[0m\n\u001b[1;32m    146\u001b[0m         block_slice = tuple([slice(start_index, start_index + step)\n\u001b[1;32m    147\u001b[0m                              for start_index, step in index_and_step_tuples])\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblock_slice\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morthogonal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblock_slice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/init.py\u001b[0m in \u001b[0;36morthogonal_\u001b[0;34m(tensor, gain)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0mph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m     \u001b[0mq\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrows\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from allennlp.predictors import Predictor\n",
    "predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/decomposable-attention-elmo-2018.02.19.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label_logits': [-3.391864776611328, 4.570619106292725, 0.9505535364151001],\n",
       " 'label_probs': [0.00033908773912116885,\n",
       "  0.9735872745513916,\n",
       "  0.02607356198132038],\n",
       " 'h2p_attention': [[0.6615484952926636,\n",
       "   0.03999358043074608,\n",
       "   0.04555582255125046,\n",
       "   0.046556826680898666,\n",
       "   0.032376978546381,\n",
       "   0.02884303592145443,\n",
       "   0.021681087091565132,\n",
       "   0.02199293114244938,\n",
       "   0.021933946758508682,\n",
       "   0.03280186280608177,\n",
       "   0.02464543841779232,\n",
       "   0.0220700204372406],\n",
       "  [2.6478128347662278e-05,\n",
       "   0.9997804164886475,\n",
       "   2.5384990294696763e-05,\n",
       "   2.803125425998587e-05,\n",
       "   1.5035763681225944e-05,\n",
       "   1.5200890629785135e-05,\n",
       "   2.0365438103908673e-05,\n",
       "   1.5323941624956205e-05,\n",
       "   1.667179458308965e-05,\n",
       "   3.005035614478402e-05,\n",
       "   1.3499801752914209e-05,\n",
       "   1.3597185898106545e-05],\n",
       "  [0.10203886777162552,\n",
       "   0.08704567700624466,\n",
       "   0.11516872048377991,\n",
       "   0.11659414321184158,\n",
       "   0.0897686704993248,\n",
       "   0.1088109016418457,\n",
       "   0.06201941892504692,\n",
       "   0.06663387268781662,\n",
       "   0.06219317018985748,\n",
       "   0.07818535715341568,\n",
       "   0.0593428835272789,\n",
       "   0.052198376506567],\n",
       "  [0.0004895933088846505,\n",
       "   0.0005661703762598336,\n",
       "   0.0014663741458207369,\n",
       "   0.9835996627807617,\n",
       "   0.003986356779932976,\n",
       "   0.0008538846741430461,\n",
       "   0.0008042860426940024,\n",
       "   0.004030915908515453,\n",
       "   0.000624869717285037,\n",
       "   0.002379189943894744,\n",
       "   0.0006751827313564718,\n",
       "   0.0005234954296611249],\n",
       "  [0.02180355414748192,\n",
       "   0.016224386170506477,\n",
       "   0.04582870751619339,\n",
       "   0.32416868209838867,\n",
       "   0.1262902319431305,\n",
       "   0.21321798861026764,\n",
       "   0.03501936420798302,\n",
       "   0.09745176136493683,\n",
       "   0.036249496042728424,\n",
       "   0.04630831629037857,\n",
       "   0.022747211158275604,\n",
       "   0.014690198004245758],\n",
       "  [0.01745988056063652,\n",
       "   0.012007005512714386,\n",
       "   0.03334576636552811,\n",
       "   0.23861074447631836,\n",
       "   0.06420435011386871,\n",
       "   0.42332977056503296,\n",
       "   0.03817636892199516,\n",
       "   0.050691068172454834,\n",
       "   0.05518278479576111,\n",
       "   0.03998453542590141,\n",
       "   0.016478458419442177,\n",
       "   0.010529308579862118],\n",
       "  [0.010657047852873802,\n",
       "   0.011342254467308521,\n",
       "   0.013355554081499577,\n",
       "   0.39848199486732483,\n",
       "   0.026157716289162636,\n",
       "   0.050147585570812225,\n",
       "   0.36920735239982605,\n",
       "   0.02936599962413311,\n",
       "   0.03243587911128998,\n",
       "   0.04311087727546692,\n",
       "   0.008368291892111301,\n",
       "   0.00736935855820775],\n",
       "  [0.012434241361916065,\n",
       "   0.013259582221508026,\n",
       "   0.03426486626267433,\n",
       "   0.24050143361091614,\n",
       "   0.13842594623565674,\n",
       "   0.37977859377861023,\n",
       "   0.06643011420965195,\n",
       "   0.05420059710741043,\n",
       "   0.014995560981333256,\n",
       "   0.025156904011964798,\n",
       "   0.01212186086922884,\n",
       "   0.008430331014096737],\n",
       "  [0.025600627064704895,\n",
       "   0.02177058905363083,\n",
       "   0.05209308862686157,\n",
       "   0.08083993941545486,\n",
       "   0.0719628632068634,\n",
       "   0.5535196661949158,\n",
       "   0.05518198758363724,\n",
       "   0.037947289645671844,\n",
       "   0.030034935101866722,\n",
       "   0.040441691875457764,\n",
       "   0.01738293468952179,\n",
       "   0.013224351219832897],\n",
       "  [0.0034482465125620365,\n",
       "   0.0038895425386726856,\n",
       "   0.005236065946519375,\n",
       "   0.013813172467052937,\n",
       "   0.010430196300148964,\n",
       "   0.05077805370092392,\n",
       "   0.641498327255249,\n",
       "   0.01323755457997322,\n",
       "   0.16897264122962952,\n",
       "   0.08184266090393066,\n",
       "   0.0035076595377177,\n",
       "   0.0033459344413131475],\n",
       "  [0.011839332059025764,\n",
       "   0.009265673346817493,\n",
       "   0.020825976505875587,\n",
       "   0.4289425313472748,\n",
       "   0.027788354083895683,\n",
       "   0.02105570212006569,\n",
       "   0.014683406800031662,\n",
       "   0.3304857909679413,\n",
       "   0.05788344889879227,\n",
       "   0.0527602918446064,\n",
       "   0.017025265842676163,\n",
       "   0.00744414608925581],\n",
       "  [0.027381373569369316,\n",
       "   0.024950889870524406,\n",
       "   0.05205394700169563,\n",
       "   0.4092806279659271,\n",
       "   0.08168795704841614,\n",
       "   0.0769209936261177,\n",
       "   0.02724587544798851,\n",
       "   0.16947263479232788,\n",
       "   0.047082576900720596,\n",
       "   0.05163945257663727,\n",
       "   0.02144119143486023,\n",
       "   0.010842563584446907],\n",
       "  [0.0038758176378905773,\n",
       "   0.003917417023330927,\n",
       "   0.005418309476226568,\n",
       "   0.045663487166166306,\n",
       "   0.007978193461894989,\n",
       "   0.009805173613131046,\n",
       "   0.012987712398171425,\n",
       "   0.11252029985189438,\n",
       "   0.21147406101226807,\n",
       "   0.5791746377944946,\n",
       "   0.004015128128230572,\n",
       "   0.003169688628986478],\n",
       "  [0.07595512270927429,\n",
       "   0.06700103729963303,\n",
       "   0.08602173626422882,\n",
       "   0.13264940679073334,\n",
       "   0.0768849328160286,\n",
       "   0.08079946041107178,\n",
       "   0.07029906660318375,\n",
       "   0.09581216424703598,\n",
       "   0.08089634776115417,\n",
       "   0.074249267578125,\n",
       "   0.0905224159359932,\n",
       "   0.06890902668237686],\n",
       "  [0.08349008858203888,\n",
       "   0.08230611681938171,\n",
       "   0.08249524235725403,\n",
       "   0.0853073000907898,\n",
       "   0.07970943301916122,\n",
       "   0.0837709978222847,\n",
       "   0.08443307131528854,\n",
       "   0.08118802309036255,\n",
       "   0.08668394386768341,\n",
       "   0.0874016061425209,\n",
       "   0.08157685399055481,\n",
       "   0.08163739740848541]],\n",
       " 'p2h_attention': [[0.5873391032218933,\n",
       "   0.037481389939785004,\n",
       "   0.036728233098983765,\n",
       "   0.01829739846289158,\n",
       "   0.028163855895400047,\n",
       "   0.031889576464891434,\n",
       "   0.026729749515652657,\n",
       "   0.027614938095211983,\n",
       "   0.03713584318757057,\n",
       "   0.02042875997722149,\n",
       "   0.034058839082717896,\n",
       "   0.05014196038246155,\n",
       "   0.02343931421637535,\n",
       "   0.0210875291377306,\n",
       "   0.019463635981082916],\n",
       "  [2.508237594156526e-05,\n",
       "   0.999733567237854,\n",
       "   2.2132628146209754e-05,\n",
       "   1.4946935152693186e-05,\n",
       "   1.4804178135818802e-05,\n",
       "   1.5491494195885025e-05,\n",
       "   2.0095949366805144e-05,\n",
       "   2.0802033759537153e-05,\n",
       "   2.2308173356577754e-05,\n",
       "   1.6277719623758458e-05,\n",
       "   1.8829146938514896e-05,\n",
       "   3.227626802981831e-05,\n",
       "   1.6735255485400558e-05,\n",
       "   1.3140176633896772e-05,\n",
       "   1.3554149518313352e-05],\n",
       "  [0.05465352162718773,\n",
       "   0.04855705797672272,\n",
       "   0.056016504764556885,\n",
       "   0.07405351102352142,\n",
       "   0.07999254763126373,\n",
       "   0.0822991207242012,\n",
       "   0.04526546597480774,\n",
       "   0.10283025354146957,\n",
       "   0.1021103709936142,\n",
       "   0.04191756993532181,\n",
       "   0.08095712214708328,\n",
       "   0.12880918383598328,\n",
       "   0.044278454035520554,\n",
       "   0.0322718508541584,\n",
       "   0.025987526401877403],\n",
       "  [0.0009891841327771544,\n",
       "   0.0009495936101302505,\n",
       "   0.001004332909360528,\n",
       "   0.879708468914032,\n",
       "   0.010020802728831768,\n",
       "   0.010429514572024345,\n",
       "   0.02391846291720867,\n",
       "   0.012782299891114235,\n",
       "   0.0028063070494681597,\n",
       "   0.0019584111869335175,\n",
       "   0.02953033149242401,\n",
       "   0.0179363414645195,\n",
       "   0.006608717143535614,\n",
       "   0.00088133366080001,\n",
       "   0.00047592856572009623],\n",
       "  [0.021002642810344696,\n",
       "   0.01555121410638094,\n",
       "   0.023608563467860222,\n",
       "   0.10885298252105713,\n",
       "   0.11919141560792923,\n",
       "   0.08568055927753448,\n",
       "   0.04793670400977135,\n",
       "   0.22462216019630432,\n",
       "   0.07627135515213013,\n",
       "   0.04514884203672409,\n",
       "   0.0584084689617157,\n",
       "   0.10929856449365616,\n",
       "   0.035253044217824936,\n",
       "   0.01559625193476677,\n",
       "   0.013577163219451904],\n",
       "  [0.007228714879602194,\n",
       "   0.0060742199420928955,\n",
       "   0.011056041345000267,\n",
       "   0.00900836382061243,\n",
       "   0.07774664461612701,\n",
       "   0.21826252341270447,\n",
       "   0.035505931824445724,\n",
       "   0.23809383809566498,\n",
       "   0.2266567349433899,\n",
       "   0.08492054045200348,\n",
       "   0.01709878444671631,\n",
       "   0.0397634282708168,\n",
       "   0.0167390089482069,\n",
       "   0.0063324240036308765,\n",
       "   0.005512842908501625],\n",
       "  [0.0035782784689217806,\n",
       "   0.005359055008739233,\n",
       "   0.004149807151407003,\n",
       "   0.0055876621045172215,\n",
       "   0.008408895693719387,\n",
       "   0.012961878441274166,\n",
       "   0.17214486002922058,\n",
       "   0.027425561100244522,\n",
       "   0.014880097471177578,\n",
       "   0.7064885497093201,\n",
       "   0.007852272130548954,\n",
       "   0.009274972602725029,\n",
       "   0.014600914902985096,\n",
       "   0.003628138452768326,\n",
       "   0.0036590411327779293],\n",
       "  [0.007103194482624531,\n",
       "   0.007891187444329262,\n",
       "   0.008725148625671864,\n",
       "   0.054802581667900085,\n",
       "   0.04579288512468338,\n",
       "   0.03368080407381058,\n",
       "   0.02679453045129776,\n",
       "   0.04378972575068474,\n",
       "   0.0200247373431921,\n",
       "   0.02852955460548401,\n",
       "   0.34585878252983093,\n",
       "   0.11289872974157333,\n",
       "   0.24754595756530762,\n",
       "   0.009676819667220116,\n",
       "   0.006885323207825422],\n",
       "  [0.0065566846169531345,\n",
       "   0.007946044206619263,\n",
       "   0.0075373295694589615,\n",
       "   0.007862917147576809,\n",
       "   0.015765480697155,\n",
       "   0.03393528610467911,\n",
       "   0.02739201858639717,\n",
       "   0.011213157325983047,\n",
       "   0.014669311232864857,\n",
       "   0.33705487847328186,\n",
       "   0.05606571584939957,\n",
       "   0.029029978439211845,\n",
       "   0.43060502409935,\n",
       "   0.007562020793557167,\n",
       "   0.00680405693128705],\n",
       "  [0.00604318268597126,\n",
       "   0.00882710050791502,\n",
       "   0.00583982327952981,\n",
       "   0.018451131880283356,\n",
       "   0.012412630952894688,\n",
       "   0.015154428780078888,\n",
       "   0.022438034415245056,\n",
       "   0.011593696661293507,\n",
       "   0.01217340212315321,\n",
       "   0.10061518102884293,\n",
       "   0.0314955934882164,\n",
       "   0.019623102620244026,\n",
       "   0.7268270254135132,\n",
       "   0.0042776064947247505,\n",
       "   0.004228129982948303],\n",
       "  [0.05502630025148392,\n",
       "   0.048057641834020615,\n",
       "   0.05371672287583351,\n",
       "   0.06345729529857635,\n",
       "   0.07389234751462936,\n",
       "   0.07568860054016113,\n",
       "   0.052783865481615067,\n",
       "   0.06770184636116028,\n",
       "   0.06341211497783661,\n",
       "   0.05225980281829834,\n",
       "   0.1231694370508194,\n",
       "   0.09874189645051956,\n",
       "   0.06106431409716606,\n",
       "   0.06320204585790634,\n",
       "   0.04782582074403763],\n",
       "  [0.06751631200313568,\n",
       "   0.06632179766893387,\n",
       "   0.06473962217569351,\n",
       "   0.06741329282522202,\n",
       "   0.06538397073745728,\n",
       "   0.0662652850151062,\n",
       "   0.06368929892778397,\n",
       "   0.06451314687728882,\n",
       "   0.0660991445183754,\n",
       "   0.06830303370952606,\n",
       "   0.07378979027271271,\n",
       "   0.06841585785150528,\n",
       "   0.06605063378810883,\n",
       "   0.06592094898223877,\n",
       "   0.06557781249284744]],\n",
       " 'premise_tokens': ['Two',\n",
       "  'women',\n",
       "  'are',\n",
       "  'wandering',\n",
       "  'along',\n",
       "  'the',\n",
       "  'shore',\n",
       "  'drinking',\n",
       "  'iced',\n",
       "  'tea',\n",
       "  '.',\n",
       "  '@@NULL@@'],\n",
       " 'hypothesis_tokens': ['Two',\n",
       "  'women',\n",
       "  'are',\n",
       "  'sitting',\n",
       "  'on',\n",
       "  'a',\n",
       "  'blanket',\n",
       "  'near',\n",
       "  'some',\n",
       "  'rocks',\n",
       "  'talking',\n",
       "  'about',\n",
       "  'politics',\n",
       "  '.',\n",
       "  '@@NULL@@']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = predictor.predict(\n",
    "  hypothesis=\"Two women are sitting on a blanket near some rocks talking about politics.\",\n",
    "  premise=\"Two women are wandering along the shore drinking iced tea.\"\n",
    ")\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(prediction['premise_tokens'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I guess I am feeling kinda tired. I feel overwhelmed, a bit, maybe hungry. I dunno. I find myself wanting something, but I'm not sure what it is. I feel stressed certainly, too much to do maybe? But I'm not totally sure what I should be doing? Now it's a lot later and it's really time for me to get to bed...but a part of me wants to stay up, nonetheless\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame([], columns=['premise', 'hypothesis', 'entailment', 'contradiction', 'neutral', 'e+c'])\n",
    "i = 0\n",
    "for premise in doc.sents:\n",
    "#     entailment, contradiction, neutral = None\n",
    "    for hypothesis in doc.sents:\n",
    "        if (premise != hypothesis):\n",
    "            prediction = predictor.predict(hypothesis=hypothesis.text, premise=premise.text)\n",
    "            entailment, contradiction, neutral = prediction['label_probs']\n",
    "            results.loc[i] = [premise.text, hypothesis.text, entailment, contradiction, neutral, (entailment + (1 - contradiction)) / 2]\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>entailment</th>\n",
       "      <th>contradiction</th>\n",
       "      <th>neutral</th>\n",
       "      <th>e+c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>0.956455</td>\n",
       "      <td>0.001234</td>\n",
       "      <td>0.042311</td>\n",
       "      <td>0.977611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>0.908410</td>\n",
       "      <td>0.005447</td>\n",
       "      <td>0.086143</td>\n",
       "      <td>0.951481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>I dunno.</td>\n",
       "      <td>0.904193</td>\n",
       "      <td>0.053517</td>\n",
       "      <td>0.042290</td>\n",
       "      <td>0.925338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>I dunno.</td>\n",
       "      <td>0.912021</td>\n",
       "      <td>0.070635</td>\n",
       "      <td>0.017344</td>\n",
       "      <td>0.920693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>0.836310</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.162772</td>\n",
       "      <td>0.917696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>I dunno.</td>\n",
       "      <td>0.867386</td>\n",
       "      <td>0.043489</td>\n",
       "      <td>0.089125</td>\n",
       "      <td>0.911948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>0.781964</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>0.216260</td>\n",
       "      <td>0.890094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>0.755611</td>\n",
       "      <td>0.008080</td>\n",
       "      <td>0.236309</td>\n",
       "      <td>0.873766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>0.767259</td>\n",
       "      <td>0.020967</td>\n",
       "      <td>0.211775</td>\n",
       "      <td>0.873146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>0.768105</td>\n",
       "      <td>0.025418</td>\n",
       "      <td>0.206478</td>\n",
       "      <td>0.871344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>0.739140</td>\n",
       "      <td>0.012235</td>\n",
       "      <td>0.248626</td>\n",
       "      <td>0.863452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>0.725396</td>\n",
       "      <td>0.002819</td>\n",
       "      <td>0.271786</td>\n",
       "      <td>0.861288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>0.735134</td>\n",
       "      <td>0.026731</td>\n",
       "      <td>0.238136</td>\n",
       "      <td>0.854201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>0.727070</td>\n",
       "      <td>0.022596</td>\n",
       "      <td>0.250334</td>\n",
       "      <td>0.852237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>0.717547</td>\n",
       "      <td>0.035562</td>\n",
       "      <td>0.246891</td>\n",
       "      <td>0.840992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>0.678346</td>\n",
       "      <td>0.013190</td>\n",
       "      <td>0.308464</td>\n",
       "      <td>0.832578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>0.659936</td>\n",
       "      <td>0.050289</td>\n",
       "      <td>0.289775</td>\n",
       "      <td>0.804823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>0.604333</td>\n",
       "      <td>0.006409</td>\n",
       "      <td>0.389258</td>\n",
       "      <td>0.798962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>0.682849</td>\n",
       "      <td>0.088772</td>\n",
       "      <td>0.228379</td>\n",
       "      <td>0.797039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>I dunno.</td>\n",
       "      <td>0.762202</td>\n",
       "      <td>0.184278</td>\n",
       "      <td>0.053520</td>\n",
       "      <td>0.788962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>0.567174</td>\n",
       "      <td>0.003467</td>\n",
       "      <td>0.429359</td>\n",
       "      <td>0.781854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>0.584167</td>\n",
       "      <td>0.030758</td>\n",
       "      <td>0.385075</td>\n",
       "      <td>0.776704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>0.566095</td>\n",
       "      <td>0.020107</td>\n",
       "      <td>0.413798</td>\n",
       "      <td>0.772994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>0.589310</td>\n",
       "      <td>0.043696</td>\n",
       "      <td>0.366994</td>\n",
       "      <td>0.772807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>0.571464</td>\n",
       "      <td>0.038498</td>\n",
       "      <td>0.390039</td>\n",
       "      <td>0.766483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>0.566257</td>\n",
       "      <td>0.038796</td>\n",
       "      <td>0.394947</td>\n",
       "      <td>0.763730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>0.538478</td>\n",
       "      <td>0.052767</td>\n",
       "      <td>0.408755</td>\n",
       "      <td>0.742856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>I dunno.</td>\n",
       "      <td>0.715452</td>\n",
       "      <td>0.238567</td>\n",
       "      <td>0.045980</td>\n",
       "      <td>0.738443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>0.548609</td>\n",
       "      <td>0.073129</td>\n",
       "      <td>0.378262</td>\n",
       "      <td>0.737740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>0.474978</td>\n",
       "      <td>0.064011</td>\n",
       "      <td>0.461011</td>\n",
       "      <td>0.705483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.081693</td>\n",
       "      <td>0.458307</td>\n",
       "      <td>0.689153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>0.523283</td>\n",
       "      <td>0.235219</td>\n",
       "      <td>0.241498</td>\n",
       "      <td>0.644032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>0.381097</td>\n",
       "      <td>0.175689</td>\n",
       "      <td>0.443214</td>\n",
       "      <td>0.602704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>0.373584</td>\n",
       "      <td>0.176693</td>\n",
       "      <td>0.449723</td>\n",
       "      <td>0.598446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>0.386267</td>\n",
       "      <td>0.217766</td>\n",
       "      <td>0.395967</td>\n",
       "      <td>0.584250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>0.329103</td>\n",
       "      <td>0.298593</td>\n",
       "      <td>0.372304</td>\n",
       "      <td>0.515255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>0.269745</td>\n",
       "      <td>0.259522</td>\n",
       "      <td>0.470732</td>\n",
       "      <td>0.505112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>I dunno.</td>\n",
       "      <td>0.285518</td>\n",
       "      <td>0.384351</td>\n",
       "      <td>0.330131</td>\n",
       "      <td>0.450584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>0.234756</td>\n",
       "      <td>0.339052</td>\n",
       "      <td>0.426192</td>\n",
       "      <td>0.447852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>0.247193</td>\n",
       "      <td>0.352285</td>\n",
       "      <td>0.400522</td>\n",
       "      <td>0.447454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.486933</td>\n",
       "      <td>0.479734</td>\n",
       "      <td>0.273200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>0.045921</td>\n",
       "      <td>0.508242</td>\n",
       "      <td>0.445838</td>\n",
       "      <td>0.268840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>0.073138</td>\n",
       "      <td>0.543047</td>\n",
       "      <td>0.383816</td>\n",
       "      <td>0.265046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>0.019627</td>\n",
       "      <td>0.526356</td>\n",
       "      <td>0.454018</td>\n",
       "      <td>0.246635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>I dunno.</td>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>0.038739</td>\n",
       "      <td>0.573239</td>\n",
       "      <td>0.388022</td>\n",
       "      <td>0.232750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>0.102812</td>\n",
       "      <td>0.712032</td>\n",
       "      <td>0.185157</td>\n",
       "      <td>0.195390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>I dunno.</td>\n",
       "      <td>0.021096</td>\n",
       "      <td>0.634602</td>\n",
       "      <td>0.344303</td>\n",
       "      <td>0.193247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>I dunno.</td>\n",
       "      <td>0.018045</td>\n",
       "      <td>0.898486</td>\n",
       "      <td>0.083470</td>\n",
       "      <td>0.059780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>I dunno.</td>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>0.000822</td>\n",
       "      <td>0.894391</td>\n",
       "      <td>0.104787</td>\n",
       "      <td>0.053215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>0.003158</td>\n",
       "      <td>0.925812</td>\n",
       "      <td>0.071030</td>\n",
       "      <td>0.038673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              premise  \\\n",
       "43   But I'm not totally sure what I should be doing?   \n",
       "44   But I'm not totally sure what I should be doing?   \n",
       "1                   I guess I am feeling kinda tired.   \n",
       "34   I feel stressed certainly, too much to do maybe?   \n",
       "8            I feel overwhelmed, a bit, maybe hungry.   \n",
       "9            I feel overwhelmed, a bit, maybe hungry.   \n",
       "32   I feel stressed certainly, too much to do maybe?   \n",
       "35   I feel stressed certainly, too much to do maybe?   \n",
       "27  I find myself wanting something, but I'm not s...   \n",
       "47   But I'm not totally sure what I should be doing?   \n",
       "40   But I'm not totally sure what I should be doing?   \n",
       "0                   I guess I am feeling kinda tired.   \n",
       "31  I find myself wanting something, but I'm not s...   \n",
       "41   But I'm not totally sure what I should be doing?   \n",
       "10           I feel overwhelmed, a bit, maybe hungry.   \n",
       "11           I feel overwhelmed, a bit, maybe hungry.   \n",
       "29  I find myself wanting something, but I'm not s...   \n",
       "3                   I guess I am feeling kinda tired.   \n",
       "39   I feel stressed certainly, too much to do maybe?   \n",
       "42   But I'm not totally sure what I should be doing?   \n",
       "33   I feel stressed certainly, too much to do maybe?   \n",
       "67     but a part of me wants to stay up, nonetheless   \n",
       "24  I find myself wanting something, but I'm not s...   \n",
       "36   I feel stressed certainly, too much to do maybe?   \n",
       "25  I find myself wanting something, but I'm not s...   \n",
       "70     but a part of me wants to stay up, nonetheless   \n",
       "68     but a part of me wants to stay up, nonetheless   \n",
       "26  I find myself wanting something, but I'm not s...   \n",
       "37   I feel stressed certainly, too much to do maybe?   \n",
       "2                   I guess I am feeling kinda tired.   \n",
       "28  I find myself wanting something, but I'm not s...   \n",
       "63       and it's really time for me to get to bed...   \n",
       "45   But I'm not totally sure what I should be doing?   \n",
       "12           I feel overwhelmed, a bit, maybe hungry.   \n",
       "55                               Now it's a lot later   \n",
       "15           I feel overwhelmed, a bit, maybe hungry.   \n",
       "4                   I guess I am feeling kinda tired.   \n",
       "66     but a part of me wants to stay up, nonetheless   \n",
       "59       and it's really time for me to get to bed...   \n",
       "51                               Now it's a lot later   \n",
       "30  I find myself wanting something, but I'm not s...   \n",
       "53                               Now it's a lot later   \n",
       "61       and it's really time for me to get to bed...   \n",
       "46   But I'm not totally sure what I should be doing?   \n",
       "21                                           I dunno.   \n",
       "60       and it's really time for me to get to bed...   \n",
       "50                               Now it's a lot later   \n",
       "58       and it's really time for me to get to bed...   \n",
       "22                                           I dunno.   \n",
       "6                   I guess I am feeling kinda tired.   \n",
       "\n",
       "                                           hypothesis  entailment  \\\n",
       "43  I find myself wanting something, but I'm not s...    0.956455   \n",
       "44   I feel stressed certainly, too much to do maybe?    0.908410   \n",
       "1                                            I dunno.    0.904193   \n",
       "34                                           I dunno.    0.912021   \n",
       "8                   I guess I am feeling kinda tired.    0.836310   \n",
       "9                                            I dunno.    0.867386   \n",
       "32                  I guess I am feeling kinda tired.    0.781964   \n",
       "35  I find myself wanting something, but I'm not s...    0.755611   \n",
       "27   I feel stressed certainly, too much to do maybe?    0.767259   \n",
       "47     but a part of me wants to stay up, nonetheless    0.768105   \n",
       "40                  I guess I am feeling kinda tired.    0.739140   \n",
       "0            I feel overwhelmed, a bit, maybe hungry.    0.725396   \n",
       "31     but a part of me wants to stay up, nonetheless    0.735134   \n",
       "41           I feel overwhelmed, a bit, maybe hungry.    0.727070   \n",
       "10  I find myself wanting something, but I'm not s...    0.717547   \n",
       "11   I feel stressed certainly, too much to do maybe?    0.678346   \n",
       "29                               Now it's a lot later    0.659936   \n",
       "3    I feel stressed certainly, too much to do maybe?    0.604333   \n",
       "39     but a part of me wants to stay up, nonetheless    0.682849   \n",
       "42                                           I dunno.    0.762202   \n",
       "33           I feel overwhelmed, a bit, maybe hungry.    0.567174   \n",
       "67  I find myself wanting something, but I'm not s...    0.584167   \n",
       "24                  I guess I am feeling kinda tired.    0.566095   \n",
       "36   But I'm not totally sure what I should be doing?    0.589310   \n",
       "25           I feel overwhelmed, a bit, maybe hungry.    0.571464   \n",
       "70                               Now it's a lot later    0.566257   \n",
       "68   I feel stressed certainly, too much to do maybe?    0.538478   \n",
       "26                                           I dunno.    0.715452   \n",
       "37                               Now it's a lot later    0.548609   \n",
       "2   I find myself wanting something, but I'm not s...    0.474978   \n",
       "28   But I'm not totally sure what I should be doing?    0.460000   \n",
       "63     but a part of me wants to stay up, nonetheless    0.523283   \n",
       "45                               Now it's a lot later    0.381097   \n",
       "12   But I'm not totally sure what I should be doing?    0.373584   \n",
       "55     but a part of me wants to stay up, nonetheless    0.386267   \n",
       "15     but a part of me wants to stay up, nonetheless    0.329103   \n",
       "4    But I'm not totally sure what I should be doing?    0.269745   \n",
       "66                                           I dunno.    0.285518   \n",
       "59  I find myself wanting something, but I'm not s...    0.234756   \n",
       "51  I find myself wanting something, but I'm not s...    0.247193   \n",
       "30       and it's really time for me to get to bed...    0.033333   \n",
       "53   But I'm not totally sure what I should be doing?    0.045921   \n",
       "61   But I'm not totally sure what I should be doing?    0.073138   \n",
       "46       and it's really time for me to get to bed...    0.019627   \n",
       "21                               Now it's a lot later    0.038739   \n",
       "60   I feel stressed certainly, too much to do maybe?    0.102812   \n",
       "50                                           I dunno.    0.021096   \n",
       "58                                           I dunno.    0.018045   \n",
       "22       and it's really time for me to get to bed...    0.000822   \n",
       "6        and it's really time for me to get to bed...    0.003158   \n",
       "\n",
       "    contradiction   neutral       e+c  \n",
       "43       0.001234  0.042311  0.977611  \n",
       "44       0.005447  0.086143  0.951481  \n",
       "1        0.053517  0.042290  0.925338  \n",
       "34       0.070635  0.017344  0.920693  \n",
       "8        0.000918  0.162772  0.917696  \n",
       "9        0.043489  0.089125  0.911948  \n",
       "32       0.001776  0.216260  0.890094  \n",
       "35       0.008080  0.236309  0.873766  \n",
       "27       0.020967  0.211775  0.873146  \n",
       "47       0.025418  0.206478  0.871344  \n",
       "40       0.012235  0.248626  0.863452  \n",
       "0        0.002819  0.271786  0.861288  \n",
       "31       0.026731  0.238136  0.854201  \n",
       "41       0.022596  0.250334  0.852237  \n",
       "10       0.035562  0.246891  0.840992  \n",
       "11       0.013190  0.308464  0.832578  \n",
       "29       0.050289  0.289775  0.804823  \n",
       "3        0.006409  0.389258  0.798962  \n",
       "39       0.088772  0.228379  0.797039  \n",
       "42       0.184278  0.053520  0.788962  \n",
       "33       0.003467  0.429359  0.781854  \n",
       "67       0.030758  0.385075  0.776704  \n",
       "24       0.020107  0.413798  0.772994  \n",
       "36       0.043696  0.366994  0.772807  \n",
       "25       0.038498  0.390039  0.766483  \n",
       "70       0.038796  0.394947  0.763730  \n",
       "68       0.052767  0.408755  0.742856  \n",
       "26       0.238567  0.045980  0.738443  \n",
       "37       0.073129  0.378262  0.737740  \n",
       "2        0.064011  0.461011  0.705483  \n",
       "28       0.081693  0.458307  0.689153  \n",
       "63       0.235219  0.241498  0.644032  \n",
       "45       0.175689  0.443214  0.602704  \n",
       "12       0.176693  0.449723  0.598446  \n",
       "55       0.217766  0.395967  0.584250  \n",
       "15       0.298593  0.372304  0.515255  \n",
       "4        0.259522  0.470732  0.505112  \n",
       "66       0.384351  0.330131  0.450584  \n",
       "59       0.339052  0.426192  0.447852  \n",
       "51       0.352285  0.400522  0.447454  \n",
       "30       0.486933  0.479734  0.273200  \n",
       "53       0.508242  0.445838  0.268840  \n",
       "61       0.543047  0.383816  0.265046  \n",
       "46       0.526356  0.454018  0.246635  \n",
       "21       0.573239  0.388022  0.232750  \n",
       "60       0.712032  0.185157  0.195390  \n",
       "50       0.634602  0.344303  0.193247  \n",
       "58       0.898486  0.083470  0.059780  \n",
       "22       0.894391  0.104787  0.053215  \n",
       "6        0.925812  0.071030  0.038673  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(by='e+c', ascending=False).loc[results['neutral'] < .5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = 'I feel stressed'\n",
    "\n",
    "results = pd.DataFrame([], columns=['premise', 'hypothesis', 'entailment', 'contradiction', 'neutral'])\n",
    "i = 0\n",
    "for premise in doc.sents:\n",
    "    prediction = predictor.predict(hypothesis=hypothesis, premise=premise.text)\n",
    "    entailment, contradiction, neutral = prediction['label_probs']\n",
    "    results.loc[i] = [premise.text, hypothesis, entailment, contradiction, neutral]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>entailment</th>\n",
       "      <th>contradiction</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>I feel stressed</td>\n",
       "      <td>0.985132</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.014467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>I feel stressed</td>\n",
       "      <td>0.936851</td>\n",
       "      <td>0.002266</td>\n",
       "      <td>0.060882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>I feel stressed</td>\n",
       "      <td>0.933847</td>\n",
       "      <td>0.002187</td>\n",
       "      <td>0.063966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>I feel stressed</td>\n",
       "      <td>0.833155</td>\n",
       "      <td>0.004319</td>\n",
       "      <td>0.162525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>I feel stressed</td>\n",
       "      <td>0.769592</td>\n",
       "      <td>0.041008</td>\n",
       "      <td>0.189401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I dunno.</td>\n",
       "      <td>I feel stressed</td>\n",
       "      <td>0.493208</td>\n",
       "      <td>0.287141</td>\n",
       "      <td>0.219651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>I feel stressed</td>\n",
       "      <td>0.293085</td>\n",
       "      <td>0.115519</td>\n",
       "      <td>0.591396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>I feel stressed</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>0.353283</td>\n",
       "      <td>0.537717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>I feel stressed</td>\n",
       "      <td>0.081763</td>\n",
       "      <td>0.259905</td>\n",
       "      <td>0.658333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             premise       hypothesis  \\\n",
       "4   I feel stressed certainly, too much to do maybe?  I feel stressed   \n",
       "0                  I guess I am feeling kinda tired.  I feel stressed   \n",
       "1           I feel overwhelmed, a bit, maybe hungry.  I feel stressed   \n",
       "3  I find myself wanting something, but I'm not s...  I feel stressed   \n",
       "5   But I'm not totally sure what I should be doing?  I feel stressed   \n",
       "2                                           I dunno.  I feel stressed   \n",
       "8     but a part of me wants to stay up, nonetheless  I feel stressed   \n",
       "7       and it's really time for me to get to bed...  I feel stressed   \n",
       "6                               Now it's a lot later  I feel stressed   \n",
       "\n",
       "   entailment  contradiction   neutral  \n",
       "4    0.985132       0.000401  0.014467  \n",
       "0    0.936851       0.002266  0.060882  \n",
       "1    0.933847       0.002187  0.063966  \n",
       "3    0.833155       0.004319  0.162525  \n",
       "5    0.769592       0.041008  0.189401  \n",
       "2    0.493208       0.287141  0.219651  \n",
       "8    0.293085       0.115519  0.591396  \n",
       "7    0.109000       0.353283  0.537717  \n",
       "6    0.081763       0.259905  0.658333  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(by='entailment', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo(shape):\n",
    "    nlp = spacy.load('en_vectors_web_lg')\n",
    "    nlp.add_pipe(KerasSimilarityShim.load(nlp.path / 'similarity', nlp, shape[0]))\n",
    "\n",
    "    doc1 = nlp(u'The king of France is bald.')\n",
    "    doc2 = nlp(u'France has no king.')\n",
    "\n",
    "    print(\"Sentence 1:\", doc1)\n",
    "    print(\"Sentence 2:\", doc2)\n",
    "\n",
    "    entailment_type, confidence = doc1.similarity(doc2)\n",
    "    print(\"Entailment type:\", entailment_type, \"(Confidence:\", confidence, \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textacy.vsm import Vectorizer\n",
    "vectorizer = Vectorizer(\n",
    "    tf_type='linear', apply_idf=True, idf_type='smooth', norm='l2',\n",
    "    min_df=3, max_df=0.95, max_n_terms=100000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = textacy.tm.TopicModel('nmf', n_topics=20)\n",
    "model.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy.keyterms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sure', 0.17393909018065556),\n",
       " ('overwhelmed', 0.1291297332498814),\n",
       " ('time', 0.12848449534695075),\n",
       " ('bit', 0.12152651186559832),\n",
       " ('lot', 0.12055447304217964),\n",
       " ('hungry', 0.11759733293982846),\n",
       " ('tired', 0.07154722366605952),\n",
       " ('bed', 0.0712721534037857),\n",
       " ('stressed', 0.06594898630506062)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms = textacy.keyterms.key_terms_from_semantic_network(doc)\n",
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sure', 0.33850825516726607),\n",
       " ('stressed', 0.1517767729454664),\n",
       " ('bed', 0.1484936700017998),\n",
       " ('time', 0.0968496495027204),\n",
       " ('lot', 0.07202910699164278),\n",
       " ('hungry', 0.07058674833731196),\n",
       " ('bit', 0.051420957354160426),\n",
       " ('overwhelmed', 0.03964374936267677),\n",
       " ('tired', 0.030691090336955648)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms = textacy.keyterms.sgrank(doc)\n",
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I guess I am feeling kinda tired. I feel overwhelmed, a bit, maybe hungry. I dunno. I find myself wanting something, but I'm not sure what it is. I feel stressed certainly, too much to do maybe? But I'm not totally sure what I should be doing? Now it's a lot later and it's really time for me to get to bed...but a part of me wants to stay up, nonetheless\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy.lexicon_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/05/2018 21:16:22 - INFO - textacy.lexicon_methods -   Downloaded DepecheMood (4MB) from https://github.com/marcoguerini/DepecheMood/releases/download/v1.0/DepecheMood_V1.0.zip and wrote it to data\n"
     ]
    }
   ],
   "source": [
    "textacy.lexicon_methods.download_depechemood(data_dir='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {'AFRAID': 0.1103335419756097,\n",
       "             'AMUSED': 0.14808456209756102,\n",
       "             'ANGRY': 0.10694153351219511,\n",
       "             'ANNOYED': 0.12443051617073168,\n",
       "             'DONT_CARE': 0.13096818899999998,\n",
       "             'HAPPY': 0.11531756726829266,\n",
       "             'INSPIRED': 0.14843126431707318,\n",
       "             'SAD': 0.11549282553658531})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textacy.lexicon_methods.emotional_valence(words=[word for word in doc], dm_data_dir='data/DepecheMood_V1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2018 17:58:27 - INFO - event2mind_hack -   loading archive file data/event2mind.tar.gz\n",
      "12/06/2018 17:58:27 - INFO - event2mind_hack -   extracting archive file data/event2mind.tar.gz to temp dir /tmp/tmp0dlhchct\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   vocabulary.type = default\n",
      "12/06/2018 17:58:28 - INFO - allennlp.data.vocabulary -   Loading token dictionary from /tmp/tmp0dlhchct/vocabulary.\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'event2mind_hack.Model'> from params {'embedding_dropout': 0.2, 'encoder': {'bidirectional': True, 'hidden_size': 50, 'input_size': 300, 'num_layers': 1, 'type': 'gru'}, 'max_decoding_steps': 10, 'source_embedder': {'token_embedders': {'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding', 'vocab_namespace': 'source_tokens'}}}, 'target_namespace': 'target_tokens', 'type': 'event2mind'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fccd4e752e8>}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.type = event2mind\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'event2mind_hack.Event2Mind'> from params {'embedding_dropout': 0.2, 'encoder': {'bidirectional': True, 'hidden_size': 50, 'input_size': 300, 'num_layers': 1, 'type': 'gru'}, 'max_decoding_steps': 10, 'source_embedder': {'token_embedders': {'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding', 'vocab_namespace': 'source_tokens'}}}, 'target_namespace': 'target_tokens'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fccd4e752e8>}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'token_embedders': {'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding', 'vocab_namespace': 'source_tokens'}}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fccd4e752e8>}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.type = basic\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.embedder_to_indexer_map = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.allow_unmatched_keys = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 300, 'trainable': False, 'type': 'embedding', 'vocab_namespace': 'source_tokens'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fccd4e752e8>}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.type = embedding\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.num_embeddings = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.vocab_namespace = source_tokens\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.embedding_dim = 300\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.pretrained_file = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.projection_dim = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.trainable = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.padding_index = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.max_norm = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.norm_type = 2.0\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.scale_grad_by_freq = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.sparse = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.embedding_dropout = 0.2\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder'> from params {'bidirectional': True, 'hidden_size': 50, 'input_size': 300, 'num_layers': 1, 'type': 'gru'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fccd4e752e8>}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.encoder.type = gru\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.encoder.batch_first = True\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.encoder.bidirectional = True\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.encoder.hidden_size = 50\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.encoder.input_size = 300\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.encoder.num_layers = 1\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.encoder.batch_first = True\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.max_decoding_steps = 10\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.beam_size = 10\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.target_names = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.target_namespace = target_tokens\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.target_embedding_dim = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'source_token_indexers': {'tokens': {'namespace': 'source_tokens', 'type': 'single_id'}}, 'source_tokenizer': {'type': 'word', 'word_splitter': {'type': 'spacy'}}, 'target_token_indexers': {'tokens': {'namespace': 'target_tokens'}}, 'target_tokenizer': {'type': 'word'}, 'type': 'event2mind'} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.type = event2mind\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.event2mind.Event2MindDatasetReader'> from params {'source_token_indexers': {'tokens': {'namespace': 'source_tokens', 'type': 'single_id'}}, 'source_tokenizer': {'type': 'word', 'word_splitter': {'type': 'spacy'}}, 'target_token_indexers': {'tokens': {'namespace': 'target_tokens'}}, 'target_tokenizer': {'type': 'word'}} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.tokenizer.Tokenizer'> from params {'type': 'word', 'word_splitter': {'type': 'spacy'}} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.type = word\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.word_tokenizer.WordTokenizer'> from params {'word_splitter': {'type': 'spacy'}} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.word_splitter.WordSplitter'> from params {'type': 'spacy'} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.word_splitter.type = spacy\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.word_splitter.SpacyWordSplitter'> from params {} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.word_splitter.language = en_core_web_sm\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.word_splitter.pos_tags = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.word_splitter.parse = False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.word_splitter.ner = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.start_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.end_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.tokenizer.Tokenizer'> from params {'type': 'word'} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_tokenizer.type = word\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.word_tokenizer.WordTokenizer'> from params {} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_tokenizer.start_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_tokenizer.end_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.token_indexers.token_indexer.TokenIndexer'> from params {'namespace': 'source_tokens', 'type': 'single_id'} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_token_indexers.tokens.type = single_id\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer'> from params {'namespace': 'source_tokens'} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_token_indexers.tokens.namespace = source_tokens\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_token_indexers.tokens.lowercase_tokens = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_token_indexers.tokens.start_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_token_indexers.tokens.end_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.token_indexers.token_indexer.TokenIndexer'> from params {'namespace': 'target_tokens'} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_token_indexers.tokens.type = single_id\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer'> from params {'namespace': 'target_tokens'} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_token_indexers.tokens.namespace = target_tokens\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_token_indexers.tokens.lowercase_tokens = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_token_indexers.tokens.start_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_token_indexers.tokens.end_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_add_start_token = True\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.dummy_instances_for_vocab_generation = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.lazy = False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'xintent_top_k_predictions': [[4, 3, 3, 3],\n",
       "  [684, 3, 3, 3],\n",
       "  [323, 3, 3, 3],\n",
       "  [282, 3, 3, 3],\n",
       "  [255, 3, 3, 3],\n",
       "  [153, 229, 3, 3],\n",
       "  [13, 175, 3, 3],\n",
       "  [44, 251, 3, 3],\n",
       "  [13, 267, 3, 3],\n",
       "  [211, 32, 80, 3]],\n",
       " 'xintent_top_k_log_probabilities': [-1.128340721130371,\n",
       "  -4.298313617706299,\n",
       "  -4.499514579772949,\n",
       "  -4.616389751434326,\n",
       "  -4.652942657470703,\n",
       "  -5.256786823272705,\n",
       "  -5.543419361114502,\n",
       "  -6.178347587585449,\n",
       "  -6.431229114532471,\n",
       "  -9.508707046508789],\n",
       " 'xreact_top_k_predictions': [[54, 3],\n",
       "  [70, 3],\n",
       "  [53, 3],\n",
       "  [109, 3],\n",
       "  [5, 3],\n",
       "  [73, 3],\n",
       "  [11, 3],\n",
       "  [92, 3],\n",
       "  [25, 3],\n",
       "  [63, 3]],\n",
       " 'xreact_top_k_log_probabilities': [-3.147449016571045,\n",
       "  -3.159241199493408,\n",
       "  -3.1768059730529785,\n",
       "  -3.2743079662323,\n",
       "  -3.3373990058898926,\n",
       "  -3.516559362411499,\n",
       "  -3.596619129180908,\n",
       "  -3.8112082481384277,\n",
       "  -3.8679120540618896,\n",
       "  -3.9374184608459473],\n",
       " 'oreact_top_k_predictions': [[4, 3],\n",
       "  [63, 3],\n",
       "  [36, 3],\n",
       "  [89, 3],\n",
       "  [83, 3],\n",
       "  [91, 3],\n",
       "  [138, 3],\n",
       "  [53, 3],\n",
       "  [92, 3],\n",
       "  [5, 3]],\n",
       " 'oreact_top_k_log_probabilities': [-1.067413330078125,\n",
       "  -2.62384033203125,\n",
       "  -3.9126994609832764,\n",
       "  -3.979201316833496,\n",
       "  -4.0716962814331055,\n",
       "  -4.296855926513672,\n",
       "  -4.327937602996826,\n",
       "  -4.378903388977051,\n",
       "  -4.453375339508057,\n",
       "  -4.506431579589844],\n",
       " 'xintent_top_k_predicted_tokens': [['none'],\n",
       "  ['annoying'],\n",
       "  ['noticed'],\n",
       "  ['communicate'],\n",
       "  ['heard'],\n",
       "  ['express', 'anger'],\n",
       "  ['get', 'attention'],\n",
       "  ['show', 'affection'],\n",
       "  ['get', 'revenge'],\n",
       "  ['let', 'someone', 'know']],\n",
       " 'xreact_top_k_predicted_tokens': [['upset'],\n",
       "  ['worried'],\n",
       "  ['nervous'],\n",
       "  ['curious'],\n",
       "  ['happy'],\n",
       "  ['scared'],\n",
       "  ['satisfied'],\n",
       "  ['anxious'],\n",
       "  ['relieved'],\n",
       "  ['annoyed']],\n",
       " 'oreact_top_k_predicted_tokens': [['none'],\n",
       "  ['annoyed'],\n",
       "  ['angry'],\n",
       "  ['informed'],\n",
       "  ['surprised'],\n",
       "  ['interested'],\n",
       "  ['frustrated'],\n",
       "  ['nervous'],\n",
       "  ['anxious'],\n",
       "  ['happy']]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from event2mind_hack import load_event2mind_archive\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "\n",
    "archive = load_event2mind_archive('data/event2mind.tar.gz')\n",
    "predictor = Predictor.from_archive(archive)\n",
    "predictor.predict(\n",
    "  source=\"PersonX drops a hint\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36787944117144233"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.exp(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>p_log</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[none]</td>\n",
       "      <td>-1.128341</td>\n",
       "      <td>0.323570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[annoying]</td>\n",
       "      <td>-4.298314</td>\n",
       "      <td>0.013591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[noticed]</td>\n",
       "      <td>-4.499515</td>\n",
       "      <td>0.011114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[communicate]</td>\n",
       "      <td>-4.616390</td>\n",
       "      <td>0.009888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[heard]</td>\n",
       "      <td>-4.652943</td>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[express, anger]</td>\n",
       "      <td>-5.256787</td>\n",
       "      <td>0.005212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[get, attention]</td>\n",
       "      <td>-5.543419</td>\n",
       "      <td>0.003913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[show, affection]</td>\n",
       "      <td>-6.178348</td>\n",
       "      <td>0.002074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[get, revenge]</td>\n",
       "      <td>-6.431229</td>\n",
       "      <td>0.001610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[let, someone, know]</td>\n",
       "      <td>-9.508707</td>\n",
       "      <td>0.000074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tokens     p_log         p\n",
       "0                [none] -1.128341  0.323570\n",
       "1            [annoying] -4.298314  0.013591\n",
       "2             [noticed] -4.499515  0.011114\n",
       "3         [communicate] -4.616390  0.009888\n",
       "4               [heard] -4.652943  0.009534\n",
       "5      [express, anger] -5.256787  0.005212\n",
       "6      [get, attention] -5.543419  0.003913\n",
       "7     [show, affection] -6.178348  0.002074\n",
       "8        [get, revenge] -6.431229  0.001610\n",
       "9  [let, someone, know] -9.508707  0.000074"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xintent = pd.DataFrame({\n",
    "    'tokens': prediction['xintent_top_k_predicted_tokens'],\n",
    "    'p_log': prediction['xintent_top_k_log_probabilities']\n",
    "})\n",
    "xintent['p'] = xintent['p_log'].apply(math.exp)\n",
    "xintent.sort_values(by='p', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>p_log</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[upset]</td>\n",
       "      <td>-3.147449</td>\n",
       "      <td>0.042962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[worried]</td>\n",
       "      <td>-3.159241</td>\n",
       "      <td>0.042458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[nervous]</td>\n",
       "      <td>-3.176806</td>\n",
       "      <td>0.041719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[curious]</td>\n",
       "      <td>-3.274308</td>\n",
       "      <td>0.037843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[happy]</td>\n",
       "      <td>-3.337399</td>\n",
       "      <td>0.035529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[scared]</td>\n",
       "      <td>-3.516559</td>\n",
       "      <td>0.029701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[satisfied]</td>\n",
       "      <td>-3.596619</td>\n",
       "      <td>0.027416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[anxious]</td>\n",
       "      <td>-3.811208</td>\n",
       "      <td>0.022121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[relieved]</td>\n",
       "      <td>-3.867912</td>\n",
       "      <td>0.020902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[annoyed]</td>\n",
       "      <td>-3.937418</td>\n",
       "      <td>0.019498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        tokens     p_log         p\n",
       "0      [upset] -3.147449  0.042962\n",
       "1    [worried] -3.159241  0.042458\n",
       "2    [nervous] -3.176806  0.041719\n",
       "3    [curious] -3.274308  0.037843\n",
       "4      [happy] -3.337399  0.035529\n",
       "5     [scared] -3.516559  0.029701\n",
       "6  [satisfied] -3.596619  0.027416\n",
       "7    [anxious] -3.811208  0.022121\n",
       "8   [relieved] -3.867912  0.020902\n",
       "9    [annoyed] -3.937418  0.019498"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xreact = pd.DataFrame({\n",
    "    'tokens': prediction['xreact_top_k_predicted_tokens'],\n",
    "    'p_log': prediction['xreact_top_k_log_probabilities']\n",
    "})\n",
    "xreact['p'] = xreact['p_log'].apply(math.exp)\n",
    "xreact.sort_values(by='p', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>p_log</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[none]</td>\n",
       "      <td>-1.067413</td>\n",
       "      <td>0.343897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[annoyed]</td>\n",
       "      <td>-2.623840</td>\n",
       "      <td>0.072524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[angry]</td>\n",
       "      <td>-3.912699</td>\n",
       "      <td>0.019986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[informed]</td>\n",
       "      <td>-3.979201</td>\n",
       "      <td>0.018701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[surprised]</td>\n",
       "      <td>-4.071696</td>\n",
       "      <td>0.017048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[interested]</td>\n",
       "      <td>-4.296856</td>\n",
       "      <td>0.013611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[frustrated]</td>\n",
       "      <td>-4.327938</td>\n",
       "      <td>0.013195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[nervous]</td>\n",
       "      <td>-4.378903</td>\n",
       "      <td>0.012539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[anxious]</td>\n",
       "      <td>-4.453375</td>\n",
       "      <td>0.011639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[happy]</td>\n",
       "      <td>-4.506432</td>\n",
       "      <td>0.011038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tokens     p_log         p\n",
       "0        [none] -1.067413  0.343897\n",
       "1     [annoyed] -2.623840  0.072524\n",
       "2       [angry] -3.912699  0.019986\n",
       "3    [informed] -3.979201  0.018701\n",
       "4   [surprised] -4.071696  0.017048\n",
       "5  [interested] -4.296856  0.013611\n",
       "6  [frustrated] -4.327938  0.013195\n",
       "7     [nervous] -4.378903  0.012539\n",
       "8     [anxious] -4.453375  0.011639\n",
       "9       [happy] -4.506432  0.011038"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oreact = pd.DataFrame({\n",
    "    'tokens': prediction['oreact_top_k_predicted_tokens'],\n",
    "    'p_log': prediction['oreact_top_k_log_probabilities']\n",
    "})\n",
    "oreact['p'] = oreact['p_log'].apply(math.exp)\n",
    "oreact.sort_values(by='p', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
