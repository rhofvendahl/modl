{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_coref_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'November was a trying month… on the 7th Dante had a major accident. 5 minutes before school and he and some friends are climbing the fence, I tell him it’s not a good idea and to get down. I turn back to talk to Jodi (on of my best mom friend’s at the school) and Dante comes to me screaming with his hand full of blood. I run him into my classroom and get him to the sink, as I turn on the water to clean the area the flap of his thumb lifts away and I see the bone. Shit. This isn’t something I can fix here, I grab my first aid kit and wrap it like crazy because it’s bleeding like crazy. I phone James and tell him to get to the ER as Dante is screaming and freaking out in the background as I’m trying to usher him back to the car as he’s bleeding like a stuffed pig. Unfortunately in the ER I learned that my child doesn’t take to freezing, an hour of gel freezing and he still felt the 2 needles as they went in, 15 minutes later and he felt the last 2 stitches of 8. He needed more because his finger still had gaps, the doctor didn’t want to cause him anymore pain so he glued them. It was an intense and deep gash that spiraled all the way up his thumb. I was trying to stay strong for him but I did break down as he screamed and cried, I was left to emotionally drained that day. James was able to take the remainder of the day off and stay with him. He missed 2 more days of school and then had an extra long weekend due to the holiday and the pro day but for 2 weeks he couldn’t write (of course it was his right hand.) 3 doctor visits later and he finally got them out full last week, the first visit the doctor wanted them in longer because of the severity. 2nd time he could only get 6 out because the glue had gotten on the last 2 stitches and he didn’t want to have to dig them out so we had to soak and dissolve the glue for 3 days. 3rd time the last 2 came out.  Even now he’s slowly regaining his writing skills as there was some nerve damage.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'So I have had a good day today. I found out we got the other half of our funding for my travel grant, which paid for my friend to come with me. So that’s good, she and I will both get some money back. I took my dogs to the pet store so my girl dog could get a new collar, but she wanted to beat everyone up. This is an ongoing issue with her. She’s so little and cute too but damn she acts like she’s gonna go for the jugular with everyone she doesn’t know! She did end up with a cute new collar tho, it has pineapples on it. I went to the dentist and she’s happy with my Invisalign progress. We have three more trays and then she does an impression to make sure my teeth are where they need to be before they get the rest of the trays. YAY! And I don’t have to make another payment until closer to the end of my treatment. I had some work emails with the festival, and Jessie was bringing up some important points, and one of our potential artists was too expensive to work with, so Mutual Friend was asking for names for some other people we could work with. So I suggested like, three artists, and Jessie actually liked the idea of one of them doing it. Which is nice. I notice she is very encouraging at whatever I contribute to our collective. It’s sweet. I kind of know this is like, the only link we have with each other right now besides social media, so it seems like she’s trying to make sure I know she still wants me to be involved and doesn’t have bad feelings for me. And there was a short period when I was seriously thinking of leaving the collective and not working with this festival anymore. I was so sad, and felt so upset, and didn’t know what to do about Jessie. It felt really close to me throwing in the towel. But I hung on through the festival and it doesn’t seem so bad from this viewpoint now with more time that has passed. And we have been gentle, if reserved, with each other. I mean her last personal email to me however many weeks ago wasn’t very nice. But it seems like we’ve been able to put it aside for work reasons. I dunno. I still feel like if anything was gonna get mended between us, she would need to make the first moves on that. I really don’t want to try reaching out and get rejected even as a friend again. I miss her though. And sometimes I think she misses me. But I don’t want to approach her assuming we both miss each other and have her turn it on me again and make out like all these things are all in my head. I don’t know about that butch I went on a date with last night. I feel more of a friend vibe from her, than a romantic one. I can’t help it, I am just not attracted to butches. And I don’t know how to flirt with them. And I don’t think of them in a sexy way. But I WOULD like another butch buddy. I mean yeah maybe Femmes do play games, or maybe I just chased all the wrong Femmes. Maybe I’ll just leave this and not think about it much until I get back to town in January.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'well, i tried to get an x-ray of my neck today but, when i got to the medical center and stood in line to wait to get checked in, i was told my doctor hadn’t sent over the orders for it! and the thing is he said he had already sent it when i asked him if i needed anything to get it done. i don’t like being lied to. so, since i have to go to the medical center thursday morning for a consultation for p/t, i’ll just go on back over and get the xray. the lady there said monday and tuesday are really busy days and thursday would be much better.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I can’t help but feel annoyed, angry, disheartened, let down… and yet in another way I want to say “you don’t deserve to know him.” Dante’s growing into such an amazing child and yet it seems our family dwindles like crazy, he brings up James’ sister “aunty Tammy” and asks why we never see her. I say she’s busy because she has 2 of her own little boys, but that’s not the case. James’ sister had this dream of being an amazing aunt to  Dante and she has done nothing to be in his life. Birthday gifts, Christmas, there’s no communication or her ever asking about him, not that I even speak to  her much but she just doesn’t care to be an active part in his world which pisses me off to no end. James’ mother couldn’t get herself clean to stay in his life… she’s non existent to him. It boils my blood because when he was born she was so proud, got clean for a while, and then couldn’t hack it (ended up visiting and left her morphine out where our very smart 2 year old brought us a handful of pills and asked if they  were candy.) That was the last time she saw him and her memory has since been forgotten. You couldn’t even get clean to be in your grandchild’s life? She was always a pathetic excuse for a mother, James’ childhood simply enrages me, the idea of a child living the way he did because of her ways makes me sick. She doesn’t deserve to know my child. My own brother “uncle Jason” is seen in passing about 5 times a year, he’s good with Dante, pleasant enough considering my brother has so many anger issues. He’s also a drug addict so it’s not like I would ever allow him time alone with Dante, not that he’d ever want to spend time with him. Christmas is coming up and in a way it’s bittersweet. My half cousin Tianna’s two girls (Stella and Piper)  have two sets of everything, tons of aunts and uncles, and they have a huge loving family unit. Dante doesn’t have that, yes his grandparents love him like crazy but his family connection is like mine. When I was young I only had one set of grandparents, my dad was adopted and his mother wasn’t around at all… in a way my grandparents adopted him as well when he married my young at the age of 18. I had my uncle George and my Omi and Opi and my mom and dad and my brother. I remember when George met Denise and I met Tianna (Denise’s child from her first marriage.) I  remember the day that I learned that George had proposed to Denise and that they were getting married, I cried and my mom thought I was happy. I wasn’t happy, I was devastated that suddenly I had to share my family (horrible to thing to cry about right?) Tianna already had 2 sets of grandparents, she had tons of aunts, and now she was getting my uncle whom I loved and thought was the coolest guy around as a dad. I was so angry. I never really got to  meet Tianna’s dad’s side of the family, I met her grandparents a few times but they never remembered my name which really hurt and annoyed me. I joined soccer and Tianna’s dad was the coach, he wasn’t nice to me which drove the wedge deeper. It hurts… it hurts that my child has the same issue that I did although I really don’t think he’s realized that he’s different. His “grandpa Morgan” isn’t his real grandpa, more like a man who took on his father and tried to “raise” him to be a man, he obviously didn’t stay with James’ mom but he’s still in our life and I’m grateful  that he’s there. Unfortunately he’s not around much, we see him 2-3 times a year because he lives in Golden. Uncle Adam is James’ childhood best friend, a good guy  who’s more of a businessman who  lives to the beat of his own drum and wouldn’t know what to do with  a child if his  life depended on it. He’s around but again it’s only a few times a year when he visits from Calgary. I want to say sometimes you get to choose your own family, but even the family I chose for him and thought would be around forever, the people who were there when he was born, grew, shared so many moments with are no longer around. They don’t seem to care either, it’s not like “aunty Kat” ever talks to me or asks about him. Seems like moving meant the end of our friendship and our “family ties.”'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Sheila was run over by a truck. She herself didn't see that coming. I told her she should take care of herself, but I know she'll just go and do her thing regardless of what I say. What a conundrum! This makes me wish I had never signed up to be friends with her, although I do love the girl.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = textacy.preprocess.normalize_whitespace(text)\n",
    "preprocessed = textacy.preprocess.preprocess_text(preprocessed, fix_unicode=True, no_contractions=True, no_accents=True)\n",
    "doc = nlp(preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Extract People"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheila PERSON\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sheila'}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people = set([ent.text for ent in doc.ents if ent.label_ == 'PERSON'])\n",
    "people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "textacy.text_utils.keyword_in_context(doc.text, 'Christmas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheila nsubjpass run\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    if ent.label_ == 'PERSON':\n",
    "        token = ent.root\n",
    "        print(ent.text, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coreference Resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AllenNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/07/2018 20:40:59 - INFO - allennlp.models.archival -   loading archive file data/coref-model.tar.gz\n",
      "12/07/2018 20:40:59 - INFO - allennlp.models.archival -   extracting archive file data/coref-model.tar.gz to temp dir /tmp/tmpcss5kksx\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   type = default\n",
      "12/07/2018 20:41:00 - INFO - allennlp.data.vocabulary -   Loading token dictionary from /tmp/tmpcss5kksx/vocabulary.\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.model.Model'> from params {'antecedent_feedforward': {'activations': 'relu', 'dropout': 0.2, 'hidden_dims': 150, 'input_dim': 3680, 'num_layers': 2}, 'context_layer': {'bidirectional': True, 'dropout': 0.2, 'hidden_size': 200, 'input_size': 400, 'num_layers': 1, 'type': 'lstm'}, 'feature_size': 20, 'initializer': [['.*linear_layers.*weight', {'type': 'xavier_normal'}], ['.*scorer._module.*weight', {'type': 'xavier_normal'}], ['.*_global_attention._module.weight', {'type': 'xavier_normal'}], ['_distance_embedding.weight', {'type': 'xavier_normal'}], ['_span_width_embedding.weight', {'type': 'xavier_normal'}], ['_context_layer._module.weight_ih.*', {'type': 'xavier_normal'}], ['_context_layer._module.weight_hh.*', {'type': 'orthogonal'}]], 'lexical_dropout': 0.5, 'max_antecedents': 150, 'max_span_width': 10, 'mention_feedforward': {'activations': 'relu', 'dropout': 0.2, 'hidden_dims': 150, 'input_dim': 1220, 'num_layers': 2}, 'spans_per_word': 0.4, 'text_field_embedder': {'token_characters': {'embedding': {'embedding_dim': 16, 'num_embeddings': 262}, 'encoder': {'embedding_dim': 16, 'ngram_filter_sizes': [5], 'num_filters': 100, 'type': 'cnn'}, 'type': 'character_encoding'}, 'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'}}, 'type': 'coref'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7f57341d7390>}\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.type = coref\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.coreference_resolution.coref.CoreferenceResolver'> from params {'antecedent_feedforward': {'activations': 'relu', 'dropout': 0.2, 'hidden_dims': 150, 'input_dim': 3680, 'num_layers': 2}, 'context_layer': {'bidirectional': True, 'dropout': 0.2, 'hidden_size': 200, 'input_size': 400, 'num_layers': 1, 'type': 'lstm'}, 'feature_size': 20, 'initializer': [['.*linear_layers.*weight', {'type': 'xavier_normal'}], ['.*scorer._module.*weight', {'type': 'xavier_normal'}], ['.*_global_attention._module.weight', {'type': 'xavier_normal'}], ['_distance_embedding.weight', {'type': 'xavier_normal'}], ['_span_width_embedding.weight', {'type': 'xavier_normal'}], ['_context_layer._module.weight_ih.*', {'type': 'xavier_normal'}], ['_context_layer._module.weight_hh.*', {'type': 'orthogonal'}]], 'lexical_dropout': 0.5, 'max_antecedents': 150, 'max_span_width': 10, 'mention_feedforward': {'activations': 'relu', 'dropout': 0.2, 'hidden_dims': 150, 'input_dim': 1220, 'num_layers': 2}, 'spans_per_word': 0.4, 'text_field_embedder': {'token_characters': {'embedding': {'embedding_dim': 16, 'num_embeddings': 262}, 'encoder': {'embedding_dim': 16, 'ngram_filter_sizes': [5], 'num_filters': 100, 'type': 'cnn'}, 'type': 'character_encoding'}, 'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'}}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7f57341d7390>}\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'token_characters': {'embedding': {'embedding_dim': 16, 'num_embeddings': 262}, 'encoder': {'embedding_dim': 16, 'ngram_filter_sizes': [5], 'num_filters': 100, 'type': 'cnn'}, 'type': 'character_encoding'}, 'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7f57341d7390>}\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.text_field_embedder.type = basic\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.text_field_embedder.embedder_to_indexer_map = None\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.text_field_embedder.allow_unmatched_keys = False\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders = None\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding': {'embedding_dim': 16, 'num_embeddings': 262}, 'encoder': {'embedding_dim': 16, 'ngram_filter_sizes': [5], 'num_filters': 100, 'type': 'cnn'}, 'type': 'character_encoding'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7f57341d7390>}\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.type = character_encoding\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.num_embeddings = 262\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.vocab_namespace = token_characters\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.embedding_dim = 16\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.pretrained_file = None\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.projection_dim = None\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.trainable = True\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.padding_index = None\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.max_norm = None\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.norm_type = 2.0\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.scale_grad_by_freq = False\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.sparse = False\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder'> from params {'embedding_dim': 16, 'ngram_filter_sizes': [5], 'num_filters': 100, 'type': 'cnn'} and extras {}\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.type = cnn\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.seq2vec_encoders.cnn_encoder.CnnEncoder'> from params {'embedding_dim': 16, 'ngram_filter_sizes': [5], 'num_filters': 100} and extras {}\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.embedding_dim = 16\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.num_filters = 100\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.ngram_filter_sizes = [5]\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.output_dim = None\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.dropout = 0.0\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7f57341d7390>}\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.type = embedding\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.num_embeddings = None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.vocab_namespace = tokens\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.embedding_dim = 300\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.pretrained_file = None\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.projection_dim = None\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.trainable = False\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.padding_index = None\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.max_norm = None\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.norm_type = 2.0\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.scale_grad_by_freq = False\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.sparse = False\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'bidirectional': True, 'dropout': 0.2, 'hidden_size': 200, 'input_size': 400, 'num_layers': 1, 'type': 'lstm'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7f57341d7390>}\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.context_layer.type = lstm\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.context_layer.batch_first = True\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.context_layer.stateful = False\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.context_layer.bidirectional = True\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.context_layer.dropout = 0.2\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.context_layer.hidden_size = 200\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.context_layer.input_size = 400\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.context_layer.num_layers = 1\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.context_layer.batch_first = True\n",
      "/home/russell/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.mention_feedforward.input_dim = 1220\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.mention_feedforward.num_layers = 2\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.mention_feedforward.hidden_dims = 150\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.mention_feedforward.activations = relu\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.mention_feedforward.dropout = 0.2\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.antecedent_feedforward.input_dim = 3680\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.antecedent_feedforward.num_layers = 2\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.antecedent_feedforward.hidden_dims = 150\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.antecedent_feedforward.activations = relu\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.antecedent_feedforward.dropout = 0.2\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.feature_size = 20\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.max_span_width = 10\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.spans_per_word = 0.4\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.max_antecedents = 150\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.lexical_dropout = 0.5\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.initializer = [['.*linear_layers.*weight', {'type': 'xavier_normal'}], ['.*scorer._module.*weight', {'type': 'xavier_normal'}], ['.*_global_attention._module.weight', {'type': 'xavier_normal'}], ['_distance_embedding.weight', {'type': 'xavier_normal'}], ['_span_width_embedding.weight', {'type': 'xavier_normal'}], ['_context_layer._module.weight_ih.*', {'type': 'xavier_normal'}], ['_context_layer._module.weight_hh.*', {'type': 'orthogonal'}]]\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.initializer.list.list.type = xavier_normal\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.initializer.list.list.type = xavier_normal\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.initializer.list.list.type = xavier_normal\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.initializer.list.list.type = xavier_normal\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.initializer.list.list.type = xavier_normal\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.initializer.list.list.type = xavier_normal\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   model.initializer.list.list.type = orthogonal\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/07/2018 20:41:00 - INFO - allennlp.nn.initializers -   Initializing parameters\n",
      "12/07/2018 20:41:00 - INFO - allennlp.nn.initializers -   Initializing _context_layer._module.weight_ih_l0 using _context_layer._module.weight_ih.* intitializer\n",
      "12/07/2018 20:41:00 - INFO - allennlp.nn.initializers -   Initializing _context_layer._module.weight_hh_l0 using _context_layer._module.weight_hh.* intitializer\n",
      "12/07/2018 20:41:00 - INFO - allennlp.nn.initializers -   Initializing _context_layer._module.weight_ih_l0_reverse using _context_layer._module.weight_ih.* intitializer\n",
      "12/07/2018 20:41:00 - INFO - allennlp.nn.initializers -   Initializing _context_layer._module.weight_hh_l0_reverse using _context_layer._module.weight_hh.* intitializer\n",
      "12/07/2018 20:41:00 - INFO - allennlp.nn.initializers -   Initializing _antecedent_feedforward._module._linear_layers.0.weight using .*linear_layers.*weight intitializer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/07/2018 20:41:00 - INFO - allennlp.nn.initializers -   Initializing _antecedent_feedforward._module._linear_layers.1.weight using .*linear_layers.*weight intitializer\n",
      "12/07/2018 20:41:00 - INFO - allennlp.nn.initializers -   Initializing _mention_pruner._scorer.0._module._linear_layers.0.weight using .*linear_layers.*weight intitializer\n",
      "12/07/2018 20:41:00 - INFO - allennlp.nn.initializers -   Initializing _mention_pruner._scorer.0._module._linear_layers.1.weight using .*linear_layers.*weight intitializer\n",
      "12/07/2018 20:41:00 - INFO - allennlp.nn.initializers -   Initializing _antecedent_scorer._module.weight using .*scorer._module.*weight intitializer\n",
      "12/07/2018 20:41:00 - INFO - allennlp.nn.initializers -   Initializing _endpoint_span_extractor._span_width_embedding.weight using _span_width_embedding.weight intitializer\n",
      "12/07/2018 20:41:00 - INFO - allennlp.nn.initializers -   Initializing _attentive_span_extractor._global_attention._module.weight using .*_global_attention._module.weight intitializer\n",
      "12/07/2018 20:41:00 - INFO - allennlp.nn.initializers -   Initializing _distance_embedding.weight using _distance_embedding.weight intitializer\n",
      "12/07/2018 20:41:00 - INFO - allennlp.nn.initializers -   Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "12/07/2018 20:41:00 - INFO - allennlp.nn.initializers -      _antecedent_feedforward._module._linear_layers.0.bias\n",
      "12/07/2018 20:41:00 - INFO - allennlp.nn.initializers -      _antecedent_feedforward._module._linear_layers.1.bias\n",
      "12/07/2018 20:41:00 - INFO - allennlp.nn.initializers -      _antecedent_scorer._module.bias\n",
      "12/07/2018 20:41:00 - INFO - allennlp.nn.initializers -      _attentive_span_extractor._global_attention._module.bias\n",
      "12/07/2018 20:41:00 - INFO - allennlp.nn.initializers -      _context_layer._module.bias_hh_l0\n",
      "12/07/2018 20:41:00 - INFO - allennlp.nn.initializers -      _context_layer._module.bias_hh_l0_reverse\n",
      "12/07/2018 20:41:00 - INFO - allennlp.nn.initializers -      _context_layer._module.bias_ih_l0\n",
      "12/07/2018 20:41:00 - INFO - allennlp.nn.initializers -      _context_layer._module.bias_ih_l0_reverse\n",
      "12/07/2018 20:41:00 - INFO - allennlp.nn.initializers -      _mention_pruner._scorer.0._module._linear_layers.0.bias\n",
      "12/07/2018 20:41:00 - INFO - allennlp.nn.initializers -      _mention_pruner._scorer.0._module._linear_layers.1.bias\n",
      "12/07/2018 20:41:00 - INFO - allennlp.nn.initializers -      _mention_pruner._scorer.1._module.bias\n",
      "12/07/2018 20:41:00 - INFO - allennlp.nn.initializers -      _mention_pruner._scorer.1._module.weight\n",
      "12/07/2018 20:41:00 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_token_characters._embedding._module.weight\n",
      "12/07/2018 20:41:00 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.bias\n",
      "12/07/2018 20:41:00 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.weight\n",
      "12/07/2018 20:41:00 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_tokens.weight\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'max_span_width': 10, 'token_indexers': {'token_characters': {'type': 'characters'}, 'tokens': {'lowercase_tokens': False, 'type': 'single_id'}}, 'type': 'coref'} and extras {}\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   dataset_reader.type = coref\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.coreference_resolution.conll.ConllCorefReader'> from params {'max_span_width': 10, 'token_indexers': {'token_characters': {'type': 'characters'}, 'tokens': {'lowercase_tokens': False, 'type': 'single_id'}}} and extras {}\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   dataset_reader.max_span_width = 10\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.token_indexers.token_indexer.TokenIndexer'> from params {'type': 'characters'} and extras {}\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   dataset_reader.token_indexers.token_characters.type = characters\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.token_indexers.token_characters_indexer.TokenCharactersIndexer'> from params {} and extras {}\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   dataset_reader.token_indexers.token_characters.namespace = token_characters\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   dataset_reader.token_indexers.token_characters.start_tokens = None\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   dataset_reader.token_indexers.token_characters.end_tokens = None\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   dataset_reader.token_indexers.token_characters.min_padding_length = 0\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.token_indexers.token_indexer.TokenIndexer'> from params {'lowercase_tokens': False, 'type': 'single_id'} and extras {}\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.type = single_id\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer'> from params {'lowercase_tokens': False} and extras {}\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.namespace = tokens\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.lowercase_tokens = False\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.start_tokens = None\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.end_tokens = None\n",
      "12/07/2018 20:41:00 - INFO - allennlp.common.params -   dataset_reader.lazy = False\n"
     ]
    }
   ],
   "source": [
    "from allennlp.models.archival import load_archive\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "\n",
    "archive = load_archive('data/coref-model.tar.gz')\n",
    "predictor = Predictor.from_archive(archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "coref = predictor.predict(document = doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sheila, She herself, herself, her, she, herself, she, her, her, the girl]\n",
      "[I, I, I, me, I, I]\n"
     ]
    }
   ],
   "source": [
    "for cluster in coref['clusters']:\n",
    "    spans = [doc[first:last+1] for first, last in cluster]\n",
    "    print(spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Sheila(0)] was run over by a truck . [She [herself(0)] did not see that coming . [I(1)] told [her(0)] [she(0)] should take care of [herself(0)] , but [I(1)] know [she(0)] will just go and do [her(0)] thing regardless of what [I(1)] say . What a conundrum ! This makes [me(1)] wish [I(1)] had never signed up to be friends with [her(0)] , although [I(1)] do love [the girl(0)] .'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def coref_resolved(doc, coref):\n",
    "    resolved = [token.text for token in doc]\n",
    "    for token in doc:\n",
    "        cluster_n = 0\n",
    "        for cluster in coref['clusters']:\n",
    "            for first, last in cluster:\n",
    "                span = doc[first:last+1]\n",
    "                if first == last:\n",
    "                    resolved[first] = '[' + doc[first].text + '(' + str(cluster_n) + ')]'\n",
    "                else:\n",
    "                    resolved[first] = '[' + doc[first].text\n",
    "                    resolved[last] = doc[last].text + '(' + str(cluster_n) + ')]'\n",
    "\n",
    "            cluster_n += 1\n",
    "    return ' '.join(resolved)\n",
    "coref_resolved(doc, coref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NeuralCoref\n",
    "(maybe not quite as good? maybe it is. certainly easier.)\n",
    "\n",
    "(I think I'll use this together with named entity recognititon to ID unique people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc._.has_coref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sheila: [Sheila, She, She herself, her, she, herself, she, her, her, the girl]]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc._.coref_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sheila was run over by a truck. She herself did not see that coming. I told her she should take care of herself, but I know she will just go and do her thing regardless of what I say. What a conundrum! This makes me wish I had never signed up to be friends with her, although I do love the girl.'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sheila was run over by a truck. Sheila did not see that coming. I told Sheila Sheila should take care of Sheila, but I know Sheila will just go and do Sheila thing regardless of what I say. What a conundrum! This makes me wish I had never signed up to be friends with Sheila, although I do love Sheila.'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc._.coref_resolved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use NEM & Coref to ID unique people\n",
    "NEM can tell which clusters are people\n",
    "NEM can give clusters better names\n",
    "NEM can link clusters together\n",
    "NEM can tell whether a cluster contains a name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sheila: [Sheila, She, She herself, her, she, herself, she, her, her, the girl]]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc._.coref_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sheila was run over by a truck. Sheila did not see that coming. I told Sheila Sheila should take care of Sheila, but I know Sheila will just go and do Sheila thing regardless of what I say. What a conundrum! This makes me wish I had never signed up to be friends with Sheila, although I do love Sheila.'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc._.coref_resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sheila'}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people = set([ent.text for ent in doc.ents if ent.label_ == 'PERSON'])\n",
    "people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sheila]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ent for ent in doc.ents if ent.label_ == 'PERSON']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now assuming all names are unique identifiers\n",
    "class Person:\n",
    "    def __init__(self, name, pronouns=None, mentions=[], user=False):\n",
    "        self.name = name\n",
    "#         self.gender = gender\n",
    "        self.mentions = mentions\n",
    "        self.user = user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPGRADE AT SOME POINT TO EXTRACT GENDER, ACCOUNT FOR CLUSTERS WITHOUT NAMES\n",
    "# UPGRADE TO INCLUDE I, USER\n",
    "\n",
    "# assumes names are unique identifiers\n",
    "# assumes misspellings are diff people\n",
    "class Page:\n",
    "    people = []\n",
    "    \n",
    "    def get_person_by_name(self, name):\n",
    "        for person in self.people:\n",
    "            if person.name == name:\n",
    "                return person\n",
    "        return None\n",
    "    \n",
    "    def update(self, doc):\n",
    "        name_mentions = [ent for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "        names = set([name_mention.text for name_mention in name_mentions])\n",
    "        \n",
    "        # for clusters that includ ename mentions\n",
    "        for cluster in doc._.coref_clusters:\n",
    "            name = None\n",
    "            \n",
    "            for mention in cluster.mentions:\n",
    "                keyword = mention.root.text\n",
    "                if keyword in names:\n",
    "                    name = keyword\n",
    "                    \n",
    "            print(name, cluster.mentions)\n",
    "            if name != None:\n",
    "                person = self.get_person_by_name(name)\n",
    "                if person == None:\n",
    "                    person = Person(name, mentions=cluster.mentions)\n",
    "                    self.people += [person]\n",
    "                else:\n",
    "                    person.mentions += cluster.mentions\n",
    "            \n",
    "            # for named entities without clusters (single mentions)\n",
    "            for name_mention in name_mentions:\n",
    "                person = self.get_person_by_name(name_mention.text)\n",
    "                if person == None:\n",
    "                    person = Person(name_mention.text, mentions=[name_mention])\n",
    "                    self.people += [person]\n",
    "        \n",
    "    def resolve_people(self, doc):\n",
    "        tokens = [token.text for token in doc]\n",
    "\n",
    "        for person in self.people:\n",
    "            for mention in person.mentions:\n",
    "                \n",
    "                # determine resolved value\n",
    "                resolved = person.name\n",
    "                if mention.root.pos == 'ADJ':\n",
    "                    resolved += '\\'s'\n",
    "                                \n",
    "                # set first token to resolved value\n",
    "                tokens[mention.start] = resolved\n",
    "                \n",
    "                # set extra tokens in mention to blank\n",
    "                for i in range(mention.start+1, mention.end):\n",
    "                    tokens[i] = ''\n",
    "                \n",
    "        return ' '.join([token for token in tokens if token != ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheila [Sheila, She, She herself, her, she, herself, she, her, her, the girl]\n",
      "\n",
      "Sheila [Sheila, She, She herself, her, she, herself, she, her, her, the girl]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sheila was run over by a truck . Sheila did not see that coming . I told Sheila Sheila should take care of Sheila , but I know Sheila will just go and do Sheila thing regardless of what I say . What a conundrum ! This makes me wish I had never signed up to be friends with Sheila , although I do love Sheila .'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = Page()\n",
    "page.update(doc)\n",
    "print()\n",
    "for person in page.people:\n",
    "    print(person.name, person.mentions)\n",
    "page.resolve_people(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 25 herself\n"
     ]
    }
   ],
   "source": [
    "herself = page.get_person_by_name('Sheila').mentions[5]\n",
    "print(herself.start, herself.end, herself.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sheila was run over by a truck . Sheila did not see that coming . I told Sheila Sheila should take care of Sheila , but I know Sheila will just go and do Sheila thing regardless of what I say . What a conundrum ! This makes me wish I had never signed up to be friends with Sheila , although I do love Sheila .'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.resolve_people(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NeuralCoref Scores to Improve Coref Res\n",
    "https://modelzoo.co/model/neuralcoref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Subject Verb Object Triples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I - want - to say\n",
      "you - don’t deserve - to know\n",
      "he - brings - James’ sister\n",
      "he - brings - ”\n",
      "we - never see - her\n",
      "she - has - 2\n",
      "that - ’s not - case\n",
      "sister - had - dream\n",
      "she - has done - nothing\n",
      "there - ’s - communication\n",
      "there - ’s - asking\n",
      "I - speak - much\n",
      "she - doesn’t care - to be\n",
      "which - pisses - me\n",
      "It - boils - blood\n",
      "old - brought - us\n",
      "old - brought - handful\n",
      "they - were - candy\n",
      "That - was - time\n",
      "she - saw - him\n",
      "You - get - to be\n",
      "She - was - excuse\n",
      "childhood - enrages - me\n",
      "She - doesn’t deserve - to know\n",
      "brother - has - anger issues\n",
      "He - ’s - drug addict\n",
      "he - want - to spend\n",
      "they - have - family unit\n",
      "Dante - doesn’t have - that\n",
      "grandparents - love - him\n",
      "I - had - set\n",
      "grandparents - adopted - him\n",
      "he - married - young\n",
      "I - had - uncle\n",
      "George - met - Denise\n",
      "I - met - Tianna\n",
      "I - remember - day\n",
      "I - had - to share\n",
      "Tianna - had - sets\n",
      "she - had - tons\n",
      "uncle - was - guy\n",
      "I - got - meet\n",
      "I - met - grandparents\n",
      "they - never remembered - name\n",
      "I - joined - soccer\n",
      "I - joined - dad\n",
      "which - drove - wedge\n",
      "child - has - issue\n",
      "” - isn’t - grandpa\n",
      "who - took - father\n",
      "him - to be - man\n",
      "we - see - him\n",
      "Uncle Adam - is - friend\n",
      "I - want - to say\n",
      "you - get - to choose\n",
      "They - don’t seem - to care\n"
     ]
    }
   ],
   "source": [
    "svo_triples = textacy.extract.subject_verb_object_triples(doc)\n",
    "\n",
    "for subj, verb, obj in svo_triples:\n",
    "    print(subj, '-', verb, '-', obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I - want - to say\n",
      "you - don’t deserve - to know\n",
      "he - brings - James’ sister\n",
      "he - brings - ”\n",
      "we - never see - her\n",
      "she - has - 2\n",
      "that - ’s not - case\n",
      "sister - had - dream\n",
      "she - has done - nothing\n",
      "there - ’s - communication\n",
      "there - ’s - asking\n",
      "I - speak - much\n",
      "she - doesn’t care - to be\n",
      "which - pisses - me\n",
      "It - boils - blood\n",
      "old - brought - us\n",
      "old - brought - handful\n",
      "they - were - candy\n",
      "That - was - time\n",
      "she - saw - him\n",
      "You - get - to be\n",
      "She - was - excuse\n",
      "childhood - enrages - me\n",
      "She - doesn’t deserve - to know\n",
      "brother - has - anger issues\n",
      "He - ’s - drug addict\n",
      "he - want - to spend\n",
      "they - have - family unit\n",
      "Dante - doesn’t have - that\n",
      "grandparents - love - him\n",
      "I - had - set\n",
      "grandparents - adopted - him\n",
      "he - married - young\n",
      "I - had - uncle\n",
      "George - met - Denise\n",
      "I - met - Tianna\n",
      "I - remember - day\n",
      "I - had - to share\n",
      "Tianna - had - sets\n",
      "she - had - tons\n",
      "uncle - was - guy\n",
      "I - got - meet\n",
      "I - met - grandparents\n",
      "they - never remembered - name\n",
      "I - joined - soccer\n",
      "I - joined - dad\n",
      "which - drove - wedge\n",
      "child - has - issue\n",
      "” - isn’t - grandpa\n",
      "who - took - father\n",
      "him - to be - man\n",
      "we - see - him\n",
      "Uncle Adam - is - friend\n",
      "I - want - to say\n",
      "you - get - to choose\n",
      "They - don’t seem - to care\n"
     ]
    }
   ],
   "source": [
    "svo_triples = textacy.extract.subject_verb_object_triples(doc)\n",
    "\n",
    "for subj, verb, obj in svo_triples:\n",
    "    subj_phrase = ' '.join([token.text for token in subj.root.subtree])\n",
    "    obj_phrase = ' '.join([token.text for token in obj.root.subtree])\n",
    "#     start, end = textacy.spacier.utils.get_span_for_verb_auxiliaries(verb.root)\n",
    "#     verb_phrase = doc[start:end+1]\n",
    "    print(subj, '-', verb, '-', obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Semistructured Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('Uncle Tim was an old person')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'So I have had a good day today. I found out we got the other half of our funding for my travel grant, which paid for my friend to come with me. So that’s good, she and I will both get some money back. I took my dogs to the pet store so my girl dog could get a new collar, but she wanted to beat everyone up. This is an ongoing issue with her. She’s so little and cute too but damn she acts like she’s gonna go for the jugular with everyone she doesn’t know! She did end up with a cute new collar tho, it has pineapples on it. I went to the dentist and she’s happy with my Invisalign progress. We have three more trays and then she does an impression to make sure my teeth are where they need to be before they get the rest of the trays. YAY! And I don’t have to make another payment until closer to the end of my treatment. I had some work emails with the festival, and Jessie was bringing up some important points, and one of our potential artists was too expensive to work with, so Mutual Friend was asking for names for some other people we could work with. So I suggested like, three artists, and Jessie actually liked the idea of one of them doing it. Which is nice. I notice she is very encouraging at whatever I contribute to our collective. It’s sweet. I kind of know this is like, the only link we have with each other right now besides social media, so it seems like she’s trying to make sure I know she still wants me to be involved and doesn’t have bad feelings for me. And there was a short period when I was seriously thinking of leaving the collective and not working with this festival anymore. I was so sad, and felt so upset, and didn’t know what to do about Jessie. It felt really close to me throwing in the towel. But I hung on through the festival and it doesn’t seem so bad from this viewpoint now with more time that has passed. And we have been gentle, if reserved, with each other. I mean her last personal email to me however many weeks ago wasn’t very nice. But it seems like we’ve been able to put it aside for work reasons. I dunno. I still feel like if anything was gonna get mended between us, she would need to make the first moves on that. I really don’t want to try reaching out and get rejected even as a friend again. I miss her though. And sometimes I think she misses me. But I don’t want to approach her assuming we both miss each other and have her turn it on me again and make out like all these things are all in my head. I don’t know about that butch I went on a date with last night. I feel more of a friend vibe from her, than a romantic one. I can’t help it, I am just not attracted to butches. And I don’t know how to flirt with them. And I don’t think of them in a sexy way. But I WOULD like another butch buddy. I mean yeah maybe Femmes do play games, or maybe I just chased all the wrong Femmes. Maybe I’ll just leave this and not think about it much until I get back to town in January.'\n",
    "preprocessed = textacy.preprocess.preprocess_text(text, fix_unicode=True, no_contractions=True, no_accents=True)\n",
    "doc = nlp(preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[had]\n",
      "['have']\n"
     ]
    }
   ],
   "source": [
    "verbs = textacy.spacier.utils.get_main_verbs_of_sent([sent for sent in doc.sents][0])\n",
    "print(verbs)\n",
    "verb_lemmas = [verb.lemma_ for verb in verbs]\n",
    "print(verb_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verb-lemma: be\n",
      "[I]\n",
      "subject: I\n",
      "I - have had - a good day today\n",
      "I - do not have - to make another payment until closer to the end of my treatment\n"
     ]
    }
   ],
   "source": [
    "for verb in verbs:\n",
    "    print('verb-lemma:', verb_lemma)\n",
    "    subjects = textacy.spacier.utils.get_subjects_of_verb(verb)\n",
    "    print(subjects)\n",
    "    for subject in subjects:\n",
    "        print('subject:', subject)\n",
    "        statements = textacy.extract.semistructured_statements(doc, subject.text, cue=verb.lemma_)\n",
    "        for entity, cue, fragment in statements:\n",
    "            print(entity, '-', cue, '-', fragment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(I, was, so sad, and felt so upset, and did not to what to do about jessie)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[statement for statement in textacy.extract.semistructured_statements(doc, 'I', cue='be')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(I, was, so sad, and felt so upset, and did not to what to do about jessie)\n"
     ]
    }
   ],
   "source": [
    "for statement in textacy.extract.semistructured_statements(doc, 'I', cue='be'):\n",
    "    print(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "for statement in textacy.extract.semistructured_statements(doc, 'I', cue='felt'):\n",
    "    print(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns (entity, cue, fragment)\n",
    "statements = textacy.extract.semistructured_statements(doc, 'I', cue='feel')\n",
    "\n",
    "for entity, cue, fragment in statements:\n",
    "    print(entity, '-', cue, '-', fragment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get cues\n",
    "all_statements = []\n",
    "for sent in doc.sents:\n",
    "    verbs = textacy.spacier.utils.get_main_verbs_of_sent(sent)\n",
    "#     print('sent:', sent, '\\nverbs:', verbs)\n",
    "    for verb in verbs:\n",
    "        objects = textacy.spacier.utils.get_objects_of_verb(verb)\n",
    "        subjects = textacy.spacier.utils.get_subjects_of_verb(verb)\n",
    "        for subject in subjects:\n",
    "            statements = textacy.extract.semistructured_statements(doc, subject.text, verb.lemma_)\n",
    "            for statement in statements:\n",
    "#                 print(subject, verb, statement)\n",
    "                all_statements += [statement]\n",
    "    \n",
    "    print('\\n')\n",
    "for entity, cue, fragment in set(all_statements):\n",
    "    print(entity, '-', cue, '-', fragment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AllenNLP OIE\n",
    "(meh, doesn't seem to outperform extract_semistructured?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2018 22:43:23 - INFO - allennlp.models.archival -   loading archive file data/openie-model.tar.gz\n",
      "12/06/2018 22:43:23 - INFO - allennlp.models.archival -   extracting archive file data/openie-model.tar.gz to temp dir /tmp/tmp2o1p6ybf\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.params -   type = default\n",
      "12/06/2018 22:43:23 - INFO - allennlp.data.vocabulary -   Loading token dictionary from /tmp/tmp2o1p6ybf/vocabulary.\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.model.Model'> from params {'binary_feature_dim': 100, 'encoder': {'hidden_size': 300, 'input_size': 200, 'num_layers': 8, 'recurrent_dropout_probability': 0.1, 'type': 'alternating_lstm', 'use_highway': True}, 'initializer': [['tag_projection_layer.*weight', {'type': 'orthogonal'}]], 'text_field_embedder': {'tokens': {'embedding_dim': 100, 'trainable': True, 'type': 'embedding'}}, 'type': 'srl'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fa5482169b0>}\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.params -   model.type = srl\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.semantic_role_labeler.SemanticRoleLabeler'> from params {'binary_feature_dim': 100, 'encoder': {'hidden_size': 300, 'input_size': 200, 'num_layers': 8, 'recurrent_dropout_probability': 0.1, 'type': 'alternating_lstm', 'use_highway': True}, 'initializer': [['tag_projection_layer.*weight', {'type': 'orthogonal'}]], 'text_field_embedder': {'tokens': {'embedding_dim': 100, 'trainable': True, 'type': 'embedding'}}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fa5482169b0>}\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'tokens': {'embedding_dim': 100, 'trainable': True, 'type': 'embedding'}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fa5482169b0>}\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.params -   model.text_field_embedder.type = basic\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.params -   model.text_field_embedder.embedder_to_indexer_map = None\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.params -   model.text_field_embedder.allow_unmatched_keys = False\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders = None\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 100, 'trainable': True, 'type': 'embedding'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fa5482169b0>}\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.type = embedding\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.num_embeddings = None\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.vocab_namespace = tokens\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.embedding_dim = 100\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.pretrained_file = None\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.projection_dim = None\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.trainable = True\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.padding_index = None\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.max_norm = None\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.norm_type = 2.0\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.scale_grad_by_freq = False\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.sparse = False\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'hidden_size': 300, 'input_size': 200, 'num_layers': 8, 'recurrent_dropout_probability': 0.1, 'type': 'alternating_lstm', 'use_highway': True} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fa5482169b0>}\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   model.encoder.type = alternating_lstm\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   model.encoder.batch_first = True\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   model.encoder.stateful = False\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   model.encoder.hidden_size = 300\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   model.encoder.input_size = 200\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   model.encoder.num_layers = 8\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   model.encoder.recurrent_dropout_probability = 0.1\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   model.encoder.use_highway = True\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   model.binary_feature_dim = 100\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   model.embedding_dropout = 0.0\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   model.initializer = [['tag_projection_layer.*weight', {'type': 'orthogonal'}]]\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   model.initializer.list.list.type = orthogonal\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   model.label_smoothing = None\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   model.ignore_span_metric = False\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -   Initializing parameters\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -   Initializing tag_projection_layer._module.weight using tag_projection_layer.*weight intitializer\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -   Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      binary_feature_embedding.weight\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_0.input_linearity.bias\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_0.input_linearity.weight\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_0.state_linearity.bias\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_0.state_linearity.weight\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_1.input_linearity.bias\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_1.input_linearity.weight\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_1.state_linearity.bias\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_1.state_linearity.weight\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_2.input_linearity.bias\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_2.input_linearity.weight\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_2.state_linearity.bias\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_2.state_linearity.weight\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_3.input_linearity.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_3.input_linearity.weight\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_3.state_linearity.bias\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_3.state_linearity.weight\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_4.input_linearity.bias\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_4.input_linearity.weight\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_4.state_linearity.bias\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_4.state_linearity.weight\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_5.input_linearity.bias\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_5.input_linearity.weight\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_5.state_linearity.bias\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_5.state_linearity.weight\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_6.input_linearity.bias\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_6.input_linearity.weight\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_6.state_linearity.bias\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_6.state_linearity.weight\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_7.input_linearity.bias\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_7.input_linearity.weight\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_7.state_linearity.bias\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_7.state_linearity.weight\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      tag_projection_layer._module.bias\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_tokens.weight\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'type': 'srl'} and extras {}\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   dataset_reader.type = srl\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.semantic_role_labeling.SrlReader'> from params {} and extras {}\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   dataset_reader.token_indexers = <allennlp.common.params.Params object at 0x7fa5103d0630>\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   dataset_reader.domain_identifier = None\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   dataset_reader.lazy = False\n"
     ]
    }
   ],
   "source": [
    "from allennlp.models.archival import load_archive\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "\n",
    "archive = load_archive('data/openie-model.tar.gz')\n",
    "oie_predictor = Predictor.from_archive(archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'verbs': [{'verb': 'have',\n",
       "    'description': '[ARG0: So] [BV(ARG0: I] [V: have] [ARG1: had] a good day today .',\n",
       "    'tags': ['B-ARG0', 'B-BV(ARG0', 'B-V', 'B-ARG1', 'O', 'O', 'O', 'O', 'O']},\n",
       "   {'verb': 'had',\n",
       "    'description': 'So [ARG0: I] [BV: have] [V: had] [ARG1: a good day today] .',\n",
       "    'tags': ['O',\n",
       "     'B-ARG0',\n",
       "     'B-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']}],\n",
       "  'words': ['So', 'I', 'have', 'had', 'a', 'good', 'day', 'today', '.']},\n",
       " {'verbs': [{'verb': 'found',\n",
       "    'description': '[ARG0: I] [V: found] [AV: out] [ARG1: we got the other half of our funding for my travel grant] , which paid for my friend to come with me .',\n",
       "    'tags': ['B-ARG0',\n",
       "     'B-V',\n",
       "     'B-AV',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'got',\n",
       "    'description': 'I found out [ARG0: we] [V: got] [ARG1: the other half of our funding for my travel grant] , which paid for my friend to come with me .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'paid',\n",
       "    'description': 'I found out we got the other half of our funding for [ARG0: my travel grant] , which [V: paid] [ARG1: for my friend] [ARG2: to come with me] .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'I-ARG0',\n",
       "     'I-ARG0',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'B-ARG2',\n",
       "     'I-ARG2',\n",
       "     'I-ARG2',\n",
       "     'I-ARG2',\n",
       "     'O']},\n",
       "   {'verb': 'come',\n",
       "    'description': 'I found out we got the other half of our funding for my travel grant , which paid for [ARG0: my friend] [BV: to] [V: come] [ARG1: with me] .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'I-ARG0',\n",
       "     'B-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']}],\n",
       "  'words': ['I',\n",
       "   'found',\n",
       "   'out',\n",
       "   'we',\n",
       "   'got',\n",
       "   'the',\n",
       "   'other',\n",
       "   'half',\n",
       "   'of',\n",
       "   'our',\n",
       "   'funding',\n",
       "   'for',\n",
       "   'my',\n",
       "   'travel',\n",
       "   'grant',\n",
       "   ',',\n",
       "   'which',\n",
       "   'paid',\n",
       "   'for',\n",
       "   'my',\n",
       "   'friend',\n",
       "   'to',\n",
       "   'come',\n",
       "   'with',\n",
       "   'me',\n",
       "   '.']},\n",
       " {'verbs': [{'verb': \"'s\",\n",
       "    'description': \"So that [V: 's] [ARG1: good] , [ARG0: she] and I will both get some money back .\",\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'will',\n",
       "    'description': \"So that 's good , [ARG0: she and I] [V: will] both get [ARG1: some money] [ARG2: back] .\",\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'I-ARG0',\n",
       "     'I-ARG0',\n",
       "     'B-V',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'B-ARG2',\n",
       "     'O']},\n",
       "   {'verb': 'get',\n",
       "    'description': \"So that 's good , [ARG0: she and I] [BV: will both] [V: get] [ARG1: some money] [ARG2: back] .\",\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'I-ARG0',\n",
       "     'I-ARG0',\n",
       "     'B-BV',\n",
       "     'I-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'B-ARG2',\n",
       "     'O']}],\n",
       "  'words': ['So',\n",
       "   'that',\n",
       "   \"'s\",\n",
       "   'good',\n",
       "   ',',\n",
       "   'she',\n",
       "   'and',\n",
       "   'I',\n",
       "   'will',\n",
       "   'both',\n",
       "   'get',\n",
       "   'some',\n",
       "   'money',\n",
       "   'back',\n",
       "   '.']},\n",
       " {'verbs': [{'verb': 'took',\n",
       "    'description': '[ARG0: I] [V: took] [ARG1: my dogs] [ARG2: to the pet store] [ARG3: so my girl dog could get] a new collar , but she wanted to beat everyone up .',\n",
       "    'tags': ['B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'B-ARG2',\n",
       "     'I-ARG2',\n",
       "     'I-ARG2',\n",
       "     'I-ARG2',\n",
       "     'B-ARG3',\n",
       "     'I-ARG3',\n",
       "     'I-ARG3',\n",
       "     'I-ARG3',\n",
       "     'I-ARG3',\n",
       "     'I-ARG3',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'could',\n",
       "    'description': 'I took my dogs to the pet store [ARG0: so my] [BV: girl dog] [V: could] [ARG1: get] a new collar , but she wanted to beat everyone up .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'I-ARG0',\n",
       "     'B-BV',\n",
       "     'I-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'get',\n",
       "    'description': 'I took my dogs to the pet store so [ARG0: my girl dog] [BV: could] [V: get] [ARG1: a new collar] , but she wanted to beat everyone up .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'I-ARG0',\n",
       "     'I-ARG0',\n",
       "     'B-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'wanted',\n",
       "    'description': 'I took my dogs to the pet store so my girl dog could get a new collar , but [ARG0: she] [V: wanted] [ARG1: to beat everyone up] .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']},\n",
       "   {'verb': 'beat',\n",
       "    'description': 'I took my dogs to the pet store so my girl dog could get a new collar , but [ARG0: she] [BV: wanted to] [V: beat] [ARG1: everyone] up .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-BV',\n",
       "     'I-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'O',\n",
       "     'O']}],\n",
       "  'words': ['I',\n",
       "   'took',\n",
       "   'my',\n",
       "   'dogs',\n",
       "   'to',\n",
       "   'the',\n",
       "   'pet',\n",
       "   'store',\n",
       "   'so',\n",
       "   'my',\n",
       "   'girl',\n",
       "   'dog',\n",
       "   'could',\n",
       "   'get',\n",
       "   'a',\n",
       "   'new',\n",
       "   'collar',\n",
       "   ',',\n",
       "   'but',\n",
       "   'she',\n",
       "   'wanted',\n",
       "   'to',\n",
       "   'beat',\n",
       "   'everyone',\n",
       "   'up',\n",
       "   '.']},\n",
       " {'verbs': [{'verb': 'is',\n",
       "    'description': '[ARG0: This] [V: is] [ARG1: an ongoing issue with her] .',\n",
       "    'tags': ['B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']}],\n",
       "  'words': ['This', 'is', 'an', 'ongoing', 'issue', 'with', 'her', '.']},\n",
       " {'verbs': [{'verb': \"'s\",\n",
       "    'description': \"[ARG0: She] [V: 's] [ARG1: so] [ARG1: little and cute] too\",\n",
       "    'tags': ['B-ARG0', 'B-V', 'B-ARG1', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'O']}],\n",
       "  'words': ['She', \"'s\", 'so', 'little', 'and', 'cute', 'too']},\n",
       " {'verbs': [{'verb': 'acts',\n",
       "    'description': \"but damn [ARG0: she] [V: acts] [ARG1: like she 's gon na] go for the jugular with everyone\",\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': \"'s\",\n",
       "    'description': \"but damn she acts like [ARG1: she] [V: 's] [ARG1: gon na] go for the jugular with everyone\",\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG1',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'gon',\n",
       "    'description': \"but damn she acts like [ARG1: she] 's [V: gon] na go for the jugular with everyone\",\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG1',\n",
       "     'O',\n",
       "     'B-V',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'go',\n",
       "    'description': \"but [ARG0: damn she acts like she 's gon na] [V: go] [ARG1: for the jugular] with everyone\",\n",
       "    'tags': ['O',\n",
       "     'B-ARG0',\n",
       "     'I-ARG0',\n",
       "     'I-ARG0',\n",
       "     'I-ARG0',\n",
       "     'I-ARG0',\n",
       "     'I-ARG0',\n",
       "     'I-ARG0',\n",
       "     'I-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O']}],\n",
       "  'words': ['but',\n",
       "   'damn',\n",
       "   'she',\n",
       "   'acts',\n",
       "   'like',\n",
       "   'she',\n",
       "   \"'s\",\n",
       "   'gon',\n",
       "   'na',\n",
       "   'go',\n",
       "   'for',\n",
       "   'the',\n",
       "   'jugular',\n",
       "   'with',\n",
       "   'everyone']},\n",
       " {'verbs': [{'verb': 'does',\n",
       "    'description': '[ARG0: she] [V: does] [ARG1: not] know !',\n",
       "    'tags': ['B-ARG0', 'B-V', 'B-ARG1', 'O', 'O']},\n",
       "   {'verb': 'know',\n",
       "    'description': '[ARG0: she] [BV: does not] [V: know] !',\n",
       "    'tags': ['B-ARG0', 'B-BV', 'I-BV', 'B-V', 'O']}],\n",
       "  'words': ['she', 'does', 'not', 'know', '!']},\n",
       " {'verbs': [{'verb': 'did',\n",
       "    'description': '[ARG0: She] [V: did] [ARG1: end] up [ARG1: with a cute new collar] tho',\n",
       "    'tags': ['B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'O',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']},\n",
       "   {'verb': 'end',\n",
       "    'description': '[ARG0: She] [BV: did] [V: end] [AV: up] [ARG1: with a cute new collar tho]',\n",
       "    'tags': ['B-ARG0',\n",
       "     'B-BV',\n",
       "     'B-V',\n",
       "     'B-AV',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1']}],\n",
       "  'words': ['She',\n",
       "   'did',\n",
       "   'end',\n",
       "   'up',\n",
       "   'with',\n",
       "   'a',\n",
       "   'cute',\n",
       "   'new',\n",
       "   'collar',\n",
       "   'tho']},\n",
       " {'verbs': [{'verb': 'has',\n",
       "    'description': ', [ARG0: it] [V: has] [ARG1: pineapples on it] .',\n",
       "    'tags': ['O', 'B-ARG0', 'B-V', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'O']}],\n",
       "  'words': [',', 'it', 'has', 'pineapples', 'on', 'it', '.']},\n",
       " {'verbs': [{'verb': 'went',\n",
       "    'description': '[ARG0: I] [V: went] [ARG1: to the dentist]',\n",
       "    'tags': ['B-ARG0', 'B-V', 'B-ARG1', 'I-ARG1', 'I-ARG1']}],\n",
       "  'words': ['I', 'went', 'to', 'the', 'dentist']},\n",
       " {'verbs': [{'verb': \"'s\",\n",
       "    'description': \"and [ARG0: she] [V: 's] [ARG1: happy with my Invisalign progress] .\",\n",
       "    'tags': ['O',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']}],\n",
       "  'words': ['and',\n",
       "   'she',\n",
       "   \"'s\",\n",
       "   'happy',\n",
       "   'with',\n",
       "   'my',\n",
       "   'Invisalign',\n",
       "   'progress',\n",
       "   '.']},\n",
       " {'verbs': [{'verb': 'have',\n",
       "    'description': '[ARG0: We] [V: have] [ARG1: three more trays]',\n",
       "    'tags': ['B-ARG0', 'B-V', 'B-ARG1', 'I-ARG1', 'I-ARG1']}],\n",
       "  'words': ['We', 'have', 'three', 'more', 'trays']},\n",
       " {'verbs': [{'verb': 'does',\n",
       "    'description': 'and then [ARG0: she] [V: does] [ARG1: an impression to make sure my teeth] are where they need to be before they get the rest of the trays .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'make',\n",
       "    'description': 'and then she does [ARG0: an] impression [BV: to] [V: make] [ARG1: sure my teeth] are where they need to be before they get the rest of the trays .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'O',\n",
       "     'B-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'are',\n",
       "    'description': 'and then she does an impression to make sure [ARG0: my teeth] [V: are] [ARG1: where they need to be before they get the rest of the trays] .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'I-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']},\n",
       "   {'verb': 'need',\n",
       "    'description': 'and then she does an impression to make sure my teeth are where [ARG0: they] [V: need] [ARG1: to be before they get the rest of the trays] .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']},\n",
       "   {'verb': 'be',\n",
       "    'description': 'and then she does an impression to make sure my teeth are where [ARG0: they] [BV: need] [BV: to] [V: be] [ARG1: before they get the rest of the trays] .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-BV',\n",
       "     'B-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']},\n",
       "   {'verb': 'get',\n",
       "    'description': 'and then she does an impression to make sure my teeth are where they need to be before [ARG0: they] [V: get] [ARG1: the rest of the trays] .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']}],\n",
       "  'words': ['and',\n",
       "   'then',\n",
       "   'she',\n",
       "   'does',\n",
       "   'an',\n",
       "   'impression',\n",
       "   'to',\n",
       "   'make',\n",
       "   'sure',\n",
       "   'my',\n",
       "   'teeth',\n",
       "   'are',\n",
       "   'where',\n",
       "   'they',\n",
       "   'need',\n",
       "   'to',\n",
       "   'be',\n",
       "   'before',\n",
       "   'they',\n",
       "   'get',\n",
       "   'the',\n",
       "   'rest',\n",
       "   'of',\n",
       "   'the',\n",
       "   'trays',\n",
       "   '.']},\n",
       " {'verbs': [], 'words': ['YAY', '!']},\n",
       " {'verbs': [{'verb': 'do',\n",
       "    'description': '[ARG2: And] [ARG0: I] [V: do] not have [ARG1: to make another payment until closer to the end of my treatment] .',\n",
       "    'tags': ['B-ARG2',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']},\n",
       "   {'verb': 'have',\n",
       "    'description': '[ARG2: And] [ARG0: I] [BV: do not] [V: have] [ARG1: to make another payment until closer to the end of my treatment] .',\n",
       "    'tags': ['B-ARG2',\n",
       "     'B-ARG0',\n",
       "     'B-BV',\n",
       "     'I-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']},\n",
       "   {'verb': 'make',\n",
       "    'description': 'And [ARG0: I] do not have [BV: to] [V: make] [ARG1: another payment] [ARG2: until closer to the end of my treatment] .',\n",
       "    'tags': ['O',\n",
       "     'B-ARG0',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'B-ARG2',\n",
       "     'I-ARG2',\n",
       "     'I-ARG2',\n",
       "     'I-ARG2',\n",
       "     'I-ARG2',\n",
       "     'I-ARG2',\n",
       "     'I-ARG2',\n",
       "     'I-ARG2',\n",
       "     'O']}],\n",
       "  'words': ['And',\n",
       "   'I',\n",
       "   'do',\n",
       "   'not',\n",
       "   'have',\n",
       "   'to',\n",
       "   'make',\n",
       "   'another',\n",
       "   'payment',\n",
       "   'until',\n",
       "   'closer',\n",
       "   'to',\n",
       "   'the',\n",
       "   'end',\n",
       "   'of',\n",
       "   'my',\n",
       "   'treatment',\n",
       "   '.']},\n",
       " {'verbs': [{'verb': 'had',\n",
       "    'description': '[ARG0: I] [V: had] [ARG1: some work emails with the festival] , and Jessie was bringing up some important points , and one of our potential artists was too expensive to work with , so Mutual Friend was asking for names for some other people we could work with .',\n",
       "    'tags': ['B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'was',\n",
       "    'description': 'I had some work emails with the festival , and [ARG0: Jessie] [V: was] [ARG1: bringing up some important points] , and one of our potential artists was too expensive to work with , so Mutual Friend was asking for names for some other people we could work with .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'bringing',\n",
       "    'description': 'I had some work emails with the festival , and [ARG0: Jessie] [BV: was] [V: bringing] [ARG1: up] [ARG1: some important points] , and one of our potential artists was too expensive to work with , so Mutual Friend was asking for names for some other people we could work with .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'was',\n",
       "    'description': 'I had some work emails with the festival , and Jessie was bringing up some important points , and [ARG0: one of our potential artists] [V: was] [ARG1: too expensive to work with] , so Mutual Friend was asking for names for some other people we could work with .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'I-ARG0',\n",
       "     'I-ARG0',\n",
       "     'I-ARG0',\n",
       "     'I-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'work',\n",
       "    'description': 'I had some work emails with the festival , and Jessie was bringing up some important points , and [ARG0: one of our potential artists] was too expensive [BV: to] [V: work] [ARG1: with] , so Mutual Friend was asking for names for some other people we could work with .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'I-ARG0',\n",
       "     'I-ARG0',\n",
       "     'I-ARG0',\n",
       "     'I-ARG0',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'was',\n",
       "    'description': 'I had some work emails with the festival , and Jessie was bringing up some important points , and one of our potential artists was too expensive to work with , so [ARG0: Mutual Friend] [V: was] [ARG1: asking for names for some other people] we could work with .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'I-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'asking',\n",
       "    'description': 'I had some work emails with the festival , and Jessie was bringing up some important points , and one of our potential artists was too expensive to work with , so [ARG0: Mutual Friend] [BV: was] [V: asking] [ARG1: for names for some other people] we could work with .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'I-ARG0',\n",
       "     'B-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'could',\n",
       "    'description': 'I had some work emails with the festival , and Jessie was bringing up some important points , and one of our potential artists was too expensive to work with , so Mutual Friend was asking for names [ARG0: for some other] [BV: people] [ARG1: we] [V: could] [ARG1: work] with .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'I-ARG0',\n",
       "     'I-ARG0',\n",
       "     'B-BV',\n",
       "     'B-ARG1',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'work',\n",
       "    'description': 'I had some work emails with the festival , and Jessie was bringing up some important points , and one of our potential artists was too expensive to work with , so Mutual Friend was asking for names for [ARG0: some other people] we [BV: could] [V: work] [ARG1: with] .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'I-ARG0',\n",
       "     'I-ARG0',\n",
       "     'O',\n",
       "     'B-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'O']}],\n",
       "  'words': ['I',\n",
       "   'had',\n",
       "   'some',\n",
       "   'work',\n",
       "   'emails',\n",
       "   'with',\n",
       "   'the',\n",
       "   'festival',\n",
       "   ',',\n",
       "   'and',\n",
       "   'Jessie',\n",
       "   'was',\n",
       "   'bringing',\n",
       "   'up',\n",
       "   'some',\n",
       "   'important',\n",
       "   'points',\n",
       "   ',',\n",
       "   'and',\n",
       "   'one',\n",
       "   'of',\n",
       "   'our',\n",
       "   'potential',\n",
       "   'artists',\n",
       "   'was',\n",
       "   'too',\n",
       "   'expensive',\n",
       "   'to',\n",
       "   'work',\n",
       "   'with',\n",
       "   ',',\n",
       "   'so',\n",
       "   'Mutual',\n",
       "   'Friend',\n",
       "   'was',\n",
       "   'asking',\n",
       "   'for',\n",
       "   'names',\n",
       "   'for',\n",
       "   'some',\n",
       "   'other',\n",
       "   'people',\n",
       "   'we',\n",
       "   'could',\n",
       "   'work',\n",
       "   'with',\n",
       "   '.']},\n",
       " {'verbs': [{'verb': 'suggested',\n",
       "    'description': '[ARG2: So] [ARG0: I] [V: suggested] [ARG1: like , three artists , and Jessie] actually liked the idea of one of them doing it .',\n",
       "    'tags': ['B-ARG2',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'liked',\n",
       "    'description': 'So I suggested like , [ARG0: three artists , and Jessie] [BV: actually] [V: liked] [ARG1: the idea of one of them doing it] .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'I-ARG0',\n",
       "     'I-ARG0',\n",
       "     'I-ARG0',\n",
       "     'I-ARG0',\n",
       "     'B-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']},\n",
       "   {'verb': 'doing',\n",
       "    'description': 'So I suggested like , three artists , and Jessie actually liked the idea of one of [ARG0: them] [V: doing] [ARG1: it] .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'O']}],\n",
       "  'words': ['So',\n",
       "   'I',\n",
       "   'suggested',\n",
       "   'like',\n",
       "   ',',\n",
       "   'three',\n",
       "   'artists',\n",
       "   ',',\n",
       "   'and',\n",
       "   'Jessie',\n",
       "   'actually',\n",
       "   'liked',\n",
       "   'the',\n",
       "   'idea',\n",
       "   'of',\n",
       "   'one',\n",
       "   'of',\n",
       "   'them',\n",
       "   'doing',\n",
       "   'it',\n",
       "   '.']},\n",
       " {'verbs': [{'verb': 'is',\n",
       "    'description': '[ARG0: Which] [V: is] [ARG1: nice] .',\n",
       "    'tags': ['B-ARG0', 'B-V', 'B-ARG1', 'O']}],\n",
       "  'words': ['Which', 'is', 'nice', '.']},\n",
       " {'verbs': [{'verb': 'notice',\n",
       "    'description': '[ARG0: I] [V: notice] [ARG1: she is very encouraging at whatever I contribute to our collective] .',\n",
       "    'tags': ['B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']},\n",
       "   {'verb': 'is',\n",
       "    'description': '[ARG0: I notice] [ARG0: she] [V: is] [ARG1: very encouraging at whatever I contribute to our collective] .',\n",
       "    'tags': ['B-ARG0',\n",
       "     'I-ARG0',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']},\n",
       "   {'verb': 'contribute',\n",
       "    'description': 'I notice she is very encouraging at whatever [ARG0: I] [V: contribute] [ARG1: to our collective] .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']}],\n",
       "  'words': ['I',\n",
       "   'notice',\n",
       "   'she',\n",
       "   'is',\n",
       "   'very',\n",
       "   'encouraging',\n",
       "   'at',\n",
       "   'whatever',\n",
       "   'I',\n",
       "   'contribute',\n",
       "   'to',\n",
       "   'our',\n",
       "   'collective',\n",
       "   '.']},\n",
       " {'verbs': [{'verb': \"'s\",\n",
       "    'description': \"[ARG0: It] [V: 's] [ARG1: sweet] .\",\n",
       "    'tags': ['B-ARG0', 'B-V', 'B-ARG1', 'O']}],\n",
       "  'words': ['It', \"'s\", 'sweet', '.']},\n",
       " {'verbs': [{'verb': 'know',\n",
       "    'description': \"[ARG0: I] [BV: kind] [BV: of] [V: know] [ARG1: this is like] , the only link we have with each other right now besides social media , so it seems like she 's trying to make sure\",\n",
       "    'tags': ['B-ARG0',\n",
       "     'B-BV',\n",
       "     'B-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'is',\n",
       "    'description': \"[ARG0: I kind of know this] [V: is] [ARG1: like] , the only link we have with each other right now besides social media , so it seems like she 's trying to make sure\",\n",
       "    'tags': ['B-ARG0',\n",
       "     'I-ARG0',\n",
       "     'I-ARG0',\n",
       "     'I-ARG0',\n",
       "     'I-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'have',\n",
       "    'description': \"I kind of know this is like , [ARG0: the only link] we [V: have] [ARG1: with each other right] now besides social media , so it seems like she 's trying to make sure\",\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'I-ARG0',\n",
       "     'I-ARG0',\n",
       "     'O',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'seems',\n",
       "    'description': \"I kind of know this is like , the only link we have with each other right now besides social media , so [ARG0: it] [V: seems] [ARG1: like she 's trying] to make sure\",\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': \"'s\",\n",
       "    'description': \"I kind of know this is like , the only link we have with each other right now besides social media , so it seems like [ARG0: she] [V: 's] trying to make sure\",\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'trying',\n",
       "    'description': \"I kind of know this is like , the only link we have with each other right now besides social media , so it seems like [ARG0: she] 's [V: trying] [ARG1: to make sure]\",\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'O',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1']},\n",
       "   {'verb': 'make',\n",
       "    'description': \"I kind of know this is like , the only link we have with each other right now besides social media , so it seems like [ARG0: she] 's [BV: trying to] [V: make] [ARG1: sure]\",\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'O',\n",
       "     'B-BV',\n",
       "     'I-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1']}],\n",
       "  'words': ['I',\n",
       "   'kind',\n",
       "   'of',\n",
       "   'know',\n",
       "   'this',\n",
       "   'is',\n",
       "   'like',\n",
       "   ',',\n",
       "   'the',\n",
       "   'only',\n",
       "   'link',\n",
       "   'we',\n",
       "   'have',\n",
       "   'with',\n",
       "   'each',\n",
       "   'other',\n",
       "   'right',\n",
       "   'now',\n",
       "   'besides',\n",
       "   'social',\n",
       "   'media',\n",
       "   ',',\n",
       "   'so',\n",
       "   'it',\n",
       "   'seems',\n",
       "   'like',\n",
       "   'she',\n",
       "   \"'s\",\n",
       "   'trying',\n",
       "   'to',\n",
       "   'make',\n",
       "   'sure']},\n",
       " {'verbs': [{'verb': 'know',\n",
       "    'description': '[ARG0: I] [V: know] [ARG1: she still wants me to be involved and does not have bad feelings for me] .',\n",
       "    'tags': ['B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']},\n",
       "   {'verb': 'wants',\n",
       "    'description': 'I know [ARG0: she] [ARG2: still] [V: wants] [ARG1: me to be involved and does not have bad feelings for me] .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-ARG2',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']},\n",
       "   {'verb': 'be',\n",
       "    'description': 'I know she still wants [ARG0: me] [BV: to] [V: be] [ARG1: involved] and does not have bad feelings for me .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'involved',\n",
       "    'description': 'I know she still wants [ARG0: me] [BV: to be] [V: involved] and does not have bad feelings for me .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-BV',\n",
       "     'I-BV',\n",
       "     'B-V',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'does',\n",
       "    'description': '[ARG0: I know she] still wants me to be involved and [V: does] not have [ARG1: bad feelings for me] .',\n",
       "    'tags': ['B-ARG0',\n",
       "     'I-ARG0',\n",
       "     'I-ARG0',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-V',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']},\n",
       "   {'verb': 'have',\n",
       "    'description': '[ARG0: I know she] still wants me to be involved and [BV: does not] [V: have] [ARG1: bad feelings for me] .',\n",
       "    'tags': ['B-ARG0',\n",
       "     'I-ARG0',\n",
       "     'I-ARG0',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-BV',\n",
       "     'I-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']}],\n",
       "  'words': ['I',\n",
       "   'know',\n",
       "   'she',\n",
       "   'still',\n",
       "   'wants',\n",
       "   'me',\n",
       "   'to',\n",
       "   'be',\n",
       "   'involved',\n",
       "   'and',\n",
       "   'does',\n",
       "   'not',\n",
       "   'have',\n",
       "   'bad',\n",
       "   'feelings',\n",
       "   'for',\n",
       "   'me',\n",
       "   '.']},\n",
       " {'verbs': [{'verb': 'was',\n",
       "    'description': '[ARG0: And there] [V: was] [ARG1: a short period when I was seriously thinking of leaving the collective and not working with this festival anymore] .',\n",
       "    'tags': ['B-ARG0',\n",
       "     'I-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']},\n",
       "   {'verb': 'was',\n",
       "    'description': 'And there was a short period when [ARG0: I] [V: was] [ARG1: seriously thinking of leaving the collective and not working with this festival] anymore .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'thinking',\n",
       "    'description': 'And there was a short period when [ARG0: I] [BV: was seriously] [V: thinking] [ARG1: of leaving the collective and not working with this festival] anymore .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-BV',\n",
       "     'I-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'leaving',\n",
       "    'description': 'And there was a short period when [ARG0: I] was seriously thinking of [V: leaving] [ARG1: the collective] and not working with this festival anymore .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'working',\n",
       "    'description': 'And there was a short period when [ARG0: I] was seriously thinking of leaving the collective and [BV: not] [V: working] [ARG1: with this festival] anymore .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O']}],\n",
       "  'words': ['And',\n",
       "   'there',\n",
       "   'was',\n",
       "   'a',\n",
       "   'short',\n",
       "   'period',\n",
       "   'when',\n",
       "   'I',\n",
       "   'was',\n",
       "   'seriously',\n",
       "   'thinking',\n",
       "   'of',\n",
       "   'leaving',\n",
       "   'the',\n",
       "   'collective',\n",
       "   'and',\n",
       "   'not',\n",
       "   'working',\n",
       "   'with',\n",
       "   'this',\n",
       "   'festival',\n",
       "   'anymore',\n",
       "   '.']},\n",
       " {'verbs': [{'verb': 'was',\n",
       "    'description': '[ARG0: I] [V: was] [ARG1: so sad] , and felt so upset , and did not know what to do about Jessie .',\n",
       "    'tags': ['B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'felt',\n",
       "    'description': '[ARG0: I] was so sad , and [V: felt] [ARG1: so upset] , and did not know what to do about Jessie .',\n",
       "    'tags': ['B-ARG0',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'did',\n",
       "    'description': '[ARG0: I] was so sad , and felt so upset , and [V: did] [AV: not] [ARG1: know what to do about Jessie] .',\n",
       "    'tags': ['B-ARG0',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-V',\n",
       "     'B-AV',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']},\n",
       "   {'verb': 'know',\n",
       "    'description': '[ARG0: I] was so sad , and felt so upset , and [BV: did not] [V: know] [ARG1: what to do about Jessie] .',\n",
       "    'tags': ['B-ARG0',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-BV',\n",
       "     'I-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']},\n",
       "   {'verb': 'do',\n",
       "    'description': '[ARG0: I] was so sad , and felt so upset , and did not know [BV: what to] [V: do] [ARG1: about Jessie] .',\n",
       "    'tags': ['B-ARG0',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-BV',\n",
       "     'I-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']}],\n",
       "  'words': ['I',\n",
       "   'was',\n",
       "   'so',\n",
       "   'sad',\n",
       "   ',',\n",
       "   'and',\n",
       "   'felt',\n",
       "   'so',\n",
       "   'upset',\n",
       "   ',',\n",
       "   'and',\n",
       "   'did',\n",
       "   'not',\n",
       "   'know',\n",
       "   'what',\n",
       "   'to',\n",
       "   'do',\n",
       "   'about',\n",
       "   'Jessie',\n",
       "   '.']},\n",
       " {'verbs': [{'verb': 'felt',\n",
       "    'description': '[ARG0: It] [V: felt] [ARG1: really close to me throwing in the towel] .',\n",
       "    'tags': ['B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']},\n",
       "   {'verb': 'throwing',\n",
       "    'description': 'It felt really close to [ARG0: me] [V: throwing] [ARG1: in the towel] .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']}],\n",
       "  'words': ['It',\n",
       "   'felt',\n",
       "   'really',\n",
       "   'close',\n",
       "   'to',\n",
       "   'me',\n",
       "   'throwing',\n",
       "   'in',\n",
       "   'the',\n",
       "   'towel',\n",
       "   '.']},\n",
       " {'verbs': [{'verb': 'hung',\n",
       "    'description': 'But [ARG0: I] [V: hung] [ARG1: on through the festival] and it does not seem so bad from this viewpoint now with more time that has passed .',\n",
       "    'tags': ['O',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'does',\n",
       "    'description': 'But I hung on through the festival and it [V: does] not seem so bad from this viewpoint now with more time that has passed .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-V',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'seem',\n",
       "    'description': 'But I hung on through the festival and [ARG0: it] [BV: does not] [V: seem] [ARG1: so bad from this viewpoint now with more] time that has passed .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-BV',\n",
       "     'I-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'has',\n",
       "    'description': 'But I hung on through the festival and it does not seem so bad from this viewpoint now [ARG0: with more] [BV: time] that [V: has] passed .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'I-ARG0',\n",
       "     'B-BV',\n",
       "     'O',\n",
       "     'B-V',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'passed',\n",
       "    'description': 'But I hung on through the festival and it does not seem so bad from this viewpoint now with [ARG0: more time] [ARG1: that] [BV: has] [V: passed] .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'I-ARG0',\n",
       "     'B-ARG1',\n",
       "     'B-BV',\n",
       "     'B-V',\n",
       "     'O']}],\n",
       "  'words': ['But',\n",
       "   'I',\n",
       "   'hung',\n",
       "   'on',\n",
       "   'through',\n",
       "   'the',\n",
       "   'festival',\n",
       "   'and',\n",
       "   'it',\n",
       "   'does',\n",
       "   'not',\n",
       "   'seem',\n",
       "   'so',\n",
       "   'bad',\n",
       "   'from',\n",
       "   'this',\n",
       "   'viewpoint',\n",
       "   'now',\n",
       "   'with',\n",
       "   'more',\n",
       "   'time',\n",
       "   'that',\n",
       "   'has',\n",
       "   'passed',\n",
       "   '.']},\n",
       " {'verbs': [{'verb': 'have',\n",
       "    'description': '[BV(ARG0: And] [ARG0: we] [V: have] [ARG1: been] gentle , if reserved , with each other .',\n",
       "    'tags': ['B-BV(ARG0',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'been',\n",
       "    'description': '[ARG2: And] [ARG0: we] [BV: have] [V: been] [ARG1: gentle] , if reserved , with each other .',\n",
       "    'tags': ['B-ARG2',\n",
       "     'B-ARG0',\n",
       "     'B-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'reserved',\n",
       "    'description': 'And [ARG0: we] have been gentle , if [V: reserved] , [ARG1: with each other] .',\n",
       "    'tags': ['O',\n",
       "     'B-ARG0',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-V',\n",
       "     'O',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']}],\n",
       "  'words': ['And',\n",
       "   'we',\n",
       "   'have',\n",
       "   'been',\n",
       "   'gentle',\n",
       "   ',',\n",
       "   'if',\n",
       "   'reserved',\n",
       "   ',',\n",
       "   'with',\n",
       "   'each',\n",
       "   'other',\n",
       "   '.']},\n",
       " {'verbs': [{'verb': 'mean',\n",
       "    'description': '[ARG0: I] [V: mean] [ARG1: her last personal email to] me',\n",
       "    'tags': ['B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']}],\n",
       "  'words': ['I', 'mean', 'her', 'last', 'personal', 'email', 'to', 'me']},\n",
       " {'verbs': [{'verb': 'was',\n",
       "    'description': \"however [ARG0: many weeks ago] [V: was] [AV: n't] [ARG1: very nice] .\",\n",
       "    'tags': ['O',\n",
       "     'B-ARG0',\n",
       "     'I-ARG0',\n",
       "     'I-ARG0',\n",
       "     'B-V',\n",
       "     'B-AV',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']}],\n",
       "  'words': ['however',\n",
       "   'many',\n",
       "   'weeks',\n",
       "   'ago',\n",
       "   'was',\n",
       "   \"n't\",\n",
       "   'very',\n",
       "   'nice',\n",
       "   '.']},\n",
       " {'verbs': [{'verb': 'seems',\n",
       "    'description': 'But [ARG0: it] [V: seems] [ARG1: like we have been able to put it aside for work reasons] .',\n",
       "    'tags': ['O',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']},\n",
       "   {'verb': 'have',\n",
       "    'description': 'But it seems like [BV(ARG0: we] [V: have] [ARG1: been] able to put it aside for work reasons .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-BV(ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'been',\n",
       "    'description': 'But it seems like [ARG0: we] [BV: have] [V: been] [ARG1: able to put it aside for work reasons] .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']},\n",
       "   {'verb': 'put',\n",
       "    'description': 'But it seems like [ARG0: we] have been able [BV: to] [V: put] [ARG1: it aside] [ARG2: for work reasons] .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'B-ARG2',\n",
       "     'I-ARG2',\n",
       "     'I-ARG2',\n",
       "     'O']}],\n",
       "  'words': ['But',\n",
       "   'it',\n",
       "   'seems',\n",
       "   'like',\n",
       "   'we',\n",
       "   'have',\n",
       "   'been',\n",
       "   'able',\n",
       "   'to',\n",
       "   'put',\n",
       "   'it',\n",
       "   'aside',\n",
       "   'for',\n",
       "   'work',\n",
       "   'reasons',\n",
       "   '.']},\n",
       " {'verbs': [{'verb': 'dunno',\n",
       "    'description': '[ARG0: I] [V: dunno] .',\n",
       "    'tags': ['B-ARG0', 'B-V', 'O']}],\n",
       "  'words': ['I', 'dunno', '.']},\n",
       " {'verbs': [{'verb': 'feel',\n",
       "    'description': '[ARG0: I] [ARG2: still] [V: feel] [ARG1: like if anything was gon na get mended between us] , she would need to make the first moves on that .',\n",
       "    'tags': ['B-ARG0',\n",
       "     'B-ARG2',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'was',\n",
       "    'description': 'I still feel like if [ARG0: anything] [V: was] [ARG1: gon na get mended between us] , she would need to make the first moves on that .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'gon',\n",
       "    'description': 'I still feel like if [ARG0: anything] [BV: was] [V: gon] [ARG1: na] get mended between us , she would need to make the first moves on that .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'get',\n",
       "    'description': 'I still feel like if anything was [ARG0: gon na] [V: get] [ARG1: mended] [ARG2: between us] , she would need to make the first moves on that .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'I-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'B-ARG2',\n",
       "     'I-ARG2',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'mended',\n",
       "    'description': 'I still feel like if anything was [ARG0: gon na] [BV: get] [V: mended] [ARG1: between us] , she would need to make the first moves on that .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'I-ARG0',\n",
       "     'B-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'would',\n",
       "    'description': 'I still feel like if anything was gon na get mended between us [BV(ARG0: , she] [V: would] [ARG1: need] to make the first moves on that .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-BV(ARG0',\n",
       "     'I-BV(ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'need',\n",
       "    'description': 'I still feel like if anything was gon na get mended between us , [ARG0: she] [BV: would] [V: need] [ARG1: to make the first moves on that] .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']},\n",
       "   {'verb': 'make',\n",
       "    'description': 'I still feel like if anything was gon na get mended between us , [ARG0: she] [BV: would need to] [V: make] [ARG1: the first moves on that] .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-BV',\n",
       "     'I-BV',\n",
       "     'I-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']}],\n",
       "  'words': ['I',\n",
       "   'still',\n",
       "   'feel',\n",
       "   'like',\n",
       "   'if',\n",
       "   'anything',\n",
       "   'was',\n",
       "   'gon',\n",
       "   'na',\n",
       "   'get',\n",
       "   'mended',\n",
       "   'between',\n",
       "   'us',\n",
       "   ',',\n",
       "   'she',\n",
       "   'would',\n",
       "   'need',\n",
       "   'to',\n",
       "   'make',\n",
       "   'the',\n",
       "   'first',\n",
       "   'moves',\n",
       "   'on',\n",
       "   'that',\n",
       "   '.']},\n",
       " {'verbs': [{'verb': 'do',\n",
       "    'description': '[ARG0: I] [BV: really] [V: do] [ARG1: not want to try reaching out] and get rejected even as a friend again .',\n",
       "    'tags': ['B-ARG0',\n",
       "     'B-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'want',\n",
       "    'description': '[ARG0: I] [BV: really do not] [V: want] [ARG1: to try reaching out and get rejected even as a friend again] .',\n",
       "    'tags': ['B-ARG0',\n",
       "     'B-BV',\n",
       "     'I-BV',\n",
       "     'I-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']},\n",
       "   {'verb': 'try',\n",
       "    'description': '[ARG0: I] [BV: really do not want to] [V: try] [ARG1: reaching out] and get rejected even as a friend again .',\n",
       "    'tags': ['B-ARG0',\n",
       "     'B-BV',\n",
       "     'I-BV',\n",
       "     'I-BV',\n",
       "     'I-BV',\n",
       "     'I-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'reaching',\n",
       "    'description': '[ARG0: I] [BV: really do not want to try] [V: reaching] [ARG1: out] and get rejected even as a friend again .',\n",
       "    'tags': ['B-ARG0',\n",
       "     'B-BV',\n",
       "     'I-BV',\n",
       "     'I-BV',\n",
       "     'I-BV',\n",
       "     'I-BV',\n",
       "     'I-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'get',\n",
       "    'description': '[ARG0: I] really do not want to try reaching out and [V: get] [ARG1: rejected even as a friend] again .',\n",
       "    'tags': ['B-ARG0',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'rejected',\n",
       "    'description': '[ARG0: I] really do not want to try reaching out and [BV: get] [V: rejected] [AV: even] [ARG1: as a friend again] .',\n",
       "    'tags': ['B-ARG0',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-BV',\n",
       "     'B-V',\n",
       "     'B-AV',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']}],\n",
       "  'words': ['I',\n",
       "   'really',\n",
       "   'do',\n",
       "   'not',\n",
       "   'want',\n",
       "   'to',\n",
       "   'try',\n",
       "   'reaching',\n",
       "   'out',\n",
       "   'and',\n",
       "   'get',\n",
       "   'rejected',\n",
       "   'even',\n",
       "   'as',\n",
       "   'a',\n",
       "   'friend',\n",
       "   'again',\n",
       "   '.']},\n",
       " {'verbs': [{'verb': 'miss',\n",
       "    'description': '[ARG0: I] [V: miss] [ARG1: her] [ARG2: though] .',\n",
       "    'tags': ['B-ARG0', 'B-V', 'B-ARG1', 'B-ARG2', 'O']}],\n",
       "  'words': ['I', 'miss', 'her', 'though', '.']},\n",
       " {'verbs': [{'verb': 'think',\n",
       "    'description': '[ARG2: And sometimes] [ARG0: I] [V: think] [ARG1: she misses me] .',\n",
       "    'tags': ['B-ARG2',\n",
       "     'I-ARG2',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']},\n",
       "   {'verb': 'misses',\n",
       "    'description': 'And sometimes I think [ARG0: she] [V: misses] [ARG1: me] .',\n",
       "    'tags': ['O', 'O', 'O', 'O', 'B-ARG0', 'B-V', 'B-ARG1', 'O']}],\n",
       "  'words': ['And', 'sometimes', 'I', 'think', 'she', 'misses', 'me', '.']},\n",
       " {'verbs': [{'verb': 'do',\n",
       "    'description': 'But [ARG0: I] [V: do] not want [ARG1: to approach her assuming] we both miss each other and have her turn it on me again and make out like all these things are all in my head .',\n",
       "    'tags': ['O',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'want',\n",
       "    'description': 'But [ARG0: I] [BV: do not] [V: want] [ARG1: to approach her assuming we both miss each other] and have her turn it on me again and make out like all these things are all in my head .',\n",
       "    'tags': ['O',\n",
       "     'B-ARG0',\n",
       "     'B-BV',\n",
       "     'I-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'approach',\n",
       "    'description': 'But [ARG0: I] [BV: do not want to] [V: approach] [ARG1: her assuming we both miss each other] and have her turn it on me again and make out like all these things are all in my head .',\n",
       "    'tags': ['O',\n",
       "     'B-ARG0',\n",
       "     'B-BV',\n",
       "     'I-BV',\n",
       "     'I-BV',\n",
       "     'I-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'assuming',\n",
       "    'description': 'But I do not want to approach [ARG0: her] [V: assuming] [ARG1: we both miss each other and have her turn it on me] again and make out like all these things are all in my head .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'miss',\n",
       "    'description': 'But I do not want to approach her assuming [ARG0: we] both [V: miss] [ARG1: each other] and have her turn it on me again and make out like all these things are all in my head .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'O',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'have',\n",
       "    'description': 'But I do not want to approach her assuming [ARG0: we] both miss each other and [V: have] [ARG1: her turn it on me again] and make out like all these things are all in my head .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'turn',\n",
       "    'description': 'But I do not want to approach her assuming we both miss each other and have [ARG0: her] [V: turn] [ARG1: it] [ARG2: on me again] and make out like all these things are all in my head .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'B-ARG2',\n",
       "     'I-ARG2',\n",
       "     'I-ARG2',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'make',\n",
       "    'description': 'But [ARG0: I] do not want to approach her assuming we both miss each other and have her turn it on me again and [V: make] [AV: out] [ARG1: like all these things] are all in my head .',\n",
       "    'tags': ['O',\n",
       "     'B-ARG0',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-V',\n",
       "     'B-AV',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'are',\n",
       "    'description': 'But I do not want to approach her assuming we both miss each other and have her turn it on me again and make out like [ARG0: all these things] [V: are] [ARG1: all in my head] .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'I-ARG0',\n",
       "     'I-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']}],\n",
       "  'words': ['But',\n",
       "   'I',\n",
       "   'do',\n",
       "   'not',\n",
       "   'want',\n",
       "   'to',\n",
       "   'approach',\n",
       "   'her',\n",
       "   'assuming',\n",
       "   'we',\n",
       "   'both',\n",
       "   'miss',\n",
       "   'each',\n",
       "   'other',\n",
       "   'and',\n",
       "   'have',\n",
       "   'her',\n",
       "   'turn',\n",
       "   'it',\n",
       "   'on',\n",
       "   'me',\n",
       "   'again',\n",
       "   'and',\n",
       "   'make',\n",
       "   'out',\n",
       "   'like',\n",
       "   'all',\n",
       "   'these',\n",
       "   'things',\n",
       "   'are',\n",
       "   'all',\n",
       "   'in',\n",
       "   'my',\n",
       "   'head',\n",
       "   '.']},\n",
       " {'verbs': [{'verb': 'do',\n",
       "    'description': '[ARG0: I] [V: do] not know [ARG1: about that butch]',\n",
       "    'tags': ['B-ARG0', 'B-V', 'O', 'O', 'B-ARG1', 'I-ARG1', 'I-ARG1']},\n",
       "   {'verb': 'know',\n",
       "    'description': '[ARG0: I] [BV: do not] [V: know] [ARG1: about that butch]',\n",
       "    'tags': ['B-ARG0', 'B-BV', 'I-BV', 'B-V', 'B-ARG1', 'I-ARG1', 'I-ARG1']}],\n",
       "  'words': ['I', 'do', 'not', 'know', 'about', 'that', 'butch']},\n",
       " {'verbs': [{'verb': 'went',\n",
       "    'description': '[ARG0: I] [V: went] [ARG1: on a date with last night] .',\n",
       "    'tags': ['B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']}],\n",
       "  'words': ['I', 'went', 'on', 'a', 'date', 'with', 'last', 'night', '.']},\n",
       " {'verbs': [{'verb': 'feel',\n",
       "    'description': '[ARG0: I] [V: feel] [ARG1: more of a friend vibe from her , than a romantic] one .',\n",
       "    'tags': ['B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O']}],\n",
       "  'words': ['I',\n",
       "   'feel',\n",
       "   'more',\n",
       "   'of',\n",
       "   'a',\n",
       "   'friend',\n",
       "   'vibe',\n",
       "   'from',\n",
       "   'her',\n",
       "   ',',\n",
       "   'than',\n",
       "   'a',\n",
       "   'romantic',\n",
       "   'one',\n",
       "   '.']},\n",
       " {'verbs': [{'verb': 'can',\n",
       "    'description': '[ARG0: I] [V: can] not help [ARG1: it] , I am just not attracted to butches .',\n",
       "    'tags': ['B-ARG0',\n",
       "     'B-V',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'help',\n",
       "    'description': '[ARG0: I] [BV: can not] [V: help] [ARG1: it] , I am just not attracted to butches .',\n",
       "    'tags': ['B-ARG0',\n",
       "     'B-BV',\n",
       "     'I-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'am',\n",
       "    'description': 'I can not help it , [ARG0: I] [V: am] [ARG1: just not attracted to butches] .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']},\n",
       "   {'verb': 'attracted',\n",
       "    'description': 'I can not help it , [ARG0: I] [BV: am] [BV: just not] [V: attracted] [ARG1: to butches] .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-BV',\n",
       "     'B-BV',\n",
       "     'I-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']}],\n",
       "  'words': ['I',\n",
       "   'can',\n",
       "   'not',\n",
       "   'help',\n",
       "   'it',\n",
       "   ',',\n",
       "   'I',\n",
       "   'am',\n",
       "   'just',\n",
       "   'not',\n",
       "   'attracted',\n",
       "   'to',\n",
       "   'butches',\n",
       "   '.']},\n",
       " {'verbs': [{'verb': 'do',\n",
       "    'description': '[ARG2: And] [ARG0: I] [V: do] [AV: not] [ARG1: know how to flirt with them] .',\n",
       "    'tags': ['B-ARG2',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'B-AV',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']},\n",
       "   {'verb': 'know',\n",
       "    'description': '[ARG2: And] [ARG0: I] [BV: do not] [V: know] [ARG1: how to flirt with them] .',\n",
       "    'tags': ['B-ARG2',\n",
       "     'B-ARG0',\n",
       "     'B-BV',\n",
       "     'I-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']},\n",
       "   {'verb': 'flirt',\n",
       "    'description': 'And [ARG0: I] do not know [BV: how to] [V: flirt] [ARG1: with them] .',\n",
       "    'tags': ['O',\n",
       "     'B-ARG0',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-BV',\n",
       "     'I-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']}],\n",
       "  'words': ['And',\n",
       "   'I',\n",
       "   'do',\n",
       "   'not',\n",
       "   'know',\n",
       "   'how',\n",
       "   'to',\n",
       "   'flirt',\n",
       "   'with',\n",
       "   'them',\n",
       "   '.']},\n",
       " {'verbs': [{'verb': 'do',\n",
       "    'description': '[ARG2: And] [ARG0: I] [V: do] [AV: not] [ARG1: think of them in a sexy way] .',\n",
       "    'tags': ['B-ARG2',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'B-AV',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']},\n",
       "   {'verb': 'think',\n",
       "    'description': '[ARG3: And] [ARG0: I] [BV: do not] [V: think] [ARG1: of them] [ARG2: in a sexy way] .',\n",
       "    'tags': ['B-ARG3',\n",
       "     'B-ARG0',\n",
       "     'B-BV',\n",
       "     'I-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'B-ARG2',\n",
       "     'I-ARG2',\n",
       "     'I-ARG2',\n",
       "     'I-ARG2',\n",
       "     'O']}],\n",
       "  'words': ['And',\n",
       "   'I',\n",
       "   'do',\n",
       "   'not',\n",
       "   'think',\n",
       "   'of',\n",
       "   'them',\n",
       "   'in',\n",
       "   'a',\n",
       "   'sexy',\n",
       "   'way',\n",
       "   '.']},\n",
       " {'verbs': [{'verb': 'WOULD',\n",
       "    'description': 'But [ARG0: I] [V: WOULD] [ARG1: like another butch buddy] .',\n",
       "    'tags': ['O',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']},\n",
       "   {'verb': 'like',\n",
       "    'description': 'But [ARG0: I WOULD] [V: like] [ARG1: another butch buddy] .',\n",
       "    'tags': ['O',\n",
       "     'B-ARG0',\n",
       "     'I-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']}],\n",
       "  'words': ['But', 'I', 'WOULD', 'like', 'another', 'butch', 'buddy', '.']},\n",
       " {'verbs': [{'verb': 'mean',\n",
       "    'description': '[ARG0: I] [V: mean]',\n",
       "    'tags': ['B-ARG0', 'B-V']}],\n",
       "  'words': ['I', 'mean']},\n",
       " {'verbs': [], 'words': ['yeah']},\n",
       " {'verbs': [{'verb': 'do',\n",
       "    'description': 'maybe [ARG0: Femmes] [V: do] [ARG1: play games] , or maybe I just chased all the wrong Femmes .',\n",
       "    'tags': ['O',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'play',\n",
       "    'description': 'maybe [ARG0: Femmes] [BV: do] [V: play] [ARG1: games] , or maybe I just chased all the wrong Femmes .',\n",
       "    'tags': ['O',\n",
       "     'B-ARG0',\n",
       "     'B-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'chased',\n",
       "    'description': 'maybe Femmes do play games , or maybe [ARG0: I] [BV: just] [V: chased] [ARG1: all the wrong Femmes] .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']}],\n",
       "  'words': ['maybe',\n",
       "   'Femmes',\n",
       "   'do',\n",
       "   'play',\n",
       "   'games',\n",
       "   ',',\n",
       "   'or',\n",
       "   'maybe',\n",
       "   'I',\n",
       "   'just',\n",
       "   'chased',\n",
       "   'all',\n",
       "   'the',\n",
       "   'wrong',\n",
       "   'Femmes',\n",
       "   '.']},\n",
       " {'verbs': [{'verb': 'will',\n",
       "    'description': '[ARG0: Maybe] [ARG0: I] [V: will] just leave [ARG1: this] and not think about it much until I get back to town in January .',\n",
       "    'tags': ['B-ARG0',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'leave',\n",
       "    'description': 'Maybe [ARG0: I] [BV: will just] [V: leave] [ARG1: this] and not think about it much until I get back to town in January .',\n",
       "    'tags': ['O',\n",
       "     'B-ARG0',\n",
       "     'B-BV',\n",
       "     'I-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O']},\n",
       "   {'verb': 'think',\n",
       "    'description': '[ARG0: Maybe] [ARG0: I] will just leave this and [BV: not] [V: think] [ARG1: about it] [ARG2: much until I get back to town in January] .',\n",
       "    'tags': ['B-ARG0',\n",
       "     'B-ARG0',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-BV',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'B-ARG2',\n",
       "     'I-ARG2',\n",
       "     'I-ARG2',\n",
       "     'I-ARG2',\n",
       "     'I-ARG2',\n",
       "     'I-ARG2',\n",
       "     'I-ARG2',\n",
       "     'I-ARG2',\n",
       "     'I-ARG2',\n",
       "     'O']},\n",
       "   {'verb': 'get',\n",
       "    'description': 'Maybe I will just leave this and not think about it much until [ARG0: I] [V: get] [ARG1: back to town in January] .',\n",
       "    'tags': ['O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'O',\n",
       "     'B-ARG0',\n",
       "     'B-V',\n",
       "     'B-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'I-ARG1',\n",
       "     'O']}],\n",
       "  'words': ['Maybe',\n",
       "   'I',\n",
       "   'will',\n",
       "   'just',\n",
       "   'leave',\n",
       "   'this',\n",
       "   'and',\n",
       "   'not',\n",
       "   'think',\n",
       "   'about',\n",
       "   'it',\n",
       "   'much',\n",
       "   'until',\n",
       "   'I',\n",
       "   'get',\n",
       "   'back',\n",
       "   'to',\n",
       "   'town',\n",
       "   'in',\n",
       "   'January',\n",
       "   '.']}]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = []\n",
    "for sent in doc.sents:\n",
    "    predictions += [oie_predictor.predict(sentence=sent.text)]\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ARG0: So] [BV(ARG0: I] [V: have] [ARG1: had] a good day today .\n",
      "So [ARG0: I] [BV: have] [V: had] [ARG1: a good day today] .\n",
      "\n",
      "[ARG0: I] [V: found] [AV: out] [ARG1: we got the other half of our funding for my travel grant] , which paid for my friend to come with me .\n",
      "I found out [ARG0: we] [V: got] [ARG1: the other half of our funding for my travel grant] , which paid for my friend to come with me .\n",
      "I found out we got the other half of our funding for [ARG0: my travel grant] , which [V: paid] [ARG1: for my friend] [ARG2: to come with me] .\n",
      "I found out we got the other half of our funding for my travel grant , which paid for [ARG0: my friend] [BV: to] [V: come] [ARG1: with me] .\n",
      "\n",
      "So that [V: 's] [ARG1: good] , [ARG0: she] and I will both get some money back .\n",
      "So that 's good , [ARG0: she and I] [V: will] both get [ARG1: some money] [ARG2: back] .\n",
      "So that 's good , [ARG0: she and I] [BV: will both] [V: get] [ARG1: some money] [ARG2: back] .\n",
      "\n",
      "[ARG0: I] [V: took] [ARG1: my dogs] [ARG2: to the pet store] [ARG3: so my girl dog could get] a new collar , but she wanted to beat everyone up .\n",
      "I took my dogs to the pet store [ARG0: so my] [BV: girl dog] [V: could] [ARG1: get] a new collar , but she wanted to beat everyone up .\n",
      "I took my dogs to the pet store so [ARG0: my girl dog] [BV: could] [V: get] [ARG1: a new collar] , but she wanted to beat everyone up .\n",
      "I took my dogs to the pet store so my girl dog could get a new collar , but [ARG0: she] [V: wanted] [ARG1: to beat everyone up] .\n",
      "I took my dogs to the pet store so my girl dog could get a new collar , but [ARG0: she] [BV: wanted to] [V: beat] [ARG1: everyone] up .\n",
      "\n",
      "[ARG0: This] [V: is] [ARG1: an ongoing issue with her] .\n",
      "\n",
      "[ARG0: She] [V: 's] [ARG1: so] [ARG1: little and cute] too\n",
      "\n",
      "but damn [ARG0: she] [V: acts] [ARG1: like she 's gon na] go for the jugular with everyone\n",
      "but damn she acts like [ARG1: she] [V: 's] [ARG1: gon na] go for the jugular with everyone\n",
      "but damn she acts like [ARG1: she] 's [V: gon] na go for the jugular with everyone\n",
      "but [ARG0: damn she acts like she 's gon na] [V: go] [ARG1: for the jugular] with everyone\n",
      "\n",
      "[ARG0: she] [V: does] [ARG1: not] know !\n",
      "[ARG0: she] [BV: does not] [V: know] !\n",
      "\n",
      "[ARG0: She] [V: did] [ARG1: end] up [ARG1: with a cute new collar] tho\n",
      "[ARG0: She] [BV: did] [V: end] [AV: up] [ARG1: with a cute new collar tho]\n",
      "\n",
      ", [ARG0: it] [V: has] [ARG1: pineapples on it] .\n",
      "\n",
      "[ARG0: I] [V: went] [ARG1: to the dentist]\n",
      "\n",
      "and [ARG0: she] [V: 's] [ARG1: happy with my Invisalign progress] .\n",
      "\n",
      "[ARG0: We] [V: have] [ARG1: three more trays]\n",
      "\n",
      "and then [ARG0: she] [V: does] [ARG1: an impression to make sure my teeth] are where they need to be before they get the rest of the trays .\n",
      "and then she does [ARG0: an] impression [BV: to] [V: make] [ARG1: sure my teeth] are where they need to be before they get the rest of the trays .\n",
      "and then she does an impression to make sure [ARG0: my teeth] [V: are] [ARG1: where they need to be before they get the rest of the trays] .\n",
      "and then she does an impression to make sure my teeth are where [ARG0: they] [V: need] [ARG1: to be before they get the rest of the trays] .\n",
      "and then she does an impression to make sure my teeth are where [ARG0: they] [BV: need] [BV: to] [V: be] [ARG1: before they get the rest of the trays] .\n",
      "and then she does an impression to make sure my teeth are where they need to be before [ARG0: they] [V: get] [ARG1: the rest of the trays] .\n",
      "\n",
      "\n",
      "[ARG2: And] [ARG0: I] [V: do] not have [ARG1: to make another payment until closer to the end of my treatment] .\n",
      "[ARG2: And] [ARG0: I] [BV: do not] [V: have] [ARG1: to make another payment until closer to the end of my treatment] .\n",
      "And [ARG0: I] do not have [BV: to] [V: make] [ARG1: another payment] [ARG2: until closer to the end of my treatment] .\n",
      "\n",
      "[ARG0: I] [V: had] [ARG1: some work emails with the festival] , and Jessie was bringing up some important points , and one of our potential artists was too expensive to work with , so Mutual Friend was asking for names for some other people we could work with .\n",
      "I had some work emails with the festival , and [ARG0: Jessie] [V: was] [ARG1: bringing up some important points] , and one of our potential artists was too expensive to work with , so Mutual Friend was asking for names for some other people we could work with .\n",
      "I had some work emails with the festival , and [ARG0: Jessie] [BV: was] [V: bringing] [ARG1: up] [ARG1: some important points] , and one of our potential artists was too expensive to work with , so Mutual Friend was asking for names for some other people we could work with .\n",
      "I had some work emails with the festival , and Jessie was bringing up some important points , and [ARG0: one of our potential artists] [V: was] [ARG1: too expensive to work with] , so Mutual Friend was asking for names for some other people we could work with .\n",
      "I had some work emails with the festival , and Jessie was bringing up some important points , and [ARG0: one of our potential artists] was too expensive [BV: to] [V: work] [ARG1: with] , so Mutual Friend was asking for names for some other people we could work with .\n",
      "I had some work emails with the festival , and Jessie was bringing up some important points , and one of our potential artists was too expensive to work with , so [ARG0: Mutual Friend] [V: was] [ARG1: asking for names for some other people] we could work with .\n",
      "I had some work emails with the festival , and Jessie was bringing up some important points , and one of our potential artists was too expensive to work with , so [ARG0: Mutual Friend] [BV: was] [V: asking] [ARG1: for names for some other people] we could work with .\n",
      "I had some work emails with the festival , and Jessie was bringing up some important points , and one of our potential artists was too expensive to work with , so Mutual Friend was asking for names [ARG0: for some other] [BV: people] [ARG1: we] [V: could] [ARG1: work] with .\n",
      "I had some work emails with the festival , and Jessie was bringing up some important points , and one of our potential artists was too expensive to work with , so Mutual Friend was asking for names for [ARG0: some other people] we [BV: could] [V: work] [ARG1: with] .\n",
      "\n",
      "[ARG2: So] [ARG0: I] [V: suggested] [ARG1: like , three artists , and Jessie] actually liked the idea of one of them doing it .\n",
      "So I suggested like , [ARG0: three artists , and Jessie] [BV: actually] [V: liked] [ARG1: the idea of one of them doing it] .\n",
      "So I suggested like , three artists , and Jessie actually liked the idea of one of [ARG0: them] [V: doing] [ARG1: it] .\n",
      "\n",
      "[ARG0: Which] [V: is] [ARG1: nice] .\n",
      "\n",
      "[ARG0: I] [V: notice] [ARG1: she is very encouraging at whatever I contribute to our collective] .\n",
      "[ARG0: I notice] [ARG0: she] [V: is] [ARG1: very encouraging at whatever I contribute to our collective] .\n",
      "I notice she is very encouraging at whatever [ARG0: I] [V: contribute] [ARG1: to our collective] .\n",
      "\n",
      "[ARG0: It] [V: 's] [ARG1: sweet] .\n",
      "\n",
      "[ARG0: I] [BV: kind] [BV: of] [V: know] [ARG1: this is like] , the only link we have with each other right now besides social media , so it seems like she 's trying to make sure\n",
      "[ARG0: I kind of know this] [V: is] [ARG1: like] , the only link we have with each other right now besides social media , so it seems like she 's trying to make sure\n",
      "I kind of know this is like , [ARG0: the only link] we [V: have] [ARG1: with each other right] now besides social media , so it seems like she 's trying to make sure\n",
      "I kind of know this is like , the only link we have with each other right now besides social media , so [ARG0: it] [V: seems] [ARG1: like she 's trying] to make sure\n",
      "I kind of know this is like , the only link we have with each other right now besides social media , so it seems like [ARG0: she] [V: 's] trying to make sure\n",
      "I kind of know this is like , the only link we have with each other right now besides social media , so it seems like [ARG0: she] 's [V: trying] [ARG1: to make sure]\n",
      "I kind of know this is like , the only link we have with each other right now besides social media , so it seems like [ARG0: she] 's [BV: trying to] [V: make] [ARG1: sure]\n",
      "\n",
      "[ARG0: I] [V: know] [ARG1: she still wants me to be involved and does not have bad feelings for me] .\n",
      "I know [ARG0: she] [ARG2: still] [V: wants] [ARG1: me to be involved and does not have bad feelings for me] .\n",
      "I know she still wants [ARG0: me] [BV: to] [V: be] [ARG1: involved] and does not have bad feelings for me .\n",
      "I know she still wants [ARG0: me] [BV: to be] [V: involved] and does not have bad feelings for me .\n",
      "[ARG0: I know she] still wants me to be involved and [V: does] not have [ARG1: bad feelings for me] .\n",
      "[ARG0: I know she] still wants me to be involved and [BV: does not] [V: have] [ARG1: bad feelings for me] .\n",
      "\n",
      "[ARG0: And there] [V: was] [ARG1: a short period when I was seriously thinking of leaving the collective and not working with this festival anymore] .\n",
      "And there was a short period when [ARG0: I] [V: was] [ARG1: seriously thinking of leaving the collective and not working with this festival] anymore .\n",
      "And there was a short period when [ARG0: I] [BV: was seriously] [V: thinking] [ARG1: of leaving the collective and not working with this festival] anymore .\n",
      "And there was a short period when [ARG0: I] was seriously thinking of [V: leaving] [ARG1: the collective] and not working with this festival anymore .\n",
      "And there was a short period when [ARG0: I] was seriously thinking of leaving the collective and [BV: not] [V: working] [ARG1: with this festival] anymore .\n",
      "\n",
      "[ARG0: I] [V: was] [ARG1: so sad] , and felt so upset , and did not know what to do about Jessie .\n",
      "[ARG0: I] was so sad , and [V: felt] [ARG1: so upset] , and did not know what to do about Jessie .\n",
      "[ARG0: I] was so sad , and felt so upset , and [V: did] [AV: not] [ARG1: know what to do about Jessie] .\n",
      "[ARG0: I] was so sad , and felt so upset , and [BV: did not] [V: know] [ARG1: what to do about Jessie] .\n",
      "[ARG0: I] was so sad , and felt so upset , and did not know [BV: what to] [V: do] [ARG1: about Jessie] .\n",
      "\n",
      "[ARG0: It] [V: felt] [ARG1: really close to me throwing in the towel] .\n",
      "It felt really close to [ARG0: me] [V: throwing] [ARG1: in the towel] .\n",
      "\n",
      "But [ARG0: I] [V: hung] [ARG1: on through the festival] and it does not seem so bad from this viewpoint now with more time that has passed .\n",
      "But I hung on through the festival and it [V: does] not seem so bad from this viewpoint now with more time that has passed .\n",
      "But I hung on through the festival and [ARG0: it] [BV: does not] [V: seem] [ARG1: so bad from this viewpoint now with more] time that has passed .\n",
      "But I hung on through the festival and it does not seem so bad from this viewpoint now [ARG0: with more] [BV: time] that [V: has] passed .\n",
      "But I hung on through the festival and it does not seem so bad from this viewpoint now with [ARG0: more time] [ARG1: that] [BV: has] [V: passed] .\n",
      "\n",
      "[BV(ARG0: And] [ARG0: we] [V: have] [ARG1: been] gentle , if reserved , with each other .\n",
      "[ARG2: And] [ARG0: we] [BV: have] [V: been] [ARG1: gentle] , if reserved , with each other .\n",
      "And [ARG0: we] have been gentle , if [V: reserved] , [ARG1: with each other] .\n",
      "\n",
      "[ARG0: I] [V: mean] [ARG1: her last personal email to] me\n",
      "\n",
      "however [ARG0: many weeks ago] [V: was] [AV: n't] [ARG1: very nice] .\n",
      "\n",
      "But [ARG0: it] [V: seems] [ARG1: like we have been able to put it aside for work reasons] .\n",
      "But it seems like [BV(ARG0: we] [V: have] [ARG1: been] able to put it aside for work reasons .\n",
      "But it seems like [ARG0: we] [BV: have] [V: been] [ARG1: able to put it aside for work reasons] .\n",
      "But it seems like [ARG0: we] have been able [BV: to] [V: put] [ARG1: it aside] [ARG2: for work reasons] .\n",
      "\n",
      "[ARG0: I] [V: dunno] .\n",
      "\n",
      "[ARG0: I] [ARG2: still] [V: feel] [ARG1: like if anything was gon na get mended between us] , she would need to make the first moves on that .\n",
      "I still feel like if [ARG0: anything] [V: was] [ARG1: gon na get mended between us] , she would need to make the first moves on that .\n",
      "I still feel like if [ARG0: anything] [BV: was] [V: gon] [ARG1: na] get mended between us , she would need to make the first moves on that .\n",
      "I still feel like if anything was [ARG0: gon na] [V: get] [ARG1: mended] [ARG2: between us] , she would need to make the first moves on that .\n",
      "I still feel like if anything was [ARG0: gon na] [BV: get] [V: mended] [ARG1: between us] , she would need to make the first moves on that .\n",
      "I still feel like if anything was gon na get mended between us [BV(ARG0: , she] [V: would] [ARG1: need] to make the first moves on that .\n",
      "I still feel like if anything was gon na get mended between us , [ARG0: she] [BV: would] [V: need] [ARG1: to make the first moves on that] .\n",
      "I still feel like if anything was gon na get mended between us , [ARG0: she] [BV: would need to] [V: make] [ARG1: the first moves on that] .\n",
      "\n",
      "[ARG0: I] [BV: really] [V: do] [ARG1: not want to try reaching out] and get rejected even as a friend again .\n",
      "[ARG0: I] [BV: really do not] [V: want] [ARG1: to try reaching out and get rejected even as a friend again] .\n",
      "[ARG0: I] [BV: really do not want to] [V: try] [ARG1: reaching out] and get rejected even as a friend again .\n",
      "[ARG0: I] [BV: really do not want to try] [V: reaching] [ARG1: out] and get rejected even as a friend again .\n",
      "[ARG0: I] really do not want to try reaching out and [V: get] [ARG1: rejected even as a friend] again .\n",
      "[ARG0: I] really do not want to try reaching out and [BV: get] [V: rejected] [AV: even] [ARG1: as a friend again] .\n",
      "\n",
      "[ARG0: I] [V: miss] [ARG1: her] [ARG2: though] .\n",
      "\n",
      "[ARG2: And sometimes] [ARG0: I] [V: think] [ARG1: she misses me] .\n",
      "And sometimes I think [ARG0: she] [V: misses] [ARG1: me] .\n",
      "\n",
      "But [ARG0: I] [V: do] not want [ARG1: to approach her assuming] we both miss each other and have her turn it on me again and make out like all these things are all in my head .\n",
      "But [ARG0: I] [BV: do not] [V: want] [ARG1: to approach her assuming we both miss each other] and have her turn it on me again and make out like all these things are all in my head .\n",
      "But [ARG0: I] [BV: do not want to] [V: approach] [ARG1: her assuming we both miss each other] and have her turn it on me again and make out like all these things are all in my head .\n",
      "But I do not want to approach [ARG0: her] [V: assuming] [ARG1: we both miss each other and have her turn it on me] again and make out like all these things are all in my head .\n",
      "But I do not want to approach her assuming [ARG0: we] both [V: miss] [ARG1: each other] and have her turn it on me again and make out like all these things are all in my head .\n",
      "But I do not want to approach her assuming [ARG0: we] both miss each other and [V: have] [ARG1: her turn it on me again] and make out like all these things are all in my head .\n",
      "But I do not want to approach her assuming we both miss each other and have [ARG0: her] [V: turn] [ARG1: it] [ARG2: on me again] and make out like all these things are all in my head .\n",
      "But [ARG0: I] do not want to approach her assuming we both miss each other and have her turn it on me again and [V: make] [AV: out] [ARG1: like all these things] are all in my head .\n",
      "But I do not want to approach her assuming we both miss each other and have her turn it on me again and make out like [ARG0: all these things] [V: are] [ARG1: all in my head] .\n",
      "\n",
      "[ARG0: I] [V: do] not know [ARG1: about that butch]\n",
      "[ARG0: I] [BV: do not] [V: know] [ARG1: about that butch]\n",
      "\n",
      "[ARG0: I] [V: went] [ARG1: on a date with last night] .\n",
      "\n",
      "[ARG0: I] [V: feel] [ARG1: more of a friend vibe from her , than a romantic] one .\n",
      "\n",
      "[ARG0: I] [V: can] not help [ARG1: it] , I am just not attracted to butches .\n",
      "[ARG0: I] [BV: can not] [V: help] [ARG1: it] , I am just not attracted to butches .\n",
      "I can not help it , [ARG0: I] [V: am] [ARG1: just not attracted to butches] .\n",
      "I can not help it , [ARG0: I] [BV: am] [BV: just not] [V: attracted] [ARG1: to butches] .\n",
      "\n",
      "[ARG2: And] [ARG0: I] [V: do] [AV: not] [ARG1: know how to flirt with them] .\n",
      "[ARG2: And] [ARG0: I] [BV: do not] [V: know] [ARG1: how to flirt with them] .\n",
      "And [ARG0: I] do not know [BV: how to] [V: flirt] [ARG1: with them] .\n",
      "\n",
      "[ARG2: And] [ARG0: I] [V: do] [AV: not] [ARG1: think of them in a sexy way] .\n",
      "[ARG3: And] [ARG0: I] [BV: do not] [V: think] [ARG1: of them] [ARG2: in a sexy way] .\n",
      "\n",
      "But [ARG0: I] [V: WOULD] [ARG1: like another butch buddy] .\n",
      "But [ARG0: I WOULD] [V: like] [ARG1: another butch buddy] .\n",
      "\n",
      "[ARG0: I] [V: mean]\n",
      "\n",
      "\n",
      "maybe [ARG0: Femmes] [V: do] [ARG1: play games] , or maybe I just chased all the wrong Femmes .\n",
      "maybe [ARG0: Femmes] [BV: do] [V: play] [ARG1: games] , or maybe I just chased all the wrong Femmes .\n",
      "maybe Femmes do play games , or maybe [ARG0: I] [BV: just] [V: chased] [ARG1: all the wrong Femmes] .\n",
      "\n",
      "[ARG0: Maybe] [ARG0: I] [V: will] just leave [ARG1: this] and not think about it much until I get back to town in January .\n",
      "Maybe [ARG0: I] [BV: will just] [V: leave] [ARG1: this] and not think about it much until I get back to town in January .\n",
      "[ARG0: Maybe] [ARG0: I] will just leave this and [BV: not] [V: think] [ARG1: about it] [ARG2: much until I get back to town in January] .\n",
      "Maybe I will just leave this and not think about it much until [ARG0: I] [V: get] [ARG1: back to town in January] .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "descriptions = []\n",
    "for prediction in predictions:\n",
    "    for verb in prediction['verbs']:\n",
    "        descriptions += [verb['description']]\n",
    "        print(verb['description'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verbs': [{'verb': \"'s\",\n",
       "   'description': \"and [ARG0: she] [V: 's] [ARG1: happy with my Invisalign progress] .\",\n",
       "   'tags': ['O',\n",
       "    'B-ARG0',\n",
       "    'B-V',\n",
       "    'B-ARG1',\n",
       "    'I-ARG1',\n",
       "    'I-ARG1',\n",
       "    'I-ARG1',\n",
       "    'I-ARG1',\n",
       "    'O']}],\n",
       " 'words': ['and',\n",
       "  'she',\n",
       "  \"'s\",\n",
       "  'happy',\n",
       "  'with',\n",
       "  'my',\n",
       "  'Invisalign',\n",
       "  'progress',\n",
       "  '.']}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verbs': [{'verb': 'feel',\n",
       "   'description': '[ARG0: I] [V: feel] [ARG1: bad] .',\n",
       "   'tags': ['B-ARG0', 'B-V', 'B-ARG1', 'O']}],\n",
       " 'words': ['I', 'feel', 'bad', '.']}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oie_predictor.predict(\n",
    "  sentence=\"I feel bad.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2018 21:16:45 - INFO - allennlp.common.file_utils -   https://s3-us-west-2.amazonaws.com/allennlp/models/openie-model.2018-08-20.tar.gz not found in cache, downloading to /tmp/tmpzzo9jnen\n",
      "100%|██████████| 65722182/65722182 [00:18<00:00, 3518115.57B/s]\n",
      "12/06/2018 21:17:04 - INFO - allennlp.common.file_utils -   copying /tmp/tmpzzo9jnen to cache at /home/russell/.allennlp/cache/dd04ba717be48bea13525e4293a243477876cdb0f0166abb8b09b5ed2e17cb3e.d68991c3e6de7fbcb5cf3e605d0e298f12cb857ca9d70aa8683abc886aa49edd\n",
      "12/06/2018 21:17:04 - INFO - allennlp.common.file_utils -   creating metadata file for /home/russell/.allennlp/cache/dd04ba717be48bea13525e4293a243477876cdb0f0166abb8b09b5ed2e17cb3e.d68991c3e6de7fbcb5cf3e605d0e298f12cb857ca9d70aa8683abc886aa49edd\n",
      "12/06/2018 21:17:04 - INFO - allennlp.common.file_utils -   removing temp file /tmp/tmpzzo9jnen\n",
      "12/06/2018 21:17:04 - INFO - allennlp.models.archival -   loading archive file https://s3-us-west-2.amazonaws.com/allennlp/models/openie-model.2018-08-20.tar.gz from cache at /home/russell/.allennlp/cache/dd04ba717be48bea13525e4293a243477876cdb0f0166abb8b09b5ed2e17cb3e.d68991c3e6de7fbcb5cf3e605d0e298f12cb857ca9d70aa8683abc886aa49edd\n",
      "12/06/2018 21:17:04 - INFO - allennlp.models.archival -   extracting archive file /home/russell/.allennlp/cache/dd04ba717be48bea13525e4293a243477876cdb0f0166abb8b09b5ed2e17cb3e.d68991c3e6de7fbcb5cf3e605d0e298f12cb857ca9d70aa8683abc886aa49edd to temp dir /tmp/tmpxls8z_09\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   type = default\n",
      "12/06/2018 21:17:05 - INFO - allennlp.data.vocabulary -   Loading token dictionary from /tmp/tmpxls8z_09/vocabulary.\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.model.Model'> from params {'binary_feature_dim': 100, 'encoder': {'hidden_size': 300, 'input_size': 200, 'num_layers': 8, 'recurrent_dropout_probability': 0.1, 'type': 'alternating_lstm', 'use_highway': True}, 'initializer': [['tag_projection_layer.*weight', {'type': 'orthogonal'}]], 'text_field_embedder': {'tokens': {'embedding_dim': 100, 'trainable': True, 'type': 'embedding'}}, 'type': 'srl'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fcab0068240>}\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.type = srl\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.semantic_role_labeler.SemanticRoleLabeler'> from params {'binary_feature_dim': 100, 'encoder': {'hidden_size': 300, 'input_size': 200, 'num_layers': 8, 'recurrent_dropout_probability': 0.1, 'type': 'alternating_lstm', 'use_highway': True}, 'initializer': [['tag_projection_layer.*weight', {'type': 'orthogonal'}]], 'text_field_embedder': {'tokens': {'embedding_dim': 100, 'trainable': True, 'type': 'embedding'}}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fcab0068240>}\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'tokens': {'embedding_dim': 100, 'trainable': True, 'type': 'embedding'}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fcab0068240>}\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.type = basic\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.embedder_to_indexer_map = None\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.allow_unmatched_keys = False\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders = None\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 100, 'trainable': True, 'type': 'embedding'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fcab0068240>}\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.type = embedding\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.num_embeddings = None\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.vocab_namespace = tokens\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.embedding_dim = 100\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.pretrained_file = None\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.projection_dim = None\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.trainable = True\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.padding_index = None\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.max_norm = None\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.norm_type = 2.0\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.scale_grad_by_freq = False\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.sparse = False\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'hidden_size': 300, 'input_size': 200, 'num_layers': 8, 'recurrent_dropout_probability': 0.1, 'type': 'alternating_lstm', 'use_highway': True} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fcab0068240>}\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.encoder.type = alternating_lstm\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.encoder.batch_first = True\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.encoder.stateful = False\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.encoder.hidden_size = 300\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.encoder.input_size = 200\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.encoder.num_layers = 8\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.encoder.recurrent_dropout_probability = 0.1\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.encoder.use_highway = True\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.params -   model.binary_feature_dim = 100\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.params -   model.embedding_dropout = 0.0\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.params -   model.initializer = [['tag_projection_layer.*weight', {'type': 'orthogonal'}]]\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.params -   model.initializer.list.list.type = orthogonal\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.params -   model.label_smoothing = None\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.params -   model.ignore_span_metric = False\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -   Initializing parameters\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -   Initializing tag_projection_layer._module.weight using tag_projection_layer.*weight intitializer\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -   Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      binary_feature_embedding.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_0.input_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_0.input_linearity.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_0.state_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_0.state_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_1.input_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_1.input_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_1.state_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_1.state_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_2.input_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_2.input_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_2.state_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_2.state_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_3.input_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_3.input_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_3.state_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_3.state_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_4.input_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_4.input_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_4.state_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_4.state_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_5.input_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_5.input_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_5.state_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_5.state_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_6.input_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_6.input_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_6.state_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_6.state_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_7.input_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_7.input_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_7.state_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_7.state_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      tag_projection_layer._module.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_tokens.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'type': 'srl'} and extras {}\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.params -   dataset_reader.type = srl\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.semantic_role_labeling.SrlReader'> from params {} and extras {}\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.params -   dataset_reader.token_indexers = <allennlp.common.params.Params object at 0x7fcab0065b00>\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.params -   dataset_reader.domain_identifier = None\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.params -   dataset_reader.lazy = False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'verbs': [{'verb': 'decided',\n",
       "   'description': '[ARG0: John] [V: decided] [ARG1: to run for office next month] .',\n",
       "   'tags': ['B-ARG0',\n",
       "    'B-V',\n",
       "    'B-ARG1',\n",
       "    'I-ARG1',\n",
       "    'I-ARG1',\n",
       "    'I-ARG1',\n",
       "    'I-ARG1',\n",
       "    'I-ARG1',\n",
       "    'O']},\n",
       "  {'verb': 'run',\n",
       "   'description': '[ARG0: John] [BV: decided to] [V: run] [ARG1: for office] [ARG2: next month] .',\n",
       "   'tags': ['B-ARG0',\n",
       "    'B-BV',\n",
       "    'I-BV',\n",
       "    'B-V',\n",
       "    'B-ARG1',\n",
       "    'I-ARG1',\n",
       "    'B-ARG2',\n",
       "    'I-ARG2',\n",
       "    'O']}],\n",
       " 'words': ['John',\n",
       "  'decided',\n",
       "  'to',\n",
       "  'run',\n",
       "  'for',\n",
       "  'office',\n",
       "  'next',\n",
       "  'month',\n",
       "  '.']}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/openie-model.2018-08-20.tar.gz\")\n",
    "predictor.predict(\n",
    "  sentence=\"John decided to run for office next month.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2018 20:43:08 - INFO - allennlp.models.archival -   loading archive file https://s3-us-west-2.amazonaws.com/allennlp/models/decomposable-attention-elmo-2018.02.19.tar.gz from cache at /home/russell/.allennlp/cache/1dbdfb3ce5af46c5b83353727b579a5596d45a121d59199f1c838928a87e3796.21e6e14db76ce734b669577cc3046333c6bc853767246356b4a8b2c6a85249a8\n",
      "12/06/2018 20:43:08 - INFO - allennlp.models.archival -   extracting archive file /home/russell/.allennlp/cache/1dbdfb3ce5af46c5b83353727b579a5596d45a121d59199f1c838928a87e3796.21e6e14db76ce734b669577cc3046333c6bc853767246356b4a8b2c6a85249a8 to temp dir /tmp/tmpcf1zn45w\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   type = default\n",
      "12/06/2018 20:43:14 - INFO - allennlp.data.vocabulary -   Loading token dictionary from /tmp/tmpcf1zn45w/vocabulary.\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.model.Model'> from params {'type': 'decomposable_attention', 'compare_feedforward': {'activations': 'relu', 'dropout': 0.2, 'hidden_dims': 200, 'input_dim': 2048, 'num_layers': 2}, 'attend_feedforward': {'activations': 'relu', 'dropout': 0.2, 'hidden_dims': 200, 'input_dim': 1024, 'num_layers': 2}, 'initializer': [['.*linear_layers.*weight', {'type': 'xavier_normal'}], ['.*token_embedder_tokens\\\\._projection.*weight', {'type': 'xavier_normal'}]], 'similarity_function': {'type': 'dot_product'}, 'aggregate_feedforward': {'activations': ['relu', 'linear'], 'dropout': [0.2, 0], 'hidden_dims': [200, 3], 'input_dim': 400, 'num_layers': 2}, 'text_field_embedder': {'elmo': {'type': 'elmo_token_embedder', 'do_layer_norm': False, 'dropout': 0.2, 'options_file': '/tmp/tmpcf1zn45w/fta/model.text_field_embedder.elmo.options_file', 'weight_file': '/tmp/tmpcf1zn45w/fta/model.text_field_embedder.elmo.weight_file'}}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fcaafe315c0>}\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.type = decomposable_attention\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.decomposable_attention.DecomposableAttention'> from params {'compare_feedforward': {'activations': 'relu', 'dropout': 0.2, 'hidden_dims': 200, 'input_dim': 2048, 'num_layers': 2}, 'attend_feedforward': {'activations': 'relu', 'dropout': 0.2, 'hidden_dims': 200, 'input_dim': 1024, 'num_layers': 2}, 'initializer': [['.*linear_layers.*weight', {'type': 'xavier_normal'}], ['.*token_embedder_tokens\\\\._projection.*weight', {'type': 'xavier_normal'}]], 'similarity_function': {'type': 'dot_product'}, 'aggregate_feedforward': {'activations': ['relu', 'linear'], 'dropout': [0.2, 0], 'hidden_dims': [200, 3], 'input_dim': 400, 'num_layers': 2}, 'text_field_embedder': {'elmo': {'type': 'elmo_token_embedder', 'do_layer_norm': False, 'dropout': 0.2, 'options_file': '/tmp/tmpcf1zn45w/fta/model.text_field_embedder.elmo.options_file', 'weight_file': '/tmp/tmpcf1zn45w/fta/model.text_field_embedder.elmo.weight_file'}}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fcaafe315c0>}\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'elmo': {'type': 'elmo_token_embedder', 'do_layer_norm': False, 'dropout': 0.2, 'options_file': '/tmp/tmpcf1zn45w/fta/model.text_field_embedder.elmo.options_file', 'weight_file': '/tmp/tmpcf1zn45w/fta/model.text_field_embedder.elmo.weight_file'}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fcaafe315c0>}\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.type = basic\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.embedder_to_indexer_map = None\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.allow_unmatched_keys = False\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders = None\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'type': 'elmo_token_embedder', 'do_layer_norm': False, 'dropout': 0.2, 'options_file': '/tmp/tmpcf1zn45w/fta/model.text_field_embedder.elmo.options_file', 'weight_file': '/tmp/tmpcf1zn45w/fta/model.text_field_embedder.elmo.weight_file'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fcaafe315c0>}\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.type = elmo_token_embedder\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.options_file = /tmp/tmpcf1zn45w/fta/model.text_field_embedder.elmo.options_file\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.weight_file = /tmp/tmpcf1zn45w/fta/model.text_field_embedder.elmo.weight_file\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.requires_grad = False\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.do_layer_norm = False\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.dropout = 0.2\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.namespace_to_cache = None\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.projection_dim = None\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.scalar_mix_parameters = None\n",
      "12/06/2018 20:43:14 - INFO - allennlp.modules.elmo -   Initializing ELMo\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-64bb115feb91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPredictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://s3-us-west-2.amazonaws.com/allennlp/models/decomposable-attention-elmo-2018.02.19.tar.gz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/predictors/predictor.py\u001b[0m in \u001b[0;36mfrom_path\u001b[0;34m(cls, archive_path, predictor_name)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mPredictor\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mPredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchive_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictor_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/models/archival.py\u001b[0m in \u001b[0;36mload_archive\u001b[0;34m(archive_file, cuda_device, overrides, weights_file)\u001b[0m\n\u001b[1;32m    151\u001b[0m                        \u001b[0mweights_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                        \u001b[0mserialization_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserialization_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                        cuda_device=cuda_device)\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtempdir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/models/model.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, config, serialization_dir, weights_file, cuda_device)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# This allows subclasses of Model to override _load.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mby_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserialization_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/models/model.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(cls, config, serialization_dir, weights_file, cuda_device)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;31m# want the code to look for it, so we remove it from the parameters here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mremove_pretrained_embedding_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0mmodel_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcuda_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/common/from_params.py\u001b[0m in \u001b[0;36mfrom_params\u001b[0;34m(cls, params, **extras)\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0mextras\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mextras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtakes_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msubclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;31m# This is not a base class, so convert our params and extras into a dict of kwargs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/common/from_params.py\u001b[0m in \u001b[0;36mfrom_params\u001b[0;34m(cls, params, **extras)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;31m# This class has a constructor, so create kwargs for it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/common/from_params.py\u001b[0m in \u001b[0;36mcreate_kwargs\u001b[0;34m(cls, params, **extras)\u001b[0m\n\u001b[1;32m    145\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannotation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mby_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannotation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msubextras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;31m# Not optional and not supplied, that's an error!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/common/from_params.py\u001b[0m in \u001b[0;36mfrom_params\u001b[0;34m(cls, params, **extras)\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0mextras\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mextras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtakes_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msubclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;31m# This is not a base class, so convert our params and extras into a dict of kwargs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/modules/text_field_embedders/basic_text_field_embedder.py\u001b[0m in \u001b[0;36mfrom_params\u001b[0;34m(cls, vocab, params)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0membedder_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m                 \u001b[0mtoken_embedders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenEmbedder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedder_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/common/from_params.py\u001b[0m in \u001b[0;36mfrom_params\u001b[0;34m(cls, params, **extras)\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0mextras\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mextras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtakes_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msubclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;31m# This is not a base class, so convert our params and extras into a dict of kwargs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/modules/token_embedders/elmo_token_embedder.py\u001b[0m in \u001b[0;36mfrom_params\u001b[0;34m(cls, vocab, params)\u001b[0m\n\u001b[1;32m    124\u001b[0m                    \u001b[0mprojection_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprojection_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                    \u001b[0mvocab_to_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_to_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                    scalar_mix_parameters=scalar_mix_parameters)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/modules/token_embedders/elmo_token_embedder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, options_file, weight_file, do_layer_norm, dropout, requires_grad, projection_dim, vocab_to_cache, scalar_mix_parameters)\u001b[0m\n\u001b[1;32m     63\u001b[0m                           \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                           \u001b[0mvocab_to_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_to_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                           scalar_mix_parameters=scalar_mix_parameters)\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprojection_dim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_projection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elmo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_output_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprojection_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/modules/elmo.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, options_file, weight_file, num_output_representations, requires_grad, do_layer_norm, dropout, vocab_to_cache, keep_sentence_boundaries, scalar_mix_parameters, module)\u001b[0m\n\u001b[1;32m    106\u001b[0m                                         \u001b[0mweight_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                                         \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                                         vocab_to_cache=vocab_to_cache)\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_cached_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab_to_cache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keep_sentence_boundaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeep_sentence_boundaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/modules/elmo.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, options_file, weight_file, requires_grad, vocab_to_cache)\u001b[0m\n\u001b[1;32m    549\u001b[0m                                    \u001b[0mmemory_cell_clip_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lstm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cell_clip'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m                                    \u001b[0mstate_projection_clip_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lstm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'proj_clip'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                                    requires_grad=requires_grad)\n\u001b[0m\u001b[1;32m    552\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elmo_lstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;31m# Number of representation layers including context independent layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/modules/elmo_lstm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size, hidden_size, cell_size, num_layers, requires_grad, recurrent_dropout_probability, memory_cell_clip_value, state_projection_clip_value)\u001b[0m\n\u001b[1;32m     92\u001b[0m                                                     \u001b[0mrecurrent_dropout_probability\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                                                     \u001b[0mmemory_cell_clip_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                                                     state_projection_clip_value)\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mlstm_input_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/modules/lstm_cell_with_projection.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size, hidden_size, cell_size, go_forward, recurrent_dropout_probability, memory_cell_clip_value, state_projection_clip_value)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# Additional projection matrix for making the hidden state smaller.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_projection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/modules/lstm_cell_with_projection.py\u001b[0m in \u001b[0;36mreset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# Use sensible default initializations for parameters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mblock_orthogonal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_linearity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mblock_orthogonal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_linearity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_linearity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/nn/initializers.py\u001b[0m in \u001b[0;36mblock_orthogonal\u001b[0;34m(tensor, split_sizes, gain)\u001b[0m\n\u001b[1;32m    146\u001b[0m         block_slice = tuple([slice(start_index, start_index + step)\n\u001b[1;32m    147\u001b[0m                              for start_index, step in index_and_step_tuples])\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblock_slice\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morthogonal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblock_slice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/init.py\u001b[0m in \u001b[0;36morthogonal_\u001b[0;34m(tensor, gain)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0mph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m     \u001b[0mq\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrows\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from allennlp.predictors import Predictor\n",
    "predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/decomposable-attention-elmo-2018.02.19.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label_logits': [-3.391864776611328, 4.570619106292725, 0.9505535364151001],\n",
       " 'label_probs': [0.00033908773912116885,\n",
       "  0.9735872745513916,\n",
       "  0.02607356198132038],\n",
       " 'h2p_attention': [[0.6615484952926636,\n",
       "   0.03999358043074608,\n",
       "   0.04555582255125046,\n",
       "   0.046556826680898666,\n",
       "   0.032376978546381,\n",
       "   0.02884303592145443,\n",
       "   0.021681087091565132,\n",
       "   0.02199293114244938,\n",
       "   0.021933946758508682,\n",
       "   0.03280186280608177,\n",
       "   0.02464543841779232,\n",
       "   0.0220700204372406],\n",
       "  [2.6478128347662278e-05,\n",
       "   0.9997804164886475,\n",
       "   2.5384990294696763e-05,\n",
       "   2.803125425998587e-05,\n",
       "   1.5035763681225944e-05,\n",
       "   1.5200890629785135e-05,\n",
       "   2.0365438103908673e-05,\n",
       "   1.5323941624956205e-05,\n",
       "   1.667179458308965e-05,\n",
       "   3.005035614478402e-05,\n",
       "   1.3499801752914209e-05,\n",
       "   1.3597185898106545e-05],\n",
       "  [0.10203886777162552,\n",
       "   0.08704567700624466,\n",
       "   0.11516872048377991,\n",
       "   0.11659414321184158,\n",
       "   0.0897686704993248,\n",
       "   0.1088109016418457,\n",
       "   0.06201941892504692,\n",
       "   0.06663387268781662,\n",
       "   0.06219317018985748,\n",
       "   0.07818535715341568,\n",
       "   0.0593428835272789,\n",
       "   0.052198376506567],\n",
       "  [0.0004895933088846505,\n",
       "   0.0005661703762598336,\n",
       "   0.0014663741458207369,\n",
       "   0.9835996627807617,\n",
       "   0.003986356779932976,\n",
       "   0.0008538846741430461,\n",
       "   0.0008042860426940024,\n",
       "   0.004030915908515453,\n",
       "   0.000624869717285037,\n",
       "   0.002379189943894744,\n",
       "   0.0006751827313564718,\n",
       "   0.0005234954296611249],\n",
       "  [0.02180355414748192,\n",
       "   0.016224386170506477,\n",
       "   0.04582870751619339,\n",
       "   0.32416868209838867,\n",
       "   0.1262902319431305,\n",
       "   0.21321798861026764,\n",
       "   0.03501936420798302,\n",
       "   0.09745176136493683,\n",
       "   0.036249496042728424,\n",
       "   0.04630831629037857,\n",
       "   0.022747211158275604,\n",
       "   0.014690198004245758],\n",
       "  [0.01745988056063652,\n",
       "   0.012007005512714386,\n",
       "   0.03334576636552811,\n",
       "   0.23861074447631836,\n",
       "   0.06420435011386871,\n",
       "   0.42332977056503296,\n",
       "   0.03817636892199516,\n",
       "   0.050691068172454834,\n",
       "   0.05518278479576111,\n",
       "   0.03998453542590141,\n",
       "   0.016478458419442177,\n",
       "   0.010529308579862118],\n",
       "  [0.010657047852873802,\n",
       "   0.011342254467308521,\n",
       "   0.013355554081499577,\n",
       "   0.39848199486732483,\n",
       "   0.026157716289162636,\n",
       "   0.050147585570812225,\n",
       "   0.36920735239982605,\n",
       "   0.02936599962413311,\n",
       "   0.03243587911128998,\n",
       "   0.04311087727546692,\n",
       "   0.008368291892111301,\n",
       "   0.00736935855820775],\n",
       "  [0.012434241361916065,\n",
       "   0.013259582221508026,\n",
       "   0.03426486626267433,\n",
       "   0.24050143361091614,\n",
       "   0.13842594623565674,\n",
       "   0.37977859377861023,\n",
       "   0.06643011420965195,\n",
       "   0.05420059710741043,\n",
       "   0.014995560981333256,\n",
       "   0.025156904011964798,\n",
       "   0.01212186086922884,\n",
       "   0.008430331014096737],\n",
       "  [0.025600627064704895,\n",
       "   0.02177058905363083,\n",
       "   0.05209308862686157,\n",
       "   0.08083993941545486,\n",
       "   0.0719628632068634,\n",
       "   0.5535196661949158,\n",
       "   0.05518198758363724,\n",
       "   0.037947289645671844,\n",
       "   0.030034935101866722,\n",
       "   0.040441691875457764,\n",
       "   0.01738293468952179,\n",
       "   0.013224351219832897],\n",
       "  [0.0034482465125620365,\n",
       "   0.0038895425386726856,\n",
       "   0.005236065946519375,\n",
       "   0.013813172467052937,\n",
       "   0.010430196300148964,\n",
       "   0.05077805370092392,\n",
       "   0.641498327255249,\n",
       "   0.01323755457997322,\n",
       "   0.16897264122962952,\n",
       "   0.08184266090393066,\n",
       "   0.0035076595377177,\n",
       "   0.0033459344413131475],\n",
       "  [0.011839332059025764,\n",
       "   0.009265673346817493,\n",
       "   0.020825976505875587,\n",
       "   0.4289425313472748,\n",
       "   0.027788354083895683,\n",
       "   0.02105570212006569,\n",
       "   0.014683406800031662,\n",
       "   0.3304857909679413,\n",
       "   0.05788344889879227,\n",
       "   0.0527602918446064,\n",
       "   0.017025265842676163,\n",
       "   0.00744414608925581],\n",
       "  [0.027381373569369316,\n",
       "   0.024950889870524406,\n",
       "   0.05205394700169563,\n",
       "   0.4092806279659271,\n",
       "   0.08168795704841614,\n",
       "   0.0769209936261177,\n",
       "   0.02724587544798851,\n",
       "   0.16947263479232788,\n",
       "   0.047082576900720596,\n",
       "   0.05163945257663727,\n",
       "   0.02144119143486023,\n",
       "   0.010842563584446907],\n",
       "  [0.0038758176378905773,\n",
       "   0.003917417023330927,\n",
       "   0.005418309476226568,\n",
       "   0.045663487166166306,\n",
       "   0.007978193461894989,\n",
       "   0.009805173613131046,\n",
       "   0.012987712398171425,\n",
       "   0.11252029985189438,\n",
       "   0.21147406101226807,\n",
       "   0.5791746377944946,\n",
       "   0.004015128128230572,\n",
       "   0.003169688628986478],\n",
       "  [0.07595512270927429,\n",
       "   0.06700103729963303,\n",
       "   0.08602173626422882,\n",
       "   0.13264940679073334,\n",
       "   0.0768849328160286,\n",
       "   0.08079946041107178,\n",
       "   0.07029906660318375,\n",
       "   0.09581216424703598,\n",
       "   0.08089634776115417,\n",
       "   0.074249267578125,\n",
       "   0.0905224159359932,\n",
       "   0.06890902668237686],\n",
       "  [0.08349008858203888,\n",
       "   0.08230611681938171,\n",
       "   0.08249524235725403,\n",
       "   0.0853073000907898,\n",
       "   0.07970943301916122,\n",
       "   0.0837709978222847,\n",
       "   0.08443307131528854,\n",
       "   0.08118802309036255,\n",
       "   0.08668394386768341,\n",
       "   0.0874016061425209,\n",
       "   0.08157685399055481,\n",
       "   0.08163739740848541]],\n",
       " 'p2h_attention': [[0.5873391032218933,\n",
       "   0.037481389939785004,\n",
       "   0.036728233098983765,\n",
       "   0.01829739846289158,\n",
       "   0.028163855895400047,\n",
       "   0.031889576464891434,\n",
       "   0.026729749515652657,\n",
       "   0.027614938095211983,\n",
       "   0.03713584318757057,\n",
       "   0.02042875997722149,\n",
       "   0.034058839082717896,\n",
       "   0.05014196038246155,\n",
       "   0.02343931421637535,\n",
       "   0.0210875291377306,\n",
       "   0.019463635981082916],\n",
       "  [2.508237594156526e-05,\n",
       "   0.999733567237854,\n",
       "   2.2132628146209754e-05,\n",
       "   1.4946935152693186e-05,\n",
       "   1.4804178135818802e-05,\n",
       "   1.5491494195885025e-05,\n",
       "   2.0095949366805144e-05,\n",
       "   2.0802033759537153e-05,\n",
       "   2.2308173356577754e-05,\n",
       "   1.6277719623758458e-05,\n",
       "   1.8829146938514896e-05,\n",
       "   3.227626802981831e-05,\n",
       "   1.6735255485400558e-05,\n",
       "   1.3140176633896772e-05,\n",
       "   1.3554149518313352e-05],\n",
       "  [0.05465352162718773,\n",
       "   0.04855705797672272,\n",
       "   0.056016504764556885,\n",
       "   0.07405351102352142,\n",
       "   0.07999254763126373,\n",
       "   0.0822991207242012,\n",
       "   0.04526546597480774,\n",
       "   0.10283025354146957,\n",
       "   0.1021103709936142,\n",
       "   0.04191756993532181,\n",
       "   0.08095712214708328,\n",
       "   0.12880918383598328,\n",
       "   0.044278454035520554,\n",
       "   0.0322718508541584,\n",
       "   0.025987526401877403],\n",
       "  [0.0009891841327771544,\n",
       "   0.0009495936101302505,\n",
       "   0.001004332909360528,\n",
       "   0.879708468914032,\n",
       "   0.010020802728831768,\n",
       "   0.010429514572024345,\n",
       "   0.02391846291720867,\n",
       "   0.012782299891114235,\n",
       "   0.0028063070494681597,\n",
       "   0.0019584111869335175,\n",
       "   0.02953033149242401,\n",
       "   0.0179363414645195,\n",
       "   0.006608717143535614,\n",
       "   0.00088133366080001,\n",
       "   0.00047592856572009623],\n",
       "  [0.021002642810344696,\n",
       "   0.01555121410638094,\n",
       "   0.023608563467860222,\n",
       "   0.10885298252105713,\n",
       "   0.11919141560792923,\n",
       "   0.08568055927753448,\n",
       "   0.04793670400977135,\n",
       "   0.22462216019630432,\n",
       "   0.07627135515213013,\n",
       "   0.04514884203672409,\n",
       "   0.0584084689617157,\n",
       "   0.10929856449365616,\n",
       "   0.035253044217824936,\n",
       "   0.01559625193476677,\n",
       "   0.013577163219451904],\n",
       "  [0.007228714879602194,\n",
       "   0.0060742199420928955,\n",
       "   0.011056041345000267,\n",
       "   0.00900836382061243,\n",
       "   0.07774664461612701,\n",
       "   0.21826252341270447,\n",
       "   0.035505931824445724,\n",
       "   0.23809383809566498,\n",
       "   0.2266567349433899,\n",
       "   0.08492054045200348,\n",
       "   0.01709878444671631,\n",
       "   0.0397634282708168,\n",
       "   0.0167390089482069,\n",
       "   0.0063324240036308765,\n",
       "   0.005512842908501625],\n",
       "  [0.0035782784689217806,\n",
       "   0.005359055008739233,\n",
       "   0.004149807151407003,\n",
       "   0.0055876621045172215,\n",
       "   0.008408895693719387,\n",
       "   0.012961878441274166,\n",
       "   0.17214486002922058,\n",
       "   0.027425561100244522,\n",
       "   0.014880097471177578,\n",
       "   0.7064885497093201,\n",
       "   0.007852272130548954,\n",
       "   0.009274972602725029,\n",
       "   0.014600914902985096,\n",
       "   0.003628138452768326,\n",
       "   0.0036590411327779293],\n",
       "  [0.007103194482624531,\n",
       "   0.007891187444329262,\n",
       "   0.008725148625671864,\n",
       "   0.054802581667900085,\n",
       "   0.04579288512468338,\n",
       "   0.03368080407381058,\n",
       "   0.02679453045129776,\n",
       "   0.04378972575068474,\n",
       "   0.0200247373431921,\n",
       "   0.02852955460548401,\n",
       "   0.34585878252983093,\n",
       "   0.11289872974157333,\n",
       "   0.24754595756530762,\n",
       "   0.009676819667220116,\n",
       "   0.006885323207825422],\n",
       "  [0.0065566846169531345,\n",
       "   0.007946044206619263,\n",
       "   0.0075373295694589615,\n",
       "   0.007862917147576809,\n",
       "   0.015765480697155,\n",
       "   0.03393528610467911,\n",
       "   0.02739201858639717,\n",
       "   0.011213157325983047,\n",
       "   0.014669311232864857,\n",
       "   0.33705487847328186,\n",
       "   0.05606571584939957,\n",
       "   0.029029978439211845,\n",
       "   0.43060502409935,\n",
       "   0.007562020793557167,\n",
       "   0.00680405693128705],\n",
       "  [0.00604318268597126,\n",
       "   0.00882710050791502,\n",
       "   0.00583982327952981,\n",
       "   0.018451131880283356,\n",
       "   0.012412630952894688,\n",
       "   0.015154428780078888,\n",
       "   0.022438034415245056,\n",
       "   0.011593696661293507,\n",
       "   0.01217340212315321,\n",
       "   0.10061518102884293,\n",
       "   0.0314955934882164,\n",
       "   0.019623102620244026,\n",
       "   0.7268270254135132,\n",
       "   0.0042776064947247505,\n",
       "   0.004228129982948303],\n",
       "  [0.05502630025148392,\n",
       "   0.048057641834020615,\n",
       "   0.05371672287583351,\n",
       "   0.06345729529857635,\n",
       "   0.07389234751462936,\n",
       "   0.07568860054016113,\n",
       "   0.052783865481615067,\n",
       "   0.06770184636116028,\n",
       "   0.06341211497783661,\n",
       "   0.05225980281829834,\n",
       "   0.1231694370508194,\n",
       "   0.09874189645051956,\n",
       "   0.06106431409716606,\n",
       "   0.06320204585790634,\n",
       "   0.04782582074403763],\n",
       "  [0.06751631200313568,\n",
       "   0.06632179766893387,\n",
       "   0.06473962217569351,\n",
       "   0.06741329282522202,\n",
       "   0.06538397073745728,\n",
       "   0.0662652850151062,\n",
       "   0.06368929892778397,\n",
       "   0.06451314687728882,\n",
       "   0.0660991445183754,\n",
       "   0.06830303370952606,\n",
       "   0.07378979027271271,\n",
       "   0.06841585785150528,\n",
       "   0.06605063378810883,\n",
       "   0.06592094898223877,\n",
       "   0.06557781249284744]],\n",
       " 'premise_tokens': ['Two',\n",
       "  'women',\n",
       "  'are',\n",
       "  'wandering',\n",
       "  'along',\n",
       "  'the',\n",
       "  'shore',\n",
       "  'drinking',\n",
       "  'iced',\n",
       "  'tea',\n",
       "  '.',\n",
       "  '@@NULL@@'],\n",
       " 'hypothesis_tokens': ['Two',\n",
       "  'women',\n",
       "  'are',\n",
       "  'sitting',\n",
       "  'on',\n",
       "  'a',\n",
       "  'blanket',\n",
       "  'near',\n",
       "  'some',\n",
       "  'rocks',\n",
       "  'talking',\n",
       "  'about',\n",
       "  'politics',\n",
       "  '.',\n",
       "  '@@NULL@@']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = predictor.predict(\n",
    "  hypothesis=\"Two women are sitting on a blanket near some rocks talking about politics.\",\n",
    "  premise=\"Two women are wandering along the shore drinking iced tea.\"\n",
    ")\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(prediction['premise_tokens'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I guess I am feeling kinda tired. I feel overwhelmed, a bit, maybe hungry. I dunno. I find myself wanting something, but I'm not sure what it is. I feel stressed certainly, too much to do maybe? But I'm not totally sure what I should be doing? Now it's a lot later and it's really time for me to get to bed...but a part of me wants to stay up, nonetheless\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame([], columns=['premise', 'hypothesis', 'entailment', 'contradiction', 'neutral', 'e+c'])\n",
    "i = 0\n",
    "for premise in doc.sents:\n",
    "#     entailment, contradiction, neutral = None\n",
    "    for hypothesis in doc.sents:\n",
    "        if (premise != hypothesis):\n",
    "            prediction = predictor.predict(hypothesis=hypothesis.text, premise=premise.text)\n",
    "            entailment, contradiction, neutral = prediction['label_probs']\n",
    "            results.loc[i] = [premise.text, hypothesis.text, entailment, contradiction, neutral, (entailment + (1 - contradiction)) / 2]\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>entailment</th>\n",
       "      <th>contradiction</th>\n",
       "      <th>neutral</th>\n",
       "      <th>e+c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>0.956455</td>\n",
       "      <td>0.001234</td>\n",
       "      <td>0.042311</td>\n",
       "      <td>0.977611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>0.908410</td>\n",
       "      <td>0.005447</td>\n",
       "      <td>0.086143</td>\n",
       "      <td>0.951481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>I dunno.</td>\n",
       "      <td>0.904193</td>\n",
       "      <td>0.053517</td>\n",
       "      <td>0.042290</td>\n",
       "      <td>0.925338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>I dunno.</td>\n",
       "      <td>0.912021</td>\n",
       "      <td>0.070635</td>\n",
       "      <td>0.017344</td>\n",
       "      <td>0.920693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>0.836310</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.162772</td>\n",
       "      <td>0.917696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>I dunno.</td>\n",
       "      <td>0.867386</td>\n",
       "      <td>0.043489</td>\n",
       "      <td>0.089125</td>\n",
       "      <td>0.911948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>0.781964</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>0.216260</td>\n",
       "      <td>0.890094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>0.755611</td>\n",
       "      <td>0.008080</td>\n",
       "      <td>0.236309</td>\n",
       "      <td>0.873766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>0.767259</td>\n",
       "      <td>0.020967</td>\n",
       "      <td>0.211775</td>\n",
       "      <td>0.873146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>0.768105</td>\n",
       "      <td>0.025418</td>\n",
       "      <td>0.206478</td>\n",
       "      <td>0.871344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>0.739140</td>\n",
       "      <td>0.012235</td>\n",
       "      <td>0.248626</td>\n",
       "      <td>0.863452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>0.725396</td>\n",
       "      <td>0.002819</td>\n",
       "      <td>0.271786</td>\n",
       "      <td>0.861288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>0.735134</td>\n",
       "      <td>0.026731</td>\n",
       "      <td>0.238136</td>\n",
       "      <td>0.854201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>0.727070</td>\n",
       "      <td>0.022596</td>\n",
       "      <td>0.250334</td>\n",
       "      <td>0.852237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>0.717547</td>\n",
       "      <td>0.035562</td>\n",
       "      <td>0.246891</td>\n",
       "      <td>0.840992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>0.678346</td>\n",
       "      <td>0.013190</td>\n",
       "      <td>0.308464</td>\n",
       "      <td>0.832578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>0.659936</td>\n",
       "      <td>0.050289</td>\n",
       "      <td>0.289775</td>\n",
       "      <td>0.804823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>0.604333</td>\n",
       "      <td>0.006409</td>\n",
       "      <td>0.389258</td>\n",
       "      <td>0.798962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>0.682849</td>\n",
       "      <td>0.088772</td>\n",
       "      <td>0.228379</td>\n",
       "      <td>0.797039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>I dunno.</td>\n",
       "      <td>0.762202</td>\n",
       "      <td>0.184278</td>\n",
       "      <td>0.053520</td>\n",
       "      <td>0.788962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>0.567174</td>\n",
       "      <td>0.003467</td>\n",
       "      <td>0.429359</td>\n",
       "      <td>0.781854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>0.584167</td>\n",
       "      <td>0.030758</td>\n",
       "      <td>0.385075</td>\n",
       "      <td>0.776704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>0.566095</td>\n",
       "      <td>0.020107</td>\n",
       "      <td>0.413798</td>\n",
       "      <td>0.772994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>0.589310</td>\n",
       "      <td>0.043696</td>\n",
       "      <td>0.366994</td>\n",
       "      <td>0.772807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>0.571464</td>\n",
       "      <td>0.038498</td>\n",
       "      <td>0.390039</td>\n",
       "      <td>0.766483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>0.566257</td>\n",
       "      <td>0.038796</td>\n",
       "      <td>0.394947</td>\n",
       "      <td>0.763730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>0.538478</td>\n",
       "      <td>0.052767</td>\n",
       "      <td>0.408755</td>\n",
       "      <td>0.742856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>I dunno.</td>\n",
       "      <td>0.715452</td>\n",
       "      <td>0.238567</td>\n",
       "      <td>0.045980</td>\n",
       "      <td>0.738443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>0.548609</td>\n",
       "      <td>0.073129</td>\n",
       "      <td>0.378262</td>\n",
       "      <td>0.737740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>0.474978</td>\n",
       "      <td>0.064011</td>\n",
       "      <td>0.461011</td>\n",
       "      <td>0.705483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.081693</td>\n",
       "      <td>0.458307</td>\n",
       "      <td>0.689153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>0.523283</td>\n",
       "      <td>0.235219</td>\n",
       "      <td>0.241498</td>\n",
       "      <td>0.644032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>0.381097</td>\n",
       "      <td>0.175689</td>\n",
       "      <td>0.443214</td>\n",
       "      <td>0.602704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>0.373584</td>\n",
       "      <td>0.176693</td>\n",
       "      <td>0.449723</td>\n",
       "      <td>0.598446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>0.386267</td>\n",
       "      <td>0.217766</td>\n",
       "      <td>0.395967</td>\n",
       "      <td>0.584250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>0.329103</td>\n",
       "      <td>0.298593</td>\n",
       "      <td>0.372304</td>\n",
       "      <td>0.515255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>0.269745</td>\n",
       "      <td>0.259522</td>\n",
       "      <td>0.470732</td>\n",
       "      <td>0.505112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>I dunno.</td>\n",
       "      <td>0.285518</td>\n",
       "      <td>0.384351</td>\n",
       "      <td>0.330131</td>\n",
       "      <td>0.450584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>0.234756</td>\n",
       "      <td>0.339052</td>\n",
       "      <td>0.426192</td>\n",
       "      <td>0.447852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>0.247193</td>\n",
       "      <td>0.352285</td>\n",
       "      <td>0.400522</td>\n",
       "      <td>0.447454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.486933</td>\n",
       "      <td>0.479734</td>\n",
       "      <td>0.273200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>0.045921</td>\n",
       "      <td>0.508242</td>\n",
       "      <td>0.445838</td>\n",
       "      <td>0.268840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>0.073138</td>\n",
       "      <td>0.543047</td>\n",
       "      <td>0.383816</td>\n",
       "      <td>0.265046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>0.019627</td>\n",
       "      <td>0.526356</td>\n",
       "      <td>0.454018</td>\n",
       "      <td>0.246635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>I dunno.</td>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>0.038739</td>\n",
       "      <td>0.573239</td>\n",
       "      <td>0.388022</td>\n",
       "      <td>0.232750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>0.102812</td>\n",
       "      <td>0.712032</td>\n",
       "      <td>0.185157</td>\n",
       "      <td>0.195390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>I dunno.</td>\n",
       "      <td>0.021096</td>\n",
       "      <td>0.634602</td>\n",
       "      <td>0.344303</td>\n",
       "      <td>0.193247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>I dunno.</td>\n",
       "      <td>0.018045</td>\n",
       "      <td>0.898486</td>\n",
       "      <td>0.083470</td>\n",
       "      <td>0.059780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>I dunno.</td>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>0.000822</td>\n",
       "      <td>0.894391</td>\n",
       "      <td>0.104787</td>\n",
       "      <td>0.053215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>0.003158</td>\n",
       "      <td>0.925812</td>\n",
       "      <td>0.071030</td>\n",
       "      <td>0.038673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              premise  \\\n",
       "43   But I'm not totally sure what I should be doing?   \n",
       "44   But I'm not totally sure what I should be doing?   \n",
       "1                   I guess I am feeling kinda tired.   \n",
       "34   I feel stressed certainly, too much to do maybe?   \n",
       "8            I feel overwhelmed, a bit, maybe hungry.   \n",
       "9            I feel overwhelmed, a bit, maybe hungry.   \n",
       "32   I feel stressed certainly, too much to do maybe?   \n",
       "35   I feel stressed certainly, too much to do maybe?   \n",
       "27  I find myself wanting something, but I'm not s...   \n",
       "47   But I'm not totally sure what I should be doing?   \n",
       "40   But I'm not totally sure what I should be doing?   \n",
       "0                   I guess I am feeling kinda tired.   \n",
       "31  I find myself wanting something, but I'm not s...   \n",
       "41   But I'm not totally sure what I should be doing?   \n",
       "10           I feel overwhelmed, a bit, maybe hungry.   \n",
       "11           I feel overwhelmed, a bit, maybe hungry.   \n",
       "29  I find myself wanting something, but I'm not s...   \n",
       "3                   I guess I am feeling kinda tired.   \n",
       "39   I feel stressed certainly, too much to do maybe?   \n",
       "42   But I'm not totally sure what I should be doing?   \n",
       "33   I feel stressed certainly, too much to do maybe?   \n",
       "67     but a part of me wants to stay up, nonetheless   \n",
       "24  I find myself wanting something, but I'm not s...   \n",
       "36   I feel stressed certainly, too much to do maybe?   \n",
       "25  I find myself wanting something, but I'm not s...   \n",
       "70     but a part of me wants to stay up, nonetheless   \n",
       "68     but a part of me wants to stay up, nonetheless   \n",
       "26  I find myself wanting something, but I'm not s...   \n",
       "37   I feel stressed certainly, too much to do maybe?   \n",
       "2                   I guess I am feeling kinda tired.   \n",
       "28  I find myself wanting something, but I'm not s...   \n",
       "63       and it's really time for me to get to bed...   \n",
       "45   But I'm not totally sure what I should be doing?   \n",
       "12           I feel overwhelmed, a bit, maybe hungry.   \n",
       "55                               Now it's a lot later   \n",
       "15           I feel overwhelmed, a bit, maybe hungry.   \n",
       "4                   I guess I am feeling kinda tired.   \n",
       "66     but a part of me wants to stay up, nonetheless   \n",
       "59       and it's really time for me to get to bed...   \n",
       "51                               Now it's a lot later   \n",
       "30  I find myself wanting something, but I'm not s...   \n",
       "53                               Now it's a lot later   \n",
       "61       and it's really time for me to get to bed...   \n",
       "46   But I'm not totally sure what I should be doing?   \n",
       "21                                           I dunno.   \n",
       "60       and it's really time for me to get to bed...   \n",
       "50                               Now it's a lot later   \n",
       "58       and it's really time for me to get to bed...   \n",
       "22                                           I dunno.   \n",
       "6                   I guess I am feeling kinda tired.   \n",
       "\n",
       "                                           hypothesis  entailment  \\\n",
       "43  I find myself wanting something, but I'm not s...    0.956455   \n",
       "44   I feel stressed certainly, too much to do maybe?    0.908410   \n",
       "1                                            I dunno.    0.904193   \n",
       "34                                           I dunno.    0.912021   \n",
       "8                   I guess I am feeling kinda tired.    0.836310   \n",
       "9                                            I dunno.    0.867386   \n",
       "32                  I guess I am feeling kinda tired.    0.781964   \n",
       "35  I find myself wanting something, but I'm not s...    0.755611   \n",
       "27   I feel stressed certainly, too much to do maybe?    0.767259   \n",
       "47     but a part of me wants to stay up, nonetheless    0.768105   \n",
       "40                  I guess I am feeling kinda tired.    0.739140   \n",
       "0            I feel overwhelmed, a bit, maybe hungry.    0.725396   \n",
       "31     but a part of me wants to stay up, nonetheless    0.735134   \n",
       "41           I feel overwhelmed, a bit, maybe hungry.    0.727070   \n",
       "10  I find myself wanting something, but I'm not s...    0.717547   \n",
       "11   I feel stressed certainly, too much to do maybe?    0.678346   \n",
       "29                               Now it's a lot later    0.659936   \n",
       "3    I feel stressed certainly, too much to do maybe?    0.604333   \n",
       "39     but a part of me wants to stay up, nonetheless    0.682849   \n",
       "42                                           I dunno.    0.762202   \n",
       "33           I feel overwhelmed, a bit, maybe hungry.    0.567174   \n",
       "67  I find myself wanting something, but I'm not s...    0.584167   \n",
       "24                  I guess I am feeling kinda tired.    0.566095   \n",
       "36   But I'm not totally sure what I should be doing?    0.589310   \n",
       "25           I feel overwhelmed, a bit, maybe hungry.    0.571464   \n",
       "70                               Now it's a lot later    0.566257   \n",
       "68   I feel stressed certainly, too much to do maybe?    0.538478   \n",
       "26                                           I dunno.    0.715452   \n",
       "37                               Now it's a lot later    0.548609   \n",
       "2   I find myself wanting something, but I'm not s...    0.474978   \n",
       "28   But I'm not totally sure what I should be doing?    0.460000   \n",
       "63     but a part of me wants to stay up, nonetheless    0.523283   \n",
       "45                               Now it's a lot later    0.381097   \n",
       "12   But I'm not totally sure what I should be doing?    0.373584   \n",
       "55     but a part of me wants to stay up, nonetheless    0.386267   \n",
       "15     but a part of me wants to stay up, nonetheless    0.329103   \n",
       "4    But I'm not totally sure what I should be doing?    0.269745   \n",
       "66                                           I dunno.    0.285518   \n",
       "59  I find myself wanting something, but I'm not s...    0.234756   \n",
       "51  I find myself wanting something, but I'm not s...    0.247193   \n",
       "30       and it's really time for me to get to bed...    0.033333   \n",
       "53   But I'm not totally sure what I should be doing?    0.045921   \n",
       "61   But I'm not totally sure what I should be doing?    0.073138   \n",
       "46       and it's really time for me to get to bed...    0.019627   \n",
       "21                               Now it's a lot later    0.038739   \n",
       "60   I feel stressed certainly, too much to do maybe?    0.102812   \n",
       "50                                           I dunno.    0.021096   \n",
       "58                                           I dunno.    0.018045   \n",
       "22       and it's really time for me to get to bed...    0.000822   \n",
       "6        and it's really time for me to get to bed...    0.003158   \n",
       "\n",
       "    contradiction   neutral       e+c  \n",
       "43       0.001234  0.042311  0.977611  \n",
       "44       0.005447  0.086143  0.951481  \n",
       "1        0.053517  0.042290  0.925338  \n",
       "34       0.070635  0.017344  0.920693  \n",
       "8        0.000918  0.162772  0.917696  \n",
       "9        0.043489  0.089125  0.911948  \n",
       "32       0.001776  0.216260  0.890094  \n",
       "35       0.008080  0.236309  0.873766  \n",
       "27       0.020967  0.211775  0.873146  \n",
       "47       0.025418  0.206478  0.871344  \n",
       "40       0.012235  0.248626  0.863452  \n",
       "0        0.002819  0.271786  0.861288  \n",
       "31       0.026731  0.238136  0.854201  \n",
       "41       0.022596  0.250334  0.852237  \n",
       "10       0.035562  0.246891  0.840992  \n",
       "11       0.013190  0.308464  0.832578  \n",
       "29       0.050289  0.289775  0.804823  \n",
       "3        0.006409  0.389258  0.798962  \n",
       "39       0.088772  0.228379  0.797039  \n",
       "42       0.184278  0.053520  0.788962  \n",
       "33       0.003467  0.429359  0.781854  \n",
       "67       0.030758  0.385075  0.776704  \n",
       "24       0.020107  0.413798  0.772994  \n",
       "36       0.043696  0.366994  0.772807  \n",
       "25       0.038498  0.390039  0.766483  \n",
       "70       0.038796  0.394947  0.763730  \n",
       "68       0.052767  0.408755  0.742856  \n",
       "26       0.238567  0.045980  0.738443  \n",
       "37       0.073129  0.378262  0.737740  \n",
       "2        0.064011  0.461011  0.705483  \n",
       "28       0.081693  0.458307  0.689153  \n",
       "63       0.235219  0.241498  0.644032  \n",
       "45       0.175689  0.443214  0.602704  \n",
       "12       0.176693  0.449723  0.598446  \n",
       "55       0.217766  0.395967  0.584250  \n",
       "15       0.298593  0.372304  0.515255  \n",
       "4        0.259522  0.470732  0.505112  \n",
       "66       0.384351  0.330131  0.450584  \n",
       "59       0.339052  0.426192  0.447852  \n",
       "51       0.352285  0.400522  0.447454  \n",
       "30       0.486933  0.479734  0.273200  \n",
       "53       0.508242  0.445838  0.268840  \n",
       "61       0.543047  0.383816  0.265046  \n",
       "46       0.526356  0.454018  0.246635  \n",
       "21       0.573239  0.388022  0.232750  \n",
       "60       0.712032  0.185157  0.195390  \n",
       "50       0.634602  0.344303  0.193247  \n",
       "58       0.898486  0.083470  0.059780  \n",
       "22       0.894391  0.104787  0.053215  \n",
       "6        0.925812  0.071030  0.038673  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(by='e+c', ascending=False).loc[results['neutral'] < .5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = 'I feel stressed'\n",
    "\n",
    "results = pd.DataFrame([], columns=['premise', 'hypothesis', 'entailment', 'contradiction', 'neutral'])\n",
    "i = 0\n",
    "for premise in doc.sents:\n",
    "    prediction = predictor.predict(hypothesis=hypothesis, premise=premise.text)\n",
    "    entailment, contradiction, neutral = prediction['label_probs']\n",
    "    results.loc[i] = [premise.text, hypothesis, entailment, contradiction, neutral]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>entailment</th>\n",
       "      <th>contradiction</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>I feel stressed</td>\n",
       "      <td>0.985132</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.014467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>I feel stressed</td>\n",
       "      <td>0.936851</td>\n",
       "      <td>0.002266</td>\n",
       "      <td>0.060882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>I feel stressed</td>\n",
       "      <td>0.933847</td>\n",
       "      <td>0.002187</td>\n",
       "      <td>0.063966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>I feel stressed</td>\n",
       "      <td>0.833155</td>\n",
       "      <td>0.004319</td>\n",
       "      <td>0.162525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>I feel stressed</td>\n",
       "      <td>0.769592</td>\n",
       "      <td>0.041008</td>\n",
       "      <td>0.189401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I dunno.</td>\n",
       "      <td>I feel stressed</td>\n",
       "      <td>0.493208</td>\n",
       "      <td>0.287141</td>\n",
       "      <td>0.219651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>I feel stressed</td>\n",
       "      <td>0.293085</td>\n",
       "      <td>0.115519</td>\n",
       "      <td>0.591396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>I feel stressed</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>0.353283</td>\n",
       "      <td>0.537717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>I feel stressed</td>\n",
       "      <td>0.081763</td>\n",
       "      <td>0.259905</td>\n",
       "      <td>0.658333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             premise       hypothesis  \\\n",
       "4   I feel stressed certainly, too much to do maybe?  I feel stressed   \n",
       "0                  I guess I am feeling kinda tired.  I feel stressed   \n",
       "1           I feel overwhelmed, a bit, maybe hungry.  I feel stressed   \n",
       "3  I find myself wanting something, but I'm not s...  I feel stressed   \n",
       "5   But I'm not totally sure what I should be doing?  I feel stressed   \n",
       "2                                           I dunno.  I feel stressed   \n",
       "8     but a part of me wants to stay up, nonetheless  I feel stressed   \n",
       "7       and it's really time for me to get to bed...  I feel stressed   \n",
       "6                               Now it's a lot later  I feel stressed   \n",
       "\n",
       "   entailment  contradiction   neutral  \n",
       "4    0.985132       0.000401  0.014467  \n",
       "0    0.936851       0.002266  0.060882  \n",
       "1    0.933847       0.002187  0.063966  \n",
       "3    0.833155       0.004319  0.162525  \n",
       "5    0.769592       0.041008  0.189401  \n",
       "2    0.493208       0.287141  0.219651  \n",
       "8    0.293085       0.115519  0.591396  \n",
       "7    0.109000       0.353283  0.537717  \n",
       "6    0.081763       0.259905  0.658333  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(by='entailment', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo(shape):\n",
    "    nlp = spacy.load('en_vectors_web_lg')\n",
    "    nlp.add_pipe(KerasSimilarityShim.load(nlp.path / 'similarity', nlp, shape[0]))\n",
    "\n",
    "    doc1 = nlp(u'The king of France is bald.')\n",
    "    doc2 = nlp(u'France has no king.')\n",
    "\n",
    "    print(\"Sentence 1:\", doc1)\n",
    "    print(\"Sentence 2:\", doc2)\n",
    "\n",
    "    entailment_type, confidence = doc1.similarity(doc2)\n",
    "    print(\"Entailment type:\", entailment_type, \"(Confidence:\", confidence, \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textacy.vsm import Vectorizer\n",
    "vectorizer = Vectorizer(\n",
    "    tf_type='linear', apply_idf=True, idf_type='smooth', norm='l2',\n",
    "    min_df=3, max_df=0.95, max_n_terms=100000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = textacy.tm.TopicModel('nmf', n_topics=20)\n",
    "model.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy.keyterms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sure', 0.17393909018065556),\n",
       " ('overwhelmed', 0.1291297332498814),\n",
       " ('time', 0.12848449534695075),\n",
       " ('bit', 0.12152651186559832),\n",
       " ('lot', 0.12055447304217964),\n",
       " ('hungry', 0.11759733293982846),\n",
       " ('tired', 0.07154722366605952),\n",
       " ('bed', 0.0712721534037857),\n",
       " ('stressed', 0.06594898630506062)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms = textacy.keyterms.key_terms_from_semantic_network(doc)\n",
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sure', 0.33850825516726607),\n",
       " ('stressed', 0.1517767729454664),\n",
       " ('bed', 0.1484936700017998),\n",
       " ('time', 0.0968496495027204),\n",
       " ('lot', 0.07202910699164278),\n",
       " ('hungry', 0.07058674833731196),\n",
       " ('bit', 0.051420957354160426),\n",
       " ('overwhelmed', 0.03964374936267677),\n",
       " ('tired', 0.030691090336955648)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms = textacy.keyterms.sgrank(doc)\n",
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I guess I am feeling kinda tired. I feel overwhelmed, a bit, maybe hungry. I dunno. I find myself wanting something, but I'm not sure what it is. I feel stressed certainly, too much to do maybe? But I'm not totally sure what I should be doing? Now it's a lot later and it's really time for me to get to bed...but a part of me wants to stay up, nonetheless\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy.lexicon_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/05/2018 21:16:22 - INFO - textacy.lexicon_methods -   Downloaded DepecheMood (4MB) from https://github.com/marcoguerini/DepecheMood/releases/download/v1.0/DepecheMood_V1.0.zip and wrote it to data\n"
     ]
    }
   ],
   "source": [
    "textacy.lexicon_methods.download_depechemood(data_dir='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {'AFRAID': 0.1103335419756097,\n",
       "             'AMUSED': 0.14808456209756102,\n",
       "             'ANGRY': 0.10694153351219511,\n",
       "             'ANNOYED': 0.12443051617073168,\n",
       "             'DONT_CARE': 0.13096818899999998,\n",
       "             'HAPPY': 0.11531756726829266,\n",
       "             'INSPIRED': 0.14843126431707318,\n",
       "             'SAD': 0.11549282553658531})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textacy.lexicon_methods.emotional_valence(words=[word for word in doc], dm_data_dir='data/DepecheMood_V1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2018 17:58:27 - INFO - event2mind_hack -   loading archive file data/event2mind.tar.gz\n",
      "12/06/2018 17:58:27 - INFO - event2mind_hack -   extracting archive file data/event2mind.tar.gz to temp dir /tmp/tmp0dlhchct\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   vocabulary.type = default\n",
      "12/06/2018 17:58:28 - INFO - allennlp.data.vocabulary -   Loading token dictionary from /tmp/tmp0dlhchct/vocabulary.\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'event2mind_hack.Model'> from params {'embedding_dropout': 0.2, 'encoder': {'bidirectional': True, 'hidden_size': 50, 'input_size': 300, 'num_layers': 1, 'type': 'gru'}, 'max_decoding_steps': 10, 'source_embedder': {'token_embedders': {'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding', 'vocab_namespace': 'source_tokens'}}}, 'target_namespace': 'target_tokens', 'type': 'event2mind'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fccd4e752e8>}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.type = event2mind\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'event2mind_hack.Event2Mind'> from params {'embedding_dropout': 0.2, 'encoder': {'bidirectional': True, 'hidden_size': 50, 'input_size': 300, 'num_layers': 1, 'type': 'gru'}, 'max_decoding_steps': 10, 'source_embedder': {'token_embedders': {'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding', 'vocab_namespace': 'source_tokens'}}}, 'target_namespace': 'target_tokens'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fccd4e752e8>}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'token_embedders': {'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding', 'vocab_namespace': 'source_tokens'}}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fccd4e752e8>}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.type = basic\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.embedder_to_indexer_map = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.allow_unmatched_keys = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 300, 'trainable': False, 'type': 'embedding', 'vocab_namespace': 'source_tokens'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fccd4e752e8>}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.type = embedding\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.num_embeddings = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.vocab_namespace = source_tokens\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.embedding_dim = 300\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.pretrained_file = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.projection_dim = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.trainable = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.padding_index = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.max_norm = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.norm_type = 2.0\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.scale_grad_by_freq = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.sparse = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.embedding_dropout = 0.2\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder'> from params {'bidirectional': True, 'hidden_size': 50, 'input_size': 300, 'num_layers': 1, 'type': 'gru'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fccd4e752e8>}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.encoder.type = gru\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.encoder.batch_first = True\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.encoder.bidirectional = True\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.encoder.hidden_size = 50\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.encoder.input_size = 300\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.encoder.num_layers = 1\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.encoder.batch_first = True\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.max_decoding_steps = 10\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.beam_size = 10\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.target_names = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.target_namespace = target_tokens\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.target_embedding_dim = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'source_token_indexers': {'tokens': {'namespace': 'source_tokens', 'type': 'single_id'}}, 'source_tokenizer': {'type': 'word', 'word_splitter': {'type': 'spacy'}}, 'target_token_indexers': {'tokens': {'namespace': 'target_tokens'}}, 'target_tokenizer': {'type': 'word'}, 'type': 'event2mind'} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.type = event2mind\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.event2mind.Event2MindDatasetReader'> from params {'source_token_indexers': {'tokens': {'namespace': 'source_tokens', 'type': 'single_id'}}, 'source_tokenizer': {'type': 'word', 'word_splitter': {'type': 'spacy'}}, 'target_token_indexers': {'tokens': {'namespace': 'target_tokens'}}, 'target_tokenizer': {'type': 'word'}} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.tokenizer.Tokenizer'> from params {'type': 'word', 'word_splitter': {'type': 'spacy'}} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.type = word\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.word_tokenizer.WordTokenizer'> from params {'word_splitter': {'type': 'spacy'}} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.word_splitter.WordSplitter'> from params {'type': 'spacy'} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.word_splitter.type = spacy\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.word_splitter.SpacyWordSplitter'> from params {} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.word_splitter.language = en_core_web_sm\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.word_splitter.pos_tags = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.word_splitter.parse = False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.word_splitter.ner = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.start_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.end_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.tokenizer.Tokenizer'> from params {'type': 'word'} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_tokenizer.type = word\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.word_tokenizer.WordTokenizer'> from params {} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_tokenizer.start_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_tokenizer.end_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.token_indexers.token_indexer.TokenIndexer'> from params {'namespace': 'source_tokens', 'type': 'single_id'} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_token_indexers.tokens.type = single_id\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer'> from params {'namespace': 'source_tokens'} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_token_indexers.tokens.namespace = source_tokens\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_token_indexers.tokens.lowercase_tokens = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_token_indexers.tokens.start_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_token_indexers.tokens.end_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.token_indexers.token_indexer.TokenIndexer'> from params {'namespace': 'target_tokens'} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_token_indexers.tokens.type = single_id\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer'> from params {'namespace': 'target_tokens'} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_token_indexers.tokens.namespace = target_tokens\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_token_indexers.tokens.lowercase_tokens = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_token_indexers.tokens.start_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_token_indexers.tokens.end_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_add_start_token = True\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.dummy_instances_for_vocab_generation = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.lazy = False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'xintent_top_k_predictions': [[4, 3, 3, 3],\n",
       "  [684, 3, 3, 3],\n",
       "  [323, 3, 3, 3],\n",
       "  [282, 3, 3, 3],\n",
       "  [255, 3, 3, 3],\n",
       "  [153, 229, 3, 3],\n",
       "  [13, 175, 3, 3],\n",
       "  [44, 251, 3, 3],\n",
       "  [13, 267, 3, 3],\n",
       "  [211, 32, 80, 3]],\n",
       " 'xintent_top_k_log_probabilities': [-1.128340721130371,\n",
       "  -4.298313617706299,\n",
       "  -4.499514579772949,\n",
       "  -4.616389751434326,\n",
       "  -4.652942657470703,\n",
       "  -5.256786823272705,\n",
       "  -5.543419361114502,\n",
       "  -6.178347587585449,\n",
       "  -6.431229114532471,\n",
       "  -9.508707046508789],\n",
       " 'xreact_top_k_predictions': [[54, 3],\n",
       "  [70, 3],\n",
       "  [53, 3],\n",
       "  [109, 3],\n",
       "  [5, 3],\n",
       "  [73, 3],\n",
       "  [11, 3],\n",
       "  [92, 3],\n",
       "  [25, 3],\n",
       "  [63, 3]],\n",
       " 'xreact_top_k_log_probabilities': [-3.147449016571045,\n",
       "  -3.159241199493408,\n",
       "  -3.1768059730529785,\n",
       "  -3.2743079662323,\n",
       "  -3.3373990058898926,\n",
       "  -3.516559362411499,\n",
       "  -3.596619129180908,\n",
       "  -3.8112082481384277,\n",
       "  -3.8679120540618896,\n",
       "  -3.9374184608459473],\n",
       " 'oreact_top_k_predictions': [[4, 3],\n",
       "  [63, 3],\n",
       "  [36, 3],\n",
       "  [89, 3],\n",
       "  [83, 3],\n",
       "  [91, 3],\n",
       "  [138, 3],\n",
       "  [53, 3],\n",
       "  [92, 3],\n",
       "  [5, 3]],\n",
       " 'oreact_top_k_log_probabilities': [-1.067413330078125,\n",
       "  -2.62384033203125,\n",
       "  -3.9126994609832764,\n",
       "  -3.979201316833496,\n",
       "  -4.0716962814331055,\n",
       "  -4.296855926513672,\n",
       "  -4.327937602996826,\n",
       "  -4.378903388977051,\n",
       "  -4.453375339508057,\n",
       "  -4.506431579589844],\n",
       " 'xintent_top_k_predicted_tokens': [['none'],\n",
       "  ['annoying'],\n",
       "  ['noticed'],\n",
       "  ['communicate'],\n",
       "  ['heard'],\n",
       "  ['express', 'anger'],\n",
       "  ['get', 'attention'],\n",
       "  ['show', 'affection'],\n",
       "  ['get', 'revenge'],\n",
       "  ['let', 'someone', 'know']],\n",
       " 'xreact_top_k_predicted_tokens': [['upset'],\n",
       "  ['worried'],\n",
       "  ['nervous'],\n",
       "  ['curious'],\n",
       "  ['happy'],\n",
       "  ['scared'],\n",
       "  ['satisfied'],\n",
       "  ['anxious'],\n",
       "  ['relieved'],\n",
       "  ['annoyed']],\n",
       " 'oreact_top_k_predicted_tokens': [['none'],\n",
       "  ['annoyed'],\n",
       "  ['angry'],\n",
       "  ['informed'],\n",
       "  ['surprised'],\n",
       "  ['interested'],\n",
       "  ['frustrated'],\n",
       "  ['nervous'],\n",
       "  ['anxious'],\n",
       "  ['happy']]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from event2mind_hack import load_event2mind_archive\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "\n",
    "archive = load_event2mind_archive('data/event2mind.tar.gz')\n",
    "predictor = Predictor.from_archive(archive)\n",
    "predictor.predict(\n",
    "  source=\"PersonX drops a hint\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36787944117144233"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.exp(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>p_log</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[none]</td>\n",
       "      <td>-1.128341</td>\n",
       "      <td>0.323570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[annoying]</td>\n",
       "      <td>-4.298314</td>\n",
       "      <td>0.013591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[noticed]</td>\n",
       "      <td>-4.499515</td>\n",
       "      <td>0.011114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[communicate]</td>\n",
       "      <td>-4.616390</td>\n",
       "      <td>0.009888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[heard]</td>\n",
       "      <td>-4.652943</td>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[express, anger]</td>\n",
       "      <td>-5.256787</td>\n",
       "      <td>0.005212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[get, attention]</td>\n",
       "      <td>-5.543419</td>\n",
       "      <td>0.003913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[show, affection]</td>\n",
       "      <td>-6.178348</td>\n",
       "      <td>0.002074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[get, revenge]</td>\n",
       "      <td>-6.431229</td>\n",
       "      <td>0.001610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[let, someone, know]</td>\n",
       "      <td>-9.508707</td>\n",
       "      <td>0.000074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tokens     p_log         p\n",
       "0                [none] -1.128341  0.323570\n",
       "1            [annoying] -4.298314  0.013591\n",
       "2             [noticed] -4.499515  0.011114\n",
       "3         [communicate] -4.616390  0.009888\n",
       "4               [heard] -4.652943  0.009534\n",
       "5      [express, anger] -5.256787  0.005212\n",
       "6      [get, attention] -5.543419  0.003913\n",
       "7     [show, affection] -6.178348  0.002074\n",
       "8        [get, revenge] -6.431229  0.001610\n",
       "9  [let, someone, know] -9.508707  0.000074"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xintent = pd.DataFrame({\n",
    "    'tokens': prediction['xintent_top_k_predicted_tokens'],\n",
    "    'p_log': prediction['xintent_top_k_log_probabilities']\n",
    "})\n",
    "xintent['p'] = xintent['p_log'].apply(math.exp)\n",
    "xintent.sort_values(by='p', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>p_log</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[upset]</td>\n",
       "      <td>-3.147449</td>\n",
       "      <td>0.042962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[worried]</td>\n",
       "      <td>-3.159241</td>\n",
       "      <td>0.042458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[nervous]</td>\n",
       "      <td>-3.176806</td>\n",
       "      <td>0.041719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[curious]</td>\n",
       "      <td>-3.274308</td>\n",
       "      <td>0.037843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[happy]</td>\n",
       "      <td>-3.337399</td>\n",
       "      <td>0.035529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[scared]</td>\n",
       "      <td>-3.516559</td>\n",
       "      <td>0.029701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[satisfied]</td>\n",
       "      <td>-3.596619</td>\n",
       "      <td>0.027416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[anxious]</td>\n",
       "      <td>-3.811208</td>\n",
       "      <td>0.022121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[relieved]</td>\n",
       "      <td>-3.867912</td>\n",
       "      <td>0.020902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[annoyed]</td>\n",
       "      <td>-3.937418</td>\n",
       "      <td>0.019498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        tokens     p_log         p\n",
       "0      [upset] -3.147449  0.042962\n",
       "1    [worried] -3.159241  0.042458\n",
       "2    [nervous] -3.176806  0.041719\n",
       "3    [curious] -3.274308  0.037843\n",
       "4      [happy] -3.337399  0.035529\n",
       "5     [scared] -3.516559  0.029701\n",
       "6  [satisfied] -3.596619  0.027416\n",
       "7    [anxious] -3.811208  0.022121\n",
       "8   [relieved] -3.867912  0.020902\n",
       "9    [annoyed] -3.937418  0.019498"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xreact = pd.DataFrame({\n",
    "    'tokens': prediction['xreact_top_k_predicted_tokens'],\n",
    "    'p_log': prediction['xreact_top_k_log_probabilities']\n",
    "})\n",
    "xreact['p'] = xreact['p_log'].apply(math.exp)\n",
    "xreact.sort_values(by='p', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>p_log</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[none]</td>\n",
       "      <td>-1.067413</td>\n",
       "      <td>0.343897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[annoyed]</td>\n",
       "      <td>-2.623840</td>\n",
       "      <td>0.072524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[angry]</td>\n",
       "      <td>-3.912699</td>\n",
       "      <td>0.019986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[informed]</td>\n",
       "      <td>-3.979201</td>\n",
       "      <td>0.018701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[surprised]</td>\n",
       "      <td>-4.071696</td>\n",
       "      <td>0.017048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[interested]</td>\n",
       "      <td>-4.296856</td>\n",
       "      <td>0.013611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[frustrated]</td>\n",
       "      <td>-4.327938</td>\n",
       "      <td>0.013195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[nervous]</td>\n",
       "      <td>-4.378903</td>\n",
       "      <td>0.012539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[anxious]</td>\n",
       "      <td>-4.453375</td>\n",
       "      <td>0.011639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[happy]</td>\n",
       "      <td>-4.506432</td>\n",
       "      <td>0.011038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tokens     p_log         p\n",
       "0        [none] -1.067413  0.343897\n",
       "1     [annoyed] -2.623840  0.072524\n",
       "2       [angry] -3.912699  0.019986\n",
       "3    [informed] -3.979201  0.018701\n",
       "4   [surprised] -4.071696  0.017048\n",
       "5  [interested] -4.296856  0.013611\n",
       "6  [frustrated] -4.327938  0.013195\n",
       "7     [nervous] -4.378903  0.012539\n",
       "8     [anxious] -4.453375  0.011639\n",
       "9       [happy] -4.506432  0.011038"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oreact = pd.DataFrame({\n",
    "    'tokens': prediction['oreact_top_k_predicted_tokens'],\n",
    "    'p_log': prediction['oreact_top_k_log_probabilities']\n",
    "})\n",
    "oreact['p'] = oreact['p_log'].apply(math.exp)\n",
    "oreact.sort_values(by='p', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
