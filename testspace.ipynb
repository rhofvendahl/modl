{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading en_coref_md...\n",
      "...done\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import textacy\n",
    "\n",
    "print('loading en_coref_md...')\n",
    "# nlp = spacy.load('en_coref_md')\n",
    "print('...done')\n",
    "\n",
    "\n",
    "\n",
    "# for now assuming all names are unique identifiers\n",
    "class Person:\n",
    "    statements = []\n",
    "    resolved_refs = []\n",
    "\n",
    "    def __init__(self, name, pronouns=None, refs=[], user=False):\n",
    "        self.name = name\n",
    "#         self.gender = gender\n",
    "        self.refs = refs\n",
    "        self.user = user\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# UPGRADE AT SOME POINT TO EXTRACT GENDER, ACCOUNT FOR CLUSTERS WITHOUT NAMES\n",
    "# UPGRADE TO INCLUDE I, USER\n",
    "\n",
    "# assumes names are unique identifiers\n",
    "# assumes misspellings are diff people\n",
    "\n",
    "# MEMORYLESS FOR NOW; each change to text means a whole new model\n",
    "# Set extensions later, for keeping track of which tokens are what\n",
    "class Model:\n",
    "    text = None\n",
    "    doc = None\n",
    "    people = []\n",
    "    resolved_text = None\n",
    "    resolved_doc = None\n",
    "\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.doc = self.get_doc()\n",
    "        self.people = self.get_people()\n",
    "        self.resolve()\n",
    "\n",
    "    def get_doc(self, text=None):\n",
    "        if text == None:\n",
    "            text = self.text\n",
    "        preprocessed = textacy.preprocess.normalize_whitespace(text)\n",
    "        preprocessed = textacy.preprocess.preprocess_text(preprocessed, fix_unicode=True, no_contractions=True, no_accents=True)\n",
    "        return nlp(preprocessed)\n",
    "\n",
    "        # merge mentions into tokens for easy coref tracking, resolution\n",
    "        # for cluster in doc._.coref_clusters:\n",
    "        #     for mention in cluster.mentions:\n",
    "        #         mention.merge()\n",
    "        # return doc\n",
    "\n",
    "    def get_person_by_name(self, name):\n",
    "        for person in self.people:\n",
    "            if person.name == name:\n",
    "                return person\n",
    "        return None\n",
    "\n",
    "    def get_people(self, doc=None):\n",
    "        if doc == None:\n",
    "            doc = self.doc\n",
    "        namedrops = [ent for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "        names = set([namedrop.text for namedrop in namedrops])\n",
    "        people = []\n",
    "\n",
    "        # for clusters that include namedrops\n",
    "        for cluster in doc._.coref_clusters:\n",
    "            name = None\n",
    "\n",
    "            for mention in cluster.mentions:\n",
    "                mention_text = mention.root.text\n",
    "                if mention_text in names:\n",
    "                    name = mention_text\n",
    "\n",
    "            if name != None:\n",
    "                person = self.get_person_by_name(name)\n",
    "                if person == None:\n",
    "                    person = Person(name, refs=cluster.mentions)\n",
    "                    people += [person]\n",
    "                else:\n",
    "                    person.refs += cluster.mentions\n",
    "\n",
    "            # for named entities without clusters (single mentions)\n",
    "            for name_mention in namedrops:\n",
    "                person = self.get_person_by_name(name_mention.text)\n",
    "                if person == None:\n",
    "                    person = Person(name_mention.text, refs=[name_mention])\n",
    "                    people += [person]\n",
    "        return people\n",
    "\n",
    "    def get_resolved_text(self, doc=None):\n",
    "        if doc == None:\n",
    "            doc = self.doc\n",
    "        tokens = [token.text for token in doc]\n",
    "        for person in self.people:\n",
    "            for ref in person.refs:\n",
    "\n",
    "                # determine resolved value\n",
    "                resolved_token = person.name\n",
    "                if ref.root.pos_ == 'ADJ':\n",
    "                    resolved_token += '\\'s'\n",
    "\n",
    "                # set first token to resolved value\n",
    "                tokens[ref.start] = resolved_token\n",
    "\n",
    "                # set extra tokens in mention to blank\n",
    "                for i in range(ref.start+1, ref.end):\n",
    "                    tokens[i] = ''\n",
    "        return ' '.join([token for token in tokens if token != ''])\n",
    "\n",
    "    def resolve(self):\n",
    "        self.resolved_text = self.get_resolved_text()\n",
    "        self.resolved_doc = nlp(self.resolved_text)\n",
    "\n",
    "        offset = 0;\n",
    "        for person in self.people:\n",
    "            for ref in person.refs:\n",
    "                resolved_ref = self.resolved_doc[ref.start-offset:ref.end-offset]\n",
    "                person.resolved_refs += [resolved_ref]\n",
    "\n",
    "                # increase offset for each multi-word ref\n",
    "                words_in_ref = (ref.end - ref.start)\n",
    "                offset += words_in_ref - 1\n",
    "\n",
    "\n",
    "    # def update_people_statements(self, doc):\n",
    "    #     res = nlp(self.resolve_people(doc))\n",
    "    #\n",
    "    #     for person in model.people:\n",
    "    #         statements = []\n",
    "    #         for ref in person.refs:\n",
    "    #             head = ref.root.head\n",
    "    #             if head.pos_ == 'VERB':\n",
    "    #                 for statement in textacy.extract.semistructured_statements(res, person.name, head.lemma_):\n",
    "    #                     statements += [statement]\n",
    "    #         person.statements = list(set(statements))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Sheila was run over by a truck. She herself didn't see that coming. I told her she should take care of herself, but I know she'll just go and do her thing regardless of what I say. What a conundrum! This makes me wish I had never signed up to be friends with her, although I do love the girl.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"November was a trying month ... on Dante Dante had a major accident . 5 minutes before school and Dante and some friends are climbing the fence , I tell Dante it 's not a good idea and to get down . I turn back to talk to Jodi ( on of my best mom friend 's at the school ) and Dante comes to me screaming with Dante's hand full of blood . I run Dante into my classroom and get Dante to the sink , as I turn on the water to clean the area the flap of Dante's thumb lifts away and I see the bone . Shit . This is not something I can fix here , I grab my first aid kit and wrap it like crazy because it 's bleeding like crazy . I phone James and tell James to get to the ER as Dante is screaming and freaking out in the background as I am trying to usher James back to the car as James 's bleeding like a stuffed pig . Unfortunately in the ER I learned that my child does not take to freezing , an hour of gel freezing and James still felt the 2 needles as they went in , 15 minutes later and James felt the last 2 stitches of 8 . James needed more because James's finger still had gaps , the doctor did not want to cause him anymore pain so he glued them . It was an intense and deep gash that spiraled all the way up his thumb . I was trying to stay strong for him but I did break down as he screamed and cried , I was left to emotionally drained that day . James was able to take the remainder of the day off and stay with James . James missed 2 more days of school and then had an extra long weekend due to the holiday and the pro day but for 2 weeks James could not write ( of course it was James's right hand . ) 3 doctor visits later and James finally got them out full last week , the first visit the doctor wanted them in longer because of the severity . 2nd time James could only get 6 out because the glue had gotten on the last 2 stitches and James did not want to have to dig them out so we had to soak and dissolve the glue for 3 days . 3rd time the last 2 came out . Even now James 's slowly regaining James's writing skills as there was some nerve damage .\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(text)\n",
    "model.resolved_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sheila was run over by a truck . Sheila did not see that coming . I told Sheila Sheila should take care of Sheila , but I know Sheila will just go and do Sheila's thing regardless of what I say . What a conundrum ! This makes me wish I had never signed up to be friends with Sheila , although I do love Sheila .\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resolved_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'November was a trying month… on the 7th Dante had a major accident. 5 minutes before school and he and some friends are climbing the fence, I tell him it’s not a good idea and to get down. I turn back to talk to Jodi (on of my best mom friend’s at the school) and Dante comes to me screaming with his hand full of blood. I run him into my classroom and get him to the sink, as I turn on the water to clean the area the flap of his thumb lifts away and I see the bone. Shit. This isn’t something I can fix here, I grab my first aid kit and wrap it like crazy because it’s bleeding like crazy. I phone James and tell him to get to the ER as Dante is screaming and freaking out in the background as I’m trying to usher him back to the car as he’s bleeding like a stuffed pig. Unfortunately in the ER I learned that my child doesn’t take to freezing, an hour of gel freezing and he still felt the 2 needles as they went in, 15 minutes later and he felt the last 2 stitches of 8. He needed more because his finger still had gaps, the doctor didn’t want to cause him anymore pain so he glued them. It was an intense and deep gash that spiraled all the way up his thumb. I was trying to stay strong for him but I did break down as he screamed and cried, I was left to emotionally drained that day. James was able to take the remainder of the day off and stay with him. He missed 2 more days of school and then had an extra long weekend due to the holiday and the pro day but for 2 weeks he couldn’t write (of course it was his right hand.) 3 doctor visits later and he finally got them out full last week, the first visit the doctor wanted them in longer because of the severity. 2nd time he could only get 6 out because the glue had gotten on the last 2 stitches and he didn’t want to have to dig them out so we had to soak and dissolve the glue for 3 days. 3rd time the last 2 came out.  Even now he’s slowly regaining his writing skills as there was some nerve damage.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 7th Dante: [the 7th Dante, he, him, Dante, his, him, him, his]\n",
      "the 7th Dante: [the 7th Dante, he, him, Dante, his, him, him, his]\n",
      "the 7th Dante: [the 7th Dante, he, him, Dante, his, him, him, his]\n",
      "the 7th Dante: [the 7th Dante, he, him, Dante, his, him, him, his]\n",
      "the 7th Dante: [the 7th Dante, he, him, Dante, his, him, him, his]\n",
      "the 7th Dante: [the 7th Dante, he, him, Dante, his, him, him, his]\n",
      "the 7th Dante: [the 7th Dante, he, him, Dante, his, him, him, his]\n",
      "the 7th Dante: [the 7th Dante, he, him, Dante, his, him, him, his]\n",
      "school: [school, the school]\n",
      "school: [school, the school]\n",
      "my first aid kit: [my first aid kit, it, it]\n",
      "my first aid kit: [my first aid kit, it, it]\n",
      "my first aid kit: [my first aid kit, it, it]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "[E037] Error calculating span: Can't find a token ending at character offset 1001.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-272ac2ebecf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcluster\u001b[0m \u001b[0;32min\u001b[0m \u001b[0madoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoref_clusters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmention\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmentions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mneuralcoref.pyx\u001b[0m in \u001b[0;36men_coref_md.neuralcoref.neuralcoref.Cluster.__str__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mneuralcoref.pyx\u001b[0m in \u001b[0;36men_coref_md.neuralcoref.neuralcoref.Cluster.__unicode__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mspan.pyx\u001b[0m in \u001b[0;36mspacy.tokens.span.Span.__repr__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mspan.pyx\u001b[0m in \u001b[0;36mspacy.tokens.span.Span.text.__get__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mspan.pyx\u001b[0m in \u001b[0;36mspacy.tokens.span.Span.text_with_ws.__get__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mspan.pyx\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mspan.pyx\u001b[0m in \u001b[0;36mspacy.tokens.span.Span._recalculate_indices\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: [E037] Error calculating span: Can't find a token ending at character offset 1001."
     ]
    }
   ],
   "source": [
    "adoc = nlp(text)\n",
    "# for cluster in adoc._.coref_clusters:\n",
    "#     for mention in cluster.mentions:\n",
    "#         mention.merge()\n",
    "        \n",
    "for cluster in adoc._.coref_clusters:\n",
    "    for mention in cluster.mentions:\n",
    "        print(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 7th Dante: [the 7th Dante, he, him, Dante, his, him, him, his]\n",
      "the 7th Dante: [the 7th Dante, he, him, Dante, his, him, him, his]\n",
      "the 7th Dante: [the 7th Dante, he, him, Dante, his, him, him, his]\n",
      "the 7th Dante: [the 7th Dante, he, him, Dante, his, him, him, his]\n",
      "the 7th Dante: [the 7th Dante, he, him, Dante, his, him, him, his]\n",
      "the 7th Dante: [the 7th Dante, he, him, Dante, his, him, him, his]\n",
      "the 7th Dante: [the 7th Dante, he, him, Dante, his, him, him, his]\n",
      "the 7th Dante: [the 7th Dante, he, him, Dante, his, him, him, his]\n",
      "school: [school, the school]\n",
      "school: [school, the school]\n",
      "my first aid kit: [my first aid kit, it, it]\n",
      "my first aid kit: [my first aid kit, it, it]\n",
      "my first aid kit: [my first aid kit, it, it]\n",
      "James: [James, him, him, he, he, he, He, his, him, he, his, him, he]\n",
      "James: [James, him, him, he, he, he, He, his, him, he, his, him, he]\n",
      "James: [James, him, him, he, he, he, He, his, him, he, his, him, he]\n",
      "James: [James, him, him, he, he, he, He, his, him, he, his, him, he]\n",
      "James: [James, him, him, he, he, he, He, his, him, he, his, him, he]\n",
      "James: [James, him, him, he, he, he, He, his, him, he, his, him, he]\n",
      "James: [James, him, him, he, he, he, He, his, him, he, his, him, he]\n",
      "James: [James, him, him, he, he, he, He, his, him, he, his, him, he]\n",
      "James: [James, him, him, he, he, he, He, his, him, he, his, him, he]\n",
      "James: [James, him, him, he, he, he, He, his, him, he, his, him, he]\n",
      "James: [James, him, him, he, he, he, He, his, him, he, his, him, he]\n",
      "James: [James, him, him, he, he, he, He, his, him, he, his, him, he]\n",
      "James: [James, him, him, he, he, he, He, his, him, he, his, him, he]\n",
      "the ER: [the ER, the ER]\n",
      "the ER: [the ER, the ER]\n",
      "his finger still had gaps: [his finger still had gaps, them]\n",
      "his finger still had gaps: [his finger still had gaps, them]\n",
      "James: [James, him, He, he, his, he, he, he, he, his]\n",
      "James: [James, him, He, he, his, he, he, he, he, his]\n",
      "James: [James, him, He, he, his, he, he, he, he, his]\n",
      "James: [James, him, He, he, his, he, he, he, he, his]\n",
      "James: [James, him, He, he, his, he, he, he, he, his]\n",
      "James: [James, him, He, he, his, he, he, he, he, his]\n",
      "James: [James, him, He, he, his, he, he, he, he, his]\n",
      "James: [James, him, He, he, his, he, he, he, he, his]\n",
      "James: [James, him, He, he, his, he, he, he, he, his]\n",
      "James: [James, him, He, he, his, he, he, he, he, his]\n",
      "them: [them, them, them]\n",
      "them: [them, them, them]\n",
      "them: [them, them, them]\n",
      "the glue: [the glue, the glue]\n",
      "the glue: [the glue, the glue]\n"
     ]
    }
   ],
   "source": [
    "adoc = nlp(text)\n",
    "# for cluster in adoc._.coref_clusters:\n",
    "#     for mention in cluster.mentions:\n",
    "#         mention.merge()\n",
    "        \n",
    "for cluster in adoc._.coref_clusters:\n",
    "    for mention in cluster.mentions:\n",
    "        print(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
