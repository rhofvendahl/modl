{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading en_coref_md...\n",
      "...done\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import textacy\n",
    "\n",
    "print('loading en_coref_md...')\n",
    "nlp = spacy.load('en_coref_md')\n",
    "print('...done')\n",
    "\n",
    "\n",
    "\n",
    "# for now assuming all names are unique identifiers\n",
    "class Person:\n",
    "    statements = []\n",
    "\n",
    "    def __init__(self, name, pronouns=None, refs=[], user=False):\n",
    "        self.name = name\n",
    "#         self.gender = gender\n",
    "        self.refs = refs\n",
    "        self.user = user\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# UPGRADE AT SOME POINT TO EXTRACT GENDER, ACCOUNT FOR CLUSTERS WITHOUT NAMES\n",
    "# UPGRADE TO INCLUDE I, USER\n",
    "\n",
    "# assumes names are unique identifiers\n",
    "# assumes misspellings are diff people\n",
    "\n",
    "# MEMORYLESS FOR NOW; each change to text means a whole new model\n",
    "# Set extensions later, for keeping track of which tokens are what\n",
    "class Model:\n",
    "    text = None\n",
    "    doc = None\n",
    "    people = []\n",
    "    resolved_text = None\n",
    "    resolved_doc = None\n",
    "\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.doc = self.get_doc()\n",
    "        self.people = self.get_people()\n",
    "        self.resolved_text = self.get_resolved()\n",
    "        self.resolved_doc = self.get_doc(self.resolved_text)\n",
    "\n",
    "    def get_doc(self, text=None):\n",
    "        if text == None:\n",
    "            text = self.text\n",
    "        preprocessed = textacy.preprocess.normalize_whitespace(text)\n",
    "        preprocessed = textacy.preprocess.preprocess_text(preprocessed, fix_unicode=True, no_contractions=True, no_accents=True)\n",
    "        doc = nlp(preprocessed)\n",
    "\n",
    "        # merge mentions into tokens for easy coref tracking, resolution\n",
    "        for cluster in doc._.coref_clusters:\n",
    "            for mention in cluster.mentions:\n",
    "                mention.merge()\n",
    "        return doc\n",
    "\n",
    "        def get_person_by_name(self, name):\n",
    "            for person in self.people:\n",
    "                if person.name == name:\n",
    "                    return person\n",
    "                    return None\n",
    "\n",
    "    def get_people(self, doc=None):\n",
    "        if doc == None:\n",
    "            doc = self.doc\n",
    "        namedrops = [ent for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "        names = set([namedrop.text for namedrop in namedrops])\n",
    "        people = []\n",
    "\n",
    "        # for clusters that include namedrops\n",
    "        for cluster in doc._.coref_clusters:\n",
    "            name = None\n",
    "\n",
    "            for mention in cluster.mentions:\n",
    "                mention_text = mention.root.text\n",
    "                if mention_text in names:\n",
    "                    name = mention_text\n",
    "\n",
    "            if name != None:\n",
    "                person = self.get_person_by_name(name)\n",
    "                refs = [mention.head.i for mention in cluster.mentions]\n",
    "                if person == None:\n",
    "                    person = Person(name, refs=refs)\n",
    "                    people += [person]\n",
    "                else:\n",
    "                    person.refs += refs\n",
    "\n",
    "            # for named entities without clusters (single mentions)\n",
    "            for name_mention in name_mentions:\n",
    "                person = self.get_person_by_name(name_mention.text)\n",
    "                if person == None:\n",
    "                    person = Person(name_mention.text, refs=[name_mention.head.i])\n",
    "                    people += [person]\n",
    "        return people\n",
    "\n",
    "    def get_resolved(self, doc=None):\n",
    "        if doc == None:\n",
    "            doc = self.doc\n",
    "        token_texts = [token.text for token in doc]\n",
    "\n",
    "        for person in self.people:\n",
    "            for ref in person.refs:\n",
    "                resolved = person.name\n",
    "                if doc[ref].pos_ == 'ADJ':\n",
    "                    resolved += '\\'s'\n",
    "\n",
    "                token_texts[ref] = resolved\n",
    "        return ' '.join(token_texts)\n",
    "\n",
    "    # def update_people_statements(self, doc):\n",
    "    #     res = nlp(self.resolve_people(doc))\n",
    "    #\n",
    "    #     for person in model.people:\n",
    "    #         statements = []\n",
    "    #         for ref in person.refss:\n",
    "    #             head = ref.root.head\n",
    "    #             if head.pos_ == 'VERB':\n",
    "    #                 for statement in textacy.extract.semistructured_statements(res, person.name, head.lemma_):\n",
    "    #                     statements += [statement]\n",
    "    #         person.statements = list(set(statements))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Sheila was run over by a truck. She herself didn't see that coming. I told her she should take care of herself, but I know she'll just go and do her thing regardless of what I say. What a conundrum! This makes me wish I had never signed up to be friends with her, although I do love the girl.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "[E037] Error calculating span: Can't find a token ending at character offset 35.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3471b0574c65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-b7b8a2297c80>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeople\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_people\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolved_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_resolved\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolved_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolved_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-b7b8a2297c80>\u001b[0m in \u001b[0;36mget_people\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmention\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmentions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mmention_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmention_text\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmention_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mspan.pyx\u001b[0m in \u001b[0;36mspacy.tokens.span.Span.root.__get__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mspan.pyx\u001b[0m in \u001b[0;36mspacy.tokens.span.Span._recalculate_indices\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: [E037] Error calculating span: Can't find a token ending at character offset 35."
     ]
    }
   ],
   "source": [
    "model = Model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'November was a trying month… on the 7th Dante had a major accident. 5 minutes before school and he and some friends are climbing the fence, I tell him it’s not a good idea and to get down. I turn back to talk to Jodi (on of my best mom friend’s at the school) and Dante comes to me screaming with his hand full of blood. I run him into my classroom and get him to the sink, as I turn on the water to clean the area the flap of his thumb lifts away and I see the bone. Shit. This isn’t something I can fix here, I grab my first aid kit and wrap it like crazy because it’s bleeding like crazy. I phone James and tell him to get to the ER as Dante is screaming and freaking out in the background as I’m trying to usher him back to the car as he’s bleeding like a stuffed pig. Unfortunately in the ER I learned that my child doesn’t take to freezing, an hour of gel freezing and he still felt the 2 needles as they went in, 15 minutes later and he felt the last 2 stitches of 8. He needed more because his finger still had gaps, the doctor didn’t want to cause him anymore pain so he glued them. It was an intense and deep gash that spiraled all the way up his thumb. I was trying to stay strong for him but I did break down as he screamed and cried, I was left to emotionally drained that day. James was able to take the remainder of the day off and stay with him. He missed 2 more days of school and then had an extra long weekend due to the holiday and the pro day but for 2 weeks he couldn’t write (of course it was his right hand.) 3 doctor visits later and he finally got them out full last week, the first visit the doctor wanted them in longer because of the severity. 2nd time he could only get 6 out because the glue had gotten on the last 2 stitches and he didn’t want to have to dig them out so we had to soak and dissolve the glue for 3 days. 3rd time the last 2 came out.  Even now he’s slowly regaining his writing skills as there was some nerve damage.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 7th Dante: [the 7th Dante, he, him, Dante, his, him, him, his]\n",
      "the 7th Dante: [the 7th Dante, he, him, Dante, his, him, him, his]\n",
      "the 7th Dante: [the 7th Dante, he, him, Dante, his, him, him, his]\n",
      "the 7th Dante: [the 7th Dante, he, him, Dante, his, him, him, his]\n",
      "the 7th Dante: [the 7th Dante, he, him, Dante, his, him, him, his]\n",
      "the 7th Dante: [the 7th Dante, he, him, Dante, his, him, him, his]\n",
      "the 7th Dante: [the 7th Dante, he, him, Dante, his, him, him, his]\n",
      "the 7th Dante: [the 7th Dante, he, him, Dante, his, him, him, his]\n",
      "school: [school, the school]\n",
      "school: [school, the school]\n",
      "my first aid kit: [my first aid kit, it, it]\n",
      "my first aid kit: [my first aid kit, it, it]\n",
      "my first aid kit: [my first aid kit, it, it]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "[E037] Error calculating span: Can't find a token ending at character offset 1001.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-272ac2ebecf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcluster\u001b[0m \u001b[0;32min\u001b[0m \u001b[0madoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoref_clusters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmention\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmentions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mneuralcoref.pyx\u001b[0m in \u001b[0;36men_coref_md.neuralcoref.neuralcoref.Cluster.__str__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mneuralcoref.pyx\u001b[0m in \u001b[0;36men_coref_md.neuralcoref.neuralcoref.Cluster.__unicode__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mspan.pyx\u001b[0m in \u001b[0;36mspacy.tokens.span.Span.__repr__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mspan.pyx\u001b[0m in \u001b[0;36mspacy.tokens.span.Span.text.__get__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mspan.pyx\u001b[0m in \u001b[0;36mspacy.tokens.span.Span.text_with_ws.__get__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mspan.pyx\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mspan.pyx\u001b[0m in \u001b[0;36mspacy.tokens.span.Span._recalculate_indices\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: [E037] Error calculating span: Can't find a token ending at character offset 1001."
     ]
    }
   ],
   "source": [
    "adoc = nlp(text)\n",
    "# for cluster in adoc._.coref_clusters:\n",
    "#     for mention in cluster.mentions:\n",
    "#         mention.merge()\n",
    "        \n",
    "for cluster in adoc._.coref_clusters:\n",
    "    for mention in cluster.mentions:\n",
    "        print(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 7th Dante: [the 7th Dante, he, him, Dante, his, him, him, his]\n",
      "the 7th Dante: [the 7th Dante, he, him, Dante, his, him, him, his]\n",
      "the 7th Dante: [the 7th Dante, he, him, Dante, his, him, him, his]\n",
      "the 7th Dante: [the 7th Dante, he, him, Dante, his, him, him, his]\n",
      "the 7th Dante: [the 7th Dante, he, him, Dante, his, him, him, his]\n",
      "the 7th Dante: [the 7th Dante, he, him, Dante, his, him, him, his]\n",
      "the 7th Dante: [the 7th Dante, he, him, Dante, his, him, him, his]\n",
      "the 7th Dante: [the 7th Dante, he, him, Dante, his, him, him, his]\n",
      "school: [school, the school]\n",
      "school: [school, the school]\n",
      "my first aid kit: [my first aid kit, it, it]\n",
      "my first aid kit: [my first aid kit, it, it]\n",
      "my first aid kit: [my first aid kit, it, it]\n",
      "James: [James, him, him, he, he, he, He, his, him, he, his, him, he]\n",
      "James: [James, him, him, he, he, he, He, his, him, he, his, him, he]\n",
      "James: [James, him, him, he, he, he, He, his, him, he, his, him, he]\n",
      "James: [James, him, him, he, he, he, He, his, him, he, his, him, he]\n",
      "James: [James, him, him, he, he, he, He, his, him, he, his, him, he]\n",
      "James: [James, him, him, he, he, he, He, his, him, he, his, him, he]\n",
      "James: [James, him, him, he, he, he, He, his, him, he, his, him, he]\n",
      "James: [James, him, him, he, he, he, He, his, him, he, his, him, he]\n",
      "James: [James, him, him, he, he, he, He, his, him, he, his, him, he]\n",
      "James: [James, him, him, he, he, he, He, his, him, he, his, him, he]\n",
      "James: [James, him, him, he, he, he, He, his, him, he, his, him, he]\n",
      "James: [James, him, him, he, he, he, He, his, him, he, his, him, he]\n",
      "James: [James, him, him, he, he, he, He, his, him, he, his, him, he]\n",
      "the ER: [the ER, the ER]\n",
      "the ER: [the ER, the ER]\n",
      "his finger still had gaps: [his finger still had gaps, them]\n",
      "his finger still had gaps: [his finger still had gaps, them]\n",
      "James: [James, him, He, he, his, he, he, he, he, his]\n",
      "James: [James, him, He, he, his, he, he, he, he, his]\n",
      "James: [James, him, He, he, his, he, he, he, he, his]\n",
      "James: [James, him, He, he, his, he, he, he, he, his]\n",
      "James: [James, him, He, he, his, he, he, he, he, his]\n",
      "James: [James, him, He, he, his, he, he, he, he, his]\n",
      "James: [James, him, He, he, his, he, he, he, he, his]\n",
      "James: [James, him, He, he, his, he, he, he, he, his]\n",
      "James: [James, him, He, he, his, he, he, he, he, his]\n",
      "James: [James, him, He, he, his, he, he, he, he, his]\n",
      "them: [them, them, them]\n",
      "them: [them, them, them]\n",
      "them: [them, them, them]\n",
      "the glue: [the glue, the glue]\n",
      "the glue: [the glue, the glue]\n"
     ]
    }
   ],
   "source": [
    "adoc = nlp(text)\n",
    "# for cluster in adoc._.coref_clusters:\n",
    "#     for mention in cluster.mentions:\n",
    "#         mention.merge()\n",
    "        \n",
    "for cluster in adoc._.coref_clusters:\n",
    "    for mention in cluster.mentions:\n",
    "        print(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
